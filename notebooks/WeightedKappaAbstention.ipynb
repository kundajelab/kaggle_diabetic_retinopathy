{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "valid_labels = LabelBinarizer().fit_transform(\n",
    "    np.array([float(x.decode(\"utf-8\").split(\"\\t\")[1])\n",
    "              for x in gzip.open(\"valid_labels.txt.gz\",'rb')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#augmenting the dataset with flips and rotaitons, for more robustness\n",
    "parent_folders = [\"flip-False_rotamt-0\",\n",
    "                  \"flip-True_rotamt-0\",\n",
    "                  \"flip-False_rotamt-90\",\n",
    "                  \"flip-True_rotamt-90\",\n",
    "                  \"flip-False_rotamt-180\",\n",
    "                  \"flip-True_rotamt-180\",\n",
    "                  \"flip-False_rotamt-270\",\n",
    "                  \"flip-True_rotamt-270\",]\n",
    "\n",
    "parent_folder_to_det_pred = {}\n",
    "for parent_folder in parent_folders:\n",
    "    det_preds = np.array([\n",
    "            [float(y) for y in x.decode(\"utf-8\").split(\"\\t\")[1:]]\n",
    "             for x in gzip.open(parent_folder+\"/deterministic_preds.txt.gz\", 'rb')])\n",
    "    parent_folder_to_det_pred[parent_folder] = det_preds\n",
    "    \n",
    "parent_folder_to_nondet_pred = {}\n",
    "parent_folder_to_mean_nondet_pred = {}\n",
    "for parent_folder in parent_folders:\n",
    "    nondet_preds = []\n",
    "    for i in range(100):\n",
    "        single_nondet_pred = np.array([\n",
    "            [float(y) for y in x.decode(\"utf-8\").split(\"\\t\")[1:]]\n",
    "             for x in gzip.open(\n",
    "              parent_folder+\"/nondeterministic_preds_\"+str(i)+\".txt.gz\", 'rb')])\n",
    "        nondet_preds.append(single_nondet_pred)\n",
    "    nondet_preds = np.array(nondet_preds)\n",
    "    parent_folder_to_nondet_pred[parent_folder] = nondet_preds\n",
    "    parent_folder_to_mean_nondet_pred[parent_folder] = np.mean(nondet_preds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flip-False_rotamt-0\n",
      "deterministic pred auROC 0.9118638796723656\n",
      "nondeterministic pred auROC 0.9129881925522253\n",
      "flip-True_rotamt-0\n",
      "deterministic pred auROC 0.9136845292158645\n",
      "nondeterministic pred auROC 0.9141930341618936\n",
      "flip-False_rotamt-90\n",
      "deterministic pred auROC 0.9077797755493358\n",
      "nondeterministic pred auROC 0.9079080860318696\n",
      "flip-True_rotamt-90\n",
      "deterministic pred auROC 0.9072243126739039\n",
      "nondeterministic pred auROC 0.9084814670645733\n",
      "flip-False_rotamt-180\n",
      "deterministic pred auROC 0.916166708887612\n",
      "nondeterministic pred auROC 0.9166587373671843\n",
      "flip-True_rotamt-180\n",
      "deterministic pred auROC 0.9131712872857287\n",
      "nondeterministic pred auROC 0.9138490879246036\n",
      "flip-False_rotamt-270\n",
      "deterministic pred auROC 0.9074450973244279\n",
      "nondeterministic pred auROC 0.9075221248051144\n",
      "flip-True_rotamt-270\n",
      "deterministic pred auROC 0.9051834963473733\n",
      "nondeterministic pred auROC 0.9063560028916199\n"
     ]
    }
   ],
   "source": [
    "#Compute the auROC/auPRC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for parent_folder in parent_folders:\n",
    "    print(parent_folder)\n",
    "    det_preds = parent_folder_to_det_pred[parent_folder]\n",
    "    mean_nondet_preds = parent_folder_to_mean_nondet_pred[parent_folder]\n",
    "    print(\"deterministic pred auROC\",\n",
    "          roc_auc_score(y_true=1-valid_labels[:,0],\n",
    "                              y_score=1-det_preds[:,0]))\n",
    "    print(\"nondeterministic pred auROC\",\n",
    "          roc_auc_score(y_true=1-valid_labels[:,0],\n",
    "                              y_score=1-mean_nondet_preds[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstainer settings uncalib_weightrescalepreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings tempscale-calib_weightrescalepreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings tempscalebiascor-calib_weightrescalepreds_imbalanced_adapted\n",
      "on fold 0\n",
      "Accuracy before calib: 0.6763038548752834\n",
      "Accuracy before adaptation: 0.685374149659864\n",
      "Accuracy after adaptation: 0.6989795918367347\n",
      "WKappa-minexpcost before calib: 0.7732867697923806\n",
      "WKappa-minexpcost before adaptation: 0.7598300134767754\n",
      "WKappa-minexpcost after adaptation: 0.8011358958862742\n",
      "on fold 1\n",
      "Accuracy before calib: 0.6712056737588652\n",
      "Accuracy before adaptation: 0.6839716312056737\n",
      "Accuracy after adaptation: 0.6695035460992907\n",
      "WKappa-minexpcost before calib: 0.7933822755982682\n",
      "WKappa-minexpcost before adaptation: 0.7793411304024486\n",
      "WKappa-minexpcost after adaptation: 0.81170290596393\n",
      "on fold 2\n",
      "Accuracy before calib: 0.6947040498442367\n",
      "Accuracy before adaptation: 0.703483432455395\n",
      "Accuracy after adaptation: 0.7006513735485698\n",
      "WKappa-minexpcost before calib: 0.8065240449273566\n",
      "WKappa-minexpcost before adaptation: 0.8009141370327689\n",
      "WKappa-minexpcost after adaptation: 0.8148035953877273\n",
      "on fold 3\n",
      "Accuracy before calib: 0.7089002267573696\n",
      "Accuracy before adaptation: 0.7083333333333334\n",
      "Accuracy after adaptation: 0.7244897959183674\n",
      "WKappa-minexpcost before calib: 0.8018622536134496\n",
      "WKappa-minexpcost before adaptation: 0.7918756079426115\n",
      "WKappa-minexpcost after adaptation: 0.8288126155210539\n",
      "on fold 4\n",
      "Accuracy before calib: 0.6785714285714286\n",
      "Accuracy before adaptation: 0.6933106575963719\n",
      "Accuracy after adaptation: 0.6964285714285714\n",
      "WKappa-minexpcost before calib: 0.7751530620814764\n",
      "WKappa-minexpcost before adaptation: 0.765613095903213\n",
      "WKappa-minexpcost after adaptation: 0.7911459872306524\n",
      "on fold 5\n",
      "Accuracy before calib: 0.6740425531914893\n",
      "Accuracy before adaptation: 0.683404255319149\n",
      "Accuracy after adaptation: 0.6904964539007092\n",
      "WKappa-minexpcost before calib: 0.7560665460926798\n",
      "WKappa-minexpcost before adaptation: 0.7374702797217801\n",
      "WKappa-minexpcost after adaptation: 0.774805225772793\n",
      "on fold 6\n",
      "Accuracy before calib: 0.6819085487077535\n",
      "Accuracy before adaptation: 0.6918489065606361\n",
      "Accuracy after adaptation: 0.6921329167850042\n",
      "WKappa-minexpcost before calib: 0.7814259793230413\n",
      "WKappa-minexpcost before adaptation: 0.7717373777750742\n",
      "WKappa-minexpcost after adaptation: 0.8046854124193988\n",
      "on fold 7\n",
      "Accuracy before calib: 0.698719772403983\n",
      "Accuracy before adaptation: 0.689900426742532\n",
      "Accuracy after adaptation: 0.7044096728307254\n",
      "WKappa-minexpcost before calib: 0.7821119875746184\n",
      "WKappa-minexpcost before adaptation: 0.76286800215251\n",
      "WKappa-minexpcost after adaptation: 0.8033654712272633\n",
      "on fold 8\n",
      "Accuracy before calib: 0.6726241134751773\n",
      "Accuracy before adaptation: 0.6831205673758866\n",
      "Accuracy after adaptation: 0.693049645390071\n",
      "WKappa-minexpcost before calib: 0.7857000014186859\n",
      "WKappa-minexpcost before adaptation: 0.7653784354272826\n",
      "WKappa-minexpcost after adaptation: 0.8014485660012354\n",
      "on fold 9\n",
      "Accuracy before calib: 0.668081494057725\n",
      "Accuracy before adaptation: 0.688737973967176\n",
      "Accuracy after adaptation: 0.642331635540464\n",
      "WKappa-minexpcost before calib: 0.786393463656124\n",
      "WKappa-minexpcost before adaptation: 0.7672700124757661\n",
      "WKappa-minexpcost after adaptation: 0.8120047759533666\n",
      "on fold 10\n",
      "Accuracy before calib: 0.6578947368421053\n",
      "Accuracy before adaptation: 0.6810979060554613\n",
      "Accuracy after adaptation: 0.6941143180531976\n",
      "WKappa-minexpcost before calib: 0.7729089864345662\n",
      "WKappa-minexpcost before adaptation: 0.7612115313421566\n",
      "WKappa-minexpcost after adaptation: 0.793757653176034\n",
      "on fold 11\n",
      "Accuracy before calib: 0.6910592255125285\n",
      "Accuracy before adaptation: 0.7007403189066059\n",
      "Accuracy after adaptation: 0.7087129840546698\n",
      "WKappa-minexpcost before calib: 0.7948730599478033\n",
      "WKappa-minexpcost before adaptation: 0.7833030292829042\n",
      "WKappa-minexpcost after adaptation: 0.8195536042828324\n",
      "on fold 12\n",
      "Accuracy before calib: 0.6932916429789653\n",
      "Accuracy before adaptation: 0.6969869243888573\n",
      "Accuracy after adaptation: 0.7029562251279136\n",
      "WKappa-minexpcost before calib: 0.768732297725115\n",
      "WKappa-minexpcost before adaptation: 0.7560555993051253\n",
      "WKappa-minexpcost after adaptation: 0.7900503127493219\n",
      "on fold 13\n",
      "Accuracy before calib: 0.6743327654741624\n",
      "Accuracy before adaptation: 0.6879613855763771\n",
      "Accuracy after adaptation: 0.6927881885292447\n",
      "WKappa-minexpcost before calib: 0.7527052796446477\n",
      "WKappa-minexpcost before adaptation: 0.7321597306335305\n",
      "WKappa-minexpcost after adaptation: 0.7742272222361226\n",
      "on fold 14\n",
      "Accuracy before calib: 0.687358276643991\n",
      "Accuracy before adaptation: 0.7009637188208617\n",
      "Accuracy after adaptation: 0.7145691609977324\n",
      "WKappa-minexpcost before calib: 0.7949002795473233\n",
      "WKappa-minexpcost before adaptation: 0.7767158049794687\n",
      "WKappa-minexpcost after adaptation: 0.8162726641954876\n",
      "on fold 15\n",
      "Accuracy before calib: 0.6836879432624113\n",
      "Accuracy before adaptation: 0.7018439716312057\n",
      "Accuracy after adaptation: 0.7154609929078014\n",
      "WKappa-minexpcost before calib: 0.8132832187555041\n",
      "WKappa-minexpcost before adaptation: 0.7963651538321526\n",
      "WKappa-minexpcost after adaptation: 0.8356057064969358\n",
      "on fold 16\n",
      "Accuracy before calib: 0.691609977324263\n",
      "Accuracy before adaptation: 0.7140022675736961\n",
      "Accuracy after adaptation: 0.7230725623582767\n",
      "WKappa-minexpcost before calib: 0.7715153361466229\n",
      "WKappa-minexpcost before adaptation: 0.7653233920263257\n",
      "WKappa-minexpcost after adaptation: 0.7969851406778442\n",
      "on fold 17\n",
      "Accuracy before calib: 0.681044267877412\n",
      "Accuracy before adaptation: 0.70261066969353\n",
      "Accuracy after adaptation: 0.7119750283768445\n",
      "WKappa-minexpcost before calib: 0.7940342631522802\n",
      "WKappa-minexpcost before adaptation: 0.7804983181627776\n",
      "WKappa-minexpcost after adaptation: 0.8255762482402219\n",
      "on fold 18\n",
      "Accuracy before calib: 0.6642978965321206\n",
      "Accuracy before adaptation: 0.6961341671404206\n",
      "Accuracy after adaptation: 0.6176805002842524\n",
      "WKappa-minexpcost before calib: 0.769298138416132\n",
      "WKappa-minexpcost before adaptation: 0.7562848735160322\n",
      "WKappa-minexpcost after adaptation: 0.7852585441558452\n",
      "on fold 19\n",
      "Accuracy before calib: 0.6736782262649232\n",
      "Accuracy before adaptation: 0.7018192154633315\n",
      "Accuracy after adaptation: 0.713757816941444\n",
      "WKappa-minexpcost before calib: 0.7632868089560791\n",
      "WKappa-minexpcost before adaptation: 0.7478844615757825\n",
      "WKappa-minexpcost after adaptation: 0.781365921180593\n",
      "on fold 20\n",
      "Accuracy before calib: 0.6765873015873016\n",
      "Accuracy before adaptation: 0.6967120181405896\n",
      "Accuracy after adaptation: 0.6984126984126984\n",
      "WKappa-minexpcost before calib: 0.775235129234463\n",
      "WKappa-minexpcost before adaptation: 0.7690788947092253\n",
      "WKappa-minexpcost after adaptation: 0.7952732321987049\n",
      "on fold 21\n",
      "Accuracy before calib: 0.679363274587834\n",
      "Accuracy before adaptation: 0.7080727686185333\n",
      "Accuracy after adaptation: 0.6304718590108016\n",
      "WKappa-minexpcost before calib: 0.814834374608578\n",
      "WKappa-minexpcost before adaptation: 0.8085713094417747\n",
      "WKappa-minexpcost after adaptation: 0.8194778917504048\n",
      "on fold 22\n",
      "Accuracy before calib: 0.6978131212723658\n",
      "Accuracy before adaptation: 0.702357284862255\n",
      "Accuracy after adaptation: 0.7088895200227208\n",
      "WKappa-minexpcost before calib: 0.8048607111790091\n",
      "WKappa-minexpcost before adaptation: 0.7925497631538071\n",
      "WKappa-minexpcost after adaptation: 0.8228450323806319\n",
      "on fold 23\n",
      "Accuracy before calib: 0.6808149405772496\n",
      "Accuracy before adaptation: 0.6983588002263724\n",
      "Accuracy after adaptation: 0.7142048670062252\n",
      "WKappa-minexpcost before calib: 0.7887611264049816\n",
      "WKappa-minexpcost before adaptation: 0.7722984407249139\n",
      "WKappa-minexpcost after adaptation: 0.8100433169643815\n",
      "on fold 24\n",
      "Accuracy before calib: 0.7039042462239954\n",
      "Accuracy before adaptation: 0.713593616414933\n",
      "Accuracy after adaptation: 0.7187232829866058\n",
      "WKappa-minexpcost before calib: 0.7964797313936098\n",
      "WKappa-minexpcost before adaptation: 0.7844080091283788\n",
      "WKappa-minexpcost after adaptation: 0.8191527383135632\n",
      "on fold 25\n",
      "Accuracy before calib: 0.6771070615034168\n",
      "Accuracy before adaptation: 0.6947608200455581\n",
      "Accuracy after adaptation: 0.6978929384965832\n",
      "WKappa-minexpcost before calib: 0.8003931916533966\n",
      "WKappa-minexpcost before adaptation: 0.7919132567239933\n",
      "WKappa-minexpcost after adaptation: 0.8135912018135693\n",
      "on fold 26\n",
      "Accuracy before calib: 0.6790684464640727\n",
      "Accuracy before adaptation: 0.6864527122976427\n",
      "Accuracy after adaptation: 0.7026412950866231\n",
      "WKappa-minexpcost before calib: 0.7678475578925098\n",
      "WKappa-minexpcost before adaptation: 0.7461017582566508\n",
      "WKappa-minexpcost after adaptation: 0.7811226531729045\n",
      "on fold 27\n",
      "Accuracy before calib: 0.6788548752834467\n",
      "Accuracy before adaptation: 0.6933106575963719\n",
      "Accuracy after adaptation: 0.7009637188208617\n",
      "WKappa-minexpcost before calib: 0.7753689237745854\n",
      "WKappa-minexpcost before adaptation: 0.7576887093660435\n",
      "WKappa-minexpcost after adaptation: 0.79307239714157\n",
      "on fold 28\n",
      "Accuracy before calib: 0.6871270247229326\n",
      "Accuracy before adaptation: 0.7036089798238135\n",
      "Accuracy after adaptation: 0.7155441886899687\n",
      "WKappa-minexpcost before calib: 0.77760786271539\n",
      "WKappa-minexpcost before adaptation: 0.7636656938087041\n",
      "WKappa-minexpcost after adaptation: 0.7948302957240703\n",
      "on fold 29\n",
      "Accuracy before calib: 0.6992630385487528\n",
      "Accuracy before adaptation: 0.7026643990929705\n",
      "Accuracy after adaptation: 0.7080498866213152\n",
      "WKappa-minexpcost before calib: 0.7954178449315027\n",
      "WKappa-minexpcost before adaptation: 0.7821356558009375\n",
      "WKappa-minexpcost after adaptation: 0.8130778282064213\n",
      "on fold 30\n",
      "Accuracy before calib: 0.6758132956152758\n",
      "Accuracy before adaptation: 0.6978783592644979\n",
      "Accuracy after adaptation: 0.6953323903818953\n",
      "WKappa-minexpcost before calib: 0.7721754075994776\n",
      "WKappa-minexpcost before adaptation: 0.7552929410931998\n",
      "WKappa-minexpcost after adaptation: 0.790053890561585\n",
      "on fold 31\n",
      "Accuracy before calib: 0.6695059625212947\n",
      "Accuracy before adaptation: 0.6942078364565588\n",
      "Accuracy after adaptation: 0.6805792163543442\n",
      "WKappa-minexpcost before calib: 0.7784441833075388\n",
      "WKappa-minexpcost before adaptation: 0.7733463706352781\n",
      "WKappa-minexpcost after adaptation: 0.8002711227289421\n",
      "on fold 32\n",
      "Accuracy before calib: 0.6757524134014764\n",
      "Accuracy before adaptation: 0.6882453151618398\n",
      "Accuracy after adaptation: 0.6666666666666666\n",
      "WKappa-minexpcost before calib: 0.7862902791933448\n",
      "WKappa-minexpcost before adaptation: 0.7724525546410317\n",
      "WKappa-minexpcost after adaptation: 0.8082931672730563\n",
      "on fold 33\n",
      "Accuracy before calib: 0.6760363429869393\n",
      "Accuracy before adaptation: 0.6959114139693356\n",
      "Accuracy after adaptation: 0.7109596819988643\n",
      "WKappa-minexpcost before calib: 0.8050441116572831\n",
      "WKappa-minexpcost before adaptation: 0.7892339228727683\n",
      "WKappa-minexpcost after adaptation: 0.817168795484567\n",
      "on fold 34\n",
      "Accuracy before calib: 0.672536211303607\n",
      "Accuracy before adaptation: 0.6790684464640727\n",
      "Accuracy after adaptation: 0.6986651519454701\n",
      "WKappa-minexpcost before calib: 0.7908737858162267\n",
      "WKappa-minexpcost before adaptation: 0.7670421459867606\n",
      "WKappa-minexpcost after adaptation: 0.8121690606067269\n",
      "on fold 35\n",
      "Accuracy before calib: 0.665435955694405\n",
      "Accuracy before adaptation: 0.6941209883555808\n",
      "Accuracy after adaptation: 0.6946890088043169\n",
      "WKappa-minexpcost before calib: 0.785880573273199\n",
      "WKappa-minexpcost before adaptation: 0.7716210635167383\n",
      "WKappa-minexpcost after adaptation: 0.8019029694429227\n",
      "on fold 36\n",
      "Accuracy before calib: 0.6859010801591814\n",
      "Accuracy before adaptation: 0.6893121091529278\n",
      "Accuracy after adaptation: 0.7075042637862422\n",
      "WKappa-minexpcost before calib: 0.7650194064541481\n",
      "WKappa-minexpcost before adaptation: 0.7475233703764252\n",
      "WKappa-minexpcost after adaptation: 0.7867322774657992\n",
      "on fold 37\n",
      "Accuracy before calib: 0.6748726655348047\n",
      "Accuracy before adaptation: 0.6859083191850595\n",
      "Accuracy after adaptation: 0.7009054895302773\n",
      "WKappa-minexpcost before calib: 0.7976121576673784\n",
      "WKappa-minexpcost before adaptation: 0.778193548661623\n",
      "WKappa-minexpcost after adaptation: 0.8176946294208405\n",
      "on fold 38\n",
      "Accuracy before calib: 0.6679931779420125\n",
      "Accuracy before adaptation: 0.6861853325753269\n",
      "Accuracy after adaptation: 0.6861853325753269\n",
      "WKappa-minexpcost before calib: 0.7792515625474361\n",
      "WKappa-minexpcost before adaptation: 0.7607096726280334\n",
      "WKappa-minexpcost after adaptation: 0.8016580929342871\n",
      "on fold 39\n",
      "Accuracy before calib: 0.6950455580865603\n",
      "Accuracy before adaptation: 0.681378132118451\n",
      "Accuracy after adaptation: 0.7058656036446469\n",
      "WKappa-minexpcost before calib: 0.7844920043656336\n",
      "WKappa-minexpcost before adaptation: 0.7662973587050744\n",
      "WKappa-minexpcost after adaptation: 0.8044678322232425\n",
      "on fold 40\n",
      "Accuracy before calib: 0.67651235444476\n",
      "Accuracy before adaptation: 0.6793524566884408\n",
      "Accuracy after adaptation: 0.6904288554387958\n",
      "WKappa-minexpcost before calib: 0.7889501946758768\n",
      "WKappa-minexpcost before adaptation: 0.764479759686499\n",
      "WKappa-minexpcost after adaptation: 0.8054240574478675\n",
      "on fold 41\n",
      "Accuracy before calib: 0.6733939738487777\n",
      "Accuracy before adaptation: 0.6881750994883457\n",
      "Accuracy after adaptation: 0.6981239340534394\n",
      "WKappa-minexpcost before calib: 0.7631048193150842\n",
      "WKappa-minexpcost before adaptation: 0.7450732864011335\n",
      "WKappa-minexpcost after adaptation: 0.7865883833202157\n",
      "on fold 42\n",
      "Accuracy before calib: 0.6813808715336729\n",
      "Accuracy before adaptation: 0.6816638370118846\n",
      "Accuracy after adaptation: 0.7051499717034522\n",
      "WKappa-minexpcost before calib: 0.7846386869070769\n",
      "WKappa-minexpcost before adaptation: 0.7647969097278231\n",
      "WKappa-minexpcost after adaptation: 0.8051568529414952\n",
      "on fold 43\n",
      "Accuracy before calib: 0.6887755102040817\n",
      "Accuracy before adaptation: 0.6831065759637188\n",
      "Accuracy after adaptation: 0.7026643990929705\n",
      "WKappa-minexpcost before calib: 0.782307471771348\n",
      "WKappa-minexpcost before adaptation: 0.7532039446880964\n",
      "WKappa-minexpcost after adaptation: 0.7968244147556671\n",
      "on fold 44\n",
      "Accuracy before calib: 0.6791489361702128\n",
      "Accuracy before adaptation: 0.6862411347517731\n",
      "Accuracy after adaptation: 0.6774468085106383\n",
      "WKappa-minexpcost before calib: 0.7641475410714454\n",
      "WKappa-minexpcost before adaptation: 0.7433299906541418\n",
      "WKappa-minexpcost after adaptation: 0.7816785598170678\n",
      "on fold 45\n",
      "Accuracy before calib: 0.6984938903097471\n",
      "Accuracy before adaptation: 0.7010514350667804\n",
      "Accuracy after adaptation: 0.707871554418869\n",
      "WKappa-minexpcost before calib: 0.79020393809125\n",
      "WKappa-minexpcost before adaptation: 0.7753529968216262\n",
      "WKappa-minexpcost after adaptation: 0.8062682899057142\n",
      "on fold 46\n",
      "Accuracy before calib: 0.6859410430839002\n",
      "Accuracy before adaptation: 0.7009637188208617\n",
      "Accuracy after adaptation: 0.7066326530612245\n",
      "WKappa-minexpcost before calib: 0.8048600036461533\n",
      "WKappa-minexpcost before adaptation: 0.7978339908035117\n",
      "WKappa-minexpcost after adaptation: 0.812524631361512\n",
      "on fold 47\n",
      "Accuracy before calib: 0.675177304964539\n",
      "Accuracy before adaptation: 0.6904964539007092\n",
      "Accuracy after adaptation: 0.7021276595744681\n",
      "WKappa-minexpcost before calib: 0.7523796452356435\n",
      "WKappa-minexpcost before adaptation: 0.7381365399628278\n",
      "WKappa-minexpcost after adaptation: 0.7818892460429798\n",
      "on fold 48\n",
      "Accuracy before calib: 0.6802490096208262\n",
      "Accuracy before adaptation: 0.6893039049235993\n",
      "Accuracy after adaptation: 0.6980758347481607\n",
      "WKappa-minexpcost before calib: 0.783208487993534\n",
      "WKappa-minexpcost before adaptation: 0.7672693296024186\n",
      "WKappa-minexpcost after adaptation: 0.8090345170247022\n",
      "on fold 49\n",
      "Accuracy before calib: 0.6760045274476514\n",
      "Accuracy before adaptation: 0.687889077532541\n",
      "Accuracy after adaptation: 0.6904357668364459\n",
      "WKappa-minexpcost before calib: 0.7861183279195899\n",
      "WKappa-minexpcost before adaptation: 0.7730939194521885\n",
      "WKappa-minexpcost after adaptation: 0.8087842338811411\n",
      "abstainer settings tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings uncalib_mcdrpreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings tempscale-calib_mcdrpreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings tempscalebiascor-calib_mcdrpreds_imbalanced_adapted\n",
      "on fold 0\n",
      "Accuracy before calib: 0.6825396825396826\n",
      "Accuracy before adaptation: 0.6836734693877551\n",
      "Accuracy after adaptation: 0.6950113378684807\n",
      "WKappa-minexpcost before calib: 0.7758723245378054\n",
      "WKappa-minexpcost before adaptation: 0.7647034215969801\n",
      "WKappa-minexpcost after adaptation: 0.8077129613681124\n",
      "on fold 1\n",
      "Accuracy before calib: 0.675177304964539\n",
      "Accuracy before adaptation: 0.6851063829787234\n",
      "Accuracy after adaptation: 0.6700709219858156\n",
      "WKappa-minexpcost before calib: 0.7945282086915755\n",
      "WKappa-minexpcost before adaptation: 0.7788171329762152\n",
      "WKappa-minexpcost after adaptation: 0.8158673230459961\n",
      "on fold 2\n",
      "Accuracy before calib: 0.6986689323137921\n",
      "Accuracy before adaptation: 0.7037666383460776\n",
      "Accuracy after adaptation: 0.6964032851883319\n",
      "WKappa-minexpcost before calib: 0.8084737969725406\n",
      "WKappa-minexpcost before adaptation: 0.8054185406072623\n",
      "WKappa-minexpcost after adaptation: 0.8167629771938085\n",
      "on fold 3\n",
      "Accuracy before calib: 0.7120181405895691\n",
      "Accuracy before adaptation: 0.7094671201814059\n",
      "Accuracy after adaptation: 0.7256235827664399\n",
      "WKappa-minexpcost before calib: 0.8059345930732544\n",
      "WKappa-minexpcost before adaptation: 0.7934848261143846\n",
      "WKappa-minexpcost after adaptation: 0.8320969192930268\n",
      "on fold 4\n",
      "Accuracy before calib: 0.6825396825396826\n",
      "Accuracy before adaptation: 0.6933106575963719\n",
      "Accuracy after adaptation: 0.69359410430839\n",
      "WKappa-minexpcost before calib: 0.7783579945939179\n",
      "WKappa-minexpcost before adaptation: 0.7705902935028067\n",
      "WKappa-minexpcost after adaptation: 0.7962003211622296\n",
      "on fold 5\n",
      "Accuracy before calib: 0.6780141843971631\n",
      "Accuracy before adaptation: 0.6836879432624113\n",
      "Accuracy after adaptation: 0.6933333333333334\n",
      "WKappa-minexpcost before calib: 0.7583019999753051\n",
      "WKappa-minexpcost before adaptation: 0.7388861311392232\n",
      "WKappa-minexpcost after adaptation: 0.7812901755418974\n",
      "on fold 6\n",
      "Accuracy before calib: 0.687588753195115\n",
      "Accuracy before adaptation: 0.6929849474581085\n",
      "Accuracy after adaptation: 0.694973019028685\n",
      "WKappa-minexpcost before calib: 0.7839174850083444\n",
      "WKappa-minexpcost before adaptation: 0.7741639244321361\n",
      "WKappa-minexpcost after adaptation: 0.8103995416368561\n",
      "on fold 7\n",
      "Accuracy before calib: 0.6995732574679943\n",
      "Accuracy before adaptation: 0.6884779516358464\n",
      "Accuracy after adaptation: 0.7049786628733997\n",
      "WKappa-minexpcost before calib: 0.7850149838250865\n",
      "WKappa-minexpcost before adaptation: 0.7637751574759448\n",
      "WKappa-minexpcost after adaptation: 0.8095765981043119\n",
      "on fold 8\n",
      "Accuracy before calib: 0.6768794326241134\n",
      "Accuracy before adaptation: 0.6817021276595745\n",
      "Accuracy after adaptation: 0.6916312056737589\n",
      "WKappa-minexpcost before calib: 0.7859925321631491\n",
      "WKappa-minexpcost before adaptation: 0.7685640463351248\n",
      "WKappa-minexpcost after adaptation: 0.8045021764161305\n",
      "on fold 9\n",
      "Accuracy before calib: 0.6714770797962648\n",
      "Accuracy before adaptation: 0.6861912846632711\n",
      "Accuracy after adaptation: 0.6420486700622524\n",
      "WKappa-minexpcost before calib: 0.787376860412062\n",
      "WKappa-minexpcost before adaptation: 0.7682850921504795\n",
      "WKappa-minexpcost after adaptation: 0.8164151226212932\n",
      "on fold 10\n",
      "Accuracy before calib: 0.6624221844934918\n",
      "Accuracy before adaptation: 0.678834182229768\n",
      "Accuracy after adaptation: 0.6992076966610073\n",
      "WKappa-minexpcost before calib: 0.7734288874618256\n",
      "WKappa-minexpcost before adaptation: 0.763322382736469\n",
      "WKappa-minexpcost after adaptation: 0.7966454751124712\n",
      "on fold 11\n",
      "Accuracy before calib: 0.6927676537585421\n",
      "Accuracy before adaptation: 0.7004555808656037\n",
      "Accuracy after adaptation: 0.7061503416856492\n",
      "WKappa-minexpcost before calib: 0.7957840737532597\n",
      "WKappa-minexpcost before adaptation: 0.788682774374018\n",
      "WKappa-minexpcost after adaptation: 0.8224210866563276\n",
      "on fold 12\n",
      "Accuracy before calib: 0.6992609437180216\n",
      "Accuracy before adaptation: 0.6944286526435475\n",
      "Accuracy after adaptation: 0.7026719727117681\n",
      "WKappa-minexpcost before calib: 0.771668310516917\n",
      "WKappa-minexpcost before adaptation: 0.7593283221765469\n",
      "WKappa-minexpcost after adaptation: 0.7937637400262305\n",
      "on fold 13\n",
      "Accuracy before calib: 0.6794434980124929\n",
      "Accuracy before adaptation: 0.6862578080636003\n",
      "Accuracy after adaptation: 0.6885292447473027\n",
      "WKappa-minexpcost before calib: 0.7549005352304556\n",
      "WKappa-minexpcost before adaptation: 0.7359413406414188\n",
      "WKappa-minexpcost after adaptation: 0.7781392609438242\n",
      "on fold 14\n",
      "Accuracy before calib: 0.6927437641723356\n",
      "Accuracy before adaptation: 0.6998299319727891\n",
      "Accuracy after adaptation: 0.7148526077097506\n",
      "WKappa-minexpcost before calib: 0.7960416112342146\n",
      "WKappa-minexpcost before adaptation: 0.7781567233728575\n",
      "WKappa-minexpcost after adaptation: 0.8192579255116454\n",
      "on fold 15\n",
      "Accuracy before calib: 0.6885106382978723\n",
      "Accuracy before adaptation: 0.699290780141844\n",
      "Accuracy after adaptation: 0.7205673758865249\n",
      "WKappa-minexpcost before calib: 0.813078532845664\n",
      "WKappa-minexpcost before adaptation: 0.7981220113232487\n",
      "WKappa-minexpcost after adaptation: 0.8395678848185806\n",
      "on fold 16\n",
      "Accuracy before calib: 0.6978458049886621\n",
      "Accuracy before adaptation: 0.715702947845805\n",
      "Accuracy after adaptation: 0.7168367346938775\n",
      "WKappa-minexpcost before calib: 0.7737477462340299\n",
      "WKappa-minexpcost before adaptation: 0.7682097506935868\n",
      "WKappa-minexpcost after adaptation: 0.8023356311752865\n",
      "on fold 17\n",
      "Accuracy before calib: 0.6861520998864926\n",
      "Accuracy before adaptation: 0.7006242905788876\n",
      "Accuracy after adaptation: 0.7145289443813848\n",
      "WKappa-minexpcost before calib: 0.7948736844341988\n",
      "WKappa-minexpcost before adaptation: 0.7840573819675435\n",
      "WKappa-minexpcost after adaptation: 0.8267800704291705\n",
      "on fold 18\n",
      "Accuracy before calib: 0.6694144400227402\n",
      "Accuracy before adaptation: 0.6961341671404206\n",
      "Accuracy after adaptation: 0.5963615690733371\n",
      "WKappa-minexpcost before calib: 0.7713196916007541\n",
      "WKappa-minexpcost before adaptation: 0.7590961877653266\n",
      "WKappa-minexpcost after adaptation: 0.7900273382411085\n",
      "on fold 19\n",
      "Accuracy before calib: 0.6770892552586697\n",
      "Accuracy before adaptation: 0.7032404775440592\n",
      "Accuracy after adaptation: 0.7123365548607163\n",
      "WKappa-minexpcost before calib: 0.7656891147174582\n",
      "WKappa-minexpcost before adaptation: 0.7517366432396573\n",
      "WKappa-minexpcost after adaptation: 0.7871591425561477\n",
      "on fold 20\n",
      "Accuracy before calib: 0.6797052154195011\n",
      "Accuracy before adaptation: 0.6947278911564626\n",
      "Accuracy after adaptation: 0.6907596371882087\n",
      "WKappa-minexpcost before calib: 0.7793599709650062\n",
      "WKappa-minexpcost before adaptation: 0.7741063175527917\n",
      "WKappa-minexpcost after adaptation: 0.7997256249028786\n",
      "on fold 21\n",
      "Accuracy before calib: 0.6841955656623081\n",
      "Accuracy before adaptation: 0.70636725412166\n",
      "Accuracy after adaptation: 0.626492325184764\n",
      "WKappa-minexpcost before calib: 0.8168341598593254\n",
      "WKappa-minexpcost before adaptation: 0.809609872234923\n",
      "WKappa-minexpcost after adaptation: 0.8213950849316113\n",
      "on fold 22\n",
      "Accuracy before calib: 0.7003692132916784\n",
      "Accuracy before adaptation: 0.7003692132916784\n",
      "Accuracy after adaptation: 0.7054813973303039\n",
      "WKappa-minexpcost before calib: 0.8076510346307051\n",
      "WKappa-minexpcost before adaptation: 0.7949710985581221\n",
      "WKappa-minexpcost after adaptation: 0.8243761316960205\n",
      "on fold 23\n",
      "Accuracy before calib: 0.6844934917940011\n",
      "Accuracy before adaptation: 0.6969439728353141\n",
      "Accuracy after adaptation: 0.7113752122241087\n",
      "WKappa-minexpcost before calib: 0.7912768964729611\n",
      "WKappa-minexpcost before adaptation: 0.776010634777756\n",
      "WKappa-minexpcost after adaptation: 0.8152623158832153\n",
      "on fold 24\n",
      "Accuracy before calib: 0.7093188942718723\n",
      "Accuracy before adaptation: 0.7130236534625249\n",
      "Accuracy after adaptation: 0.7204331718438302\n",
      "WKappa-minexpcost before calib: 0.7985493494701741\n",
      "WKappa-minexpcost before adaptation: 0.7873857119934399\n",
      "WKappa-minexpcost after adaptation: 0.8213567178143686\n",
      "on fold 25\n",
      "Accuracy before calib: 0.6796697038724373\n",
      "Accuracy before adaptation: 0.6964692482915718\n",
      "Accuracy after adaptation: 0.6939066059225513\n",
      "WKappa-minexpcost before calib: 0.8006737292793022\n",
      "WKappa-minexpcost before adaptation: 0.7944579680054058\n",
      "WKappa-minexpcost after adaptation: 0.8155316680487388\n",
      "on fold 26\n",
      "Accuracy before calib: 0.6827605793808577\n",
      "Accuracy before adaptation: 0.6847486509514342\n",
      "Accuracy after adaptation: 0.7029253053109912\n",
      "WKappa-minexpcost before calib: 0.7688023634112238\n",
      "WKappa-minexpcost before adaptation: 0.7506263014879262\n",
      "WKappa-minexpcost after adaptation: 0.7886509056803467\n",
      "on fold 27\n",
      "Accuracy before calib: 0.6819727891156463\n",
      "Accuracy before adaptation: 0.69359410430839\n",
      "Accuracy after adaptation: 0.7040816326530612\n",
      "WKappa-minexpcost before calib: 0.7768515002924108\n",
      "WKappa-minexpcost before adaptation: 0.763081497320331\n",
      "WKappa-minexpcost after adaptation: 0.7968431372402502\n",
      "on fold 28\n",
      "Accuracy before calib: 0.6930946291560103\n",
      "Accuracy before adaptation: 0.7055981813015061\n",
      "Accuracy after adaptation: 0.7158283603296391\n",
      "WKappa-minexpcost before calib: 0.7803077634791797\n",
      "WKappa-minexpcost before adaptation: 0.7672832572336186\n",
      "WKappa-minexpcost after adaptation: 0.7973657814076348\n",
      "on fold 29\n",
      "Accuracy before calib: 0.703514739229025\n",
      "Accuracy before adaptation: 0.7020975056689343\n",
      "Accuracy after adaptation: 0.7077664399092971\n",
      "WKappa-minexpcost before calib: 0.7969625748085939\n",
      "WKappa-minexpcost before adaptation: 0.7822051787599051\n",
      "WKappa-minexpcost after adaptation: 0.8163461795619267\n",
      "on fold 30\n",
      "Accuracy before calib: 0.6794908062234795\n",
      "Accuracy before adaptation: 0.695898161244696\n",
      "Accuracy after adaptation: 0.6964639321074965\n",
      "WKappa-minexpcost before calib: 0.7733978125294655\n",
      "WKappa-minexpcost before adaptation: 0.758342500188632\n",
      "WKappa-minexpcost after adaptation: 0.7936059506878376\n",
      "on fold 31\n",
      "Accuracy before calib: 0.6731970471323112\n",
      "Accuracy before adaptation: 0.6936399772856332\n",
      "Accuracy after adaptation: 0.6783077796706417\n",
      "WKappa-minexpcost before calib: 0.7817906487166362\n",
      "WKappa-minexpcost before adaptation: 0.7755871456627004\n",
      "WKappa-minexpcost after adaptation: 0.8000212637028847\n",
      "on fold 32\n",
      "Accuracy before calib: 0.6791595684270301\n",
      "Accuracy before adaptation: 0.6862578080636003\n",
      "Accuracy after adaptation: 0.6709256104486088\n",
      "WKappa-minexpcost before calib: 0.7884774699796586\n",
      "WKappa-minexpcost before adaptation: 0.7771113049322957\n",
      "WKappa-minexpcost after adaptation: 0.8133168305452955\n",
      "on fold 33\n",
      "Accuracy before calib: 0.6825667234525837\n",
      "Accuracy before adaptation: 0.6944917660420216\n",
      "Accuracy after adaptation: 0.7106757524134015\n",
      "WKappa-minexpcost before calib: 0.8045257388417112\n",
      "WKappa-minexpcost before adaptation: 0.7913810751997129\n",
      "WKappa-minexpcost after adaptation: 0.8176154479833792\n",
      "on fold 34\n",
      "Accuracy before calib: 0.6796364669128089\n",
      "Accuracy before adaptation: 0.6785004260153366\n",
      "Accuracy after adaptation: 0.6975291110479978\n",
      "WKappa-minexpcost before calib: 0.7919890629542968\n",
      "WKappa-minexpcost before adaptation: 0.7662443632079933\n",
      "WKappa-minexpcost after adaptation: 0.8156722005216621\n",
      "on fold 35\n",
      "Accuracy before calib: 0.6679920477137177\n",
      "Accuracy before adaptation: 0.6935529679068446\n",
      "Accuracy after adaptation: 0.6921329167850042\n",
      "WKappa-minexpcost before calib: 0.7848469046745844\n",
      "WKappa-minexpcost before adaptation: 0.7741771929931065\n",
      "WKappa-minexpcost after adaptation: 0.8026889504443696\n",
      "on fold 36\n",
      "Accuracy before calib: 0.6881750994883457\n",
      "Accuracy before adaptation: 0.6876065946560546\n",
      "Accuracy after adaptation: 0.70636725412166\n",
      "WKappa-minexpcost before calib: 0.7667861247488184\n",
      "WKappa-minexpcost before adaptation: 0.7491591678153965\n",
      "WKappa-minexpcost after adaptation: 0.7928953104285535\n",
      "on fold 37\n",
      "Accuracy before calib: 0.6785512167515563\n",
      "Accuracy before adaptation: 0.6839275608375778\n",
      "Accuracy after adaptation: 0.6989247311827957\n",
      "WKappa-minexpcost before calib: 0.798878124773473\n",
      "WKappa-minexpcost before adaptation: 0.7808574593769508\n",
      "WKappa-minexpcost after adaptation: 0.819356573129128\n",
      "on fold 38\n",
      "Accuracy before calib: 0.6688459351904491\n",
      "Accuracy before adaptation: 0.6861853325753269\n",
      "Accuracy after adaptation: 0.6824900511654349\n",
      "WKappa-minexpcost before calib: 0.7821882248906626\n",
      "WKappa-minexpcost before adaptation: 0.7635534587050776\n",
      "WKappa-minexpcost after adaptation: 0.8028151903687388\n",
      "on fold 39\n",
      "Accuracy before calib: 0.6978929384965832\n",
      "Accuracy before adaptation: 0.683371298405467\n",
      "Accuracy after adaptation: 0.7075740318906606\n",
      "WKappa-minexpcost before calib: 0.7868557842295841\n",
      "WKappa-minexpcost before adaptation: 0.7674031597715034\n",
      "WKappa-minexpcost after adaptation: 0.8057640957063189\n",
      "on fold 40\n",
      "Accuracy before calib: 0.6819085487077535\n",
      "Accuracy before adaptation: 0.6810565180346493\n",
      "Accuracy after adaptation: 0.6867367225220108\n",
      "WKappa-minexpcost before calib: 0.7914809204548399\n",
      "WKappa-minexpcost before adaptation: 0.7667241011632249\n",
      "WKappa-minexpcost after adaptation: 0.8099762525946552\n",
      "on fold 41\n",
      "Accuracy before calib: 0.6776577600909608\n",
      "Accuracy before adaptation: 0.686753837407618\n",
      "Accuracy after adaptation: 0.6952814098919841\n",
      "WKappa-minexpcost before calib: 0.7643361116514026\n",
      "WKappa-minexpcost before adaptation: 0.7475524632101147\n",
      "WKappa-minexpcost after adaptation: 0.7909878682518999\n",
      "on fold 42\n",
      "Accuracy before calib: 0.6853423882286361\n",
      "Accuracy before adaptation: 0.6774193548387096\n",
      "Accuracy after adaptation: 0.7079796264855688\n",
      "WKappa-minexpcost before calib: 0.7872074639603076\n",
      "WKappa-minexpcost before adaptation: 0.7665743743184241\n",
      "WKappa-minexpcost after adaptation: 0.8088679473515151\n",
      "on fold 43\n",
      "Accuracy before calib: 0.6955782312925171\n",
      "Accuracy before adaptation: 0.6856575963718821\n",
      "Accuracy after adaptation: 0.7020975056689343\n",
      "WKappa-minexpcost before calib: 0.783928174690171\n",
      "WKappa-minexpcost before adaptation: 0.7524340688211322\n",
      "WKappa-minexpcost after adaptation: 0.8025907862247017\n",
      "on fold 44\n",
      "Accuracy before calib: 0.6822695035460993\n",
      "Accuracy before adaptation: 0.6825531914893617\n",
      "Accuracy after adaptation: 0.6780141843971631\n",
      "WKappa-minexpcost before calib: 0.7659319078393001\n",
      "WKappa-minexpcost before adaptation: 0.742724236823852\n",
      "WKappa-minexpcost after adaptation: 0.7884898443438371\n",
      "on fold 45\n",
      "Accuracy before calib: 0.7016197783461211\n",
      "Accuracy before adaptation: 0.7016197783461211\n",
      "Accuracy after adaptation: 0.7087240693378801\n",
      "WKappa-minexpcost before calib: 0.7917205908605338\n",
      "WKappa-minexpcost before adaptation: 0.7814556525189347\n",
      "WKappa-minexpcost after adaptation: 0.8079640942371827\n",
      "on fold 46\n",
      "Accuracy before calib: 0.691609977324263\n",
      "Accuracy before adaptation: 0.7012471655328798\n",
      "Accuracy after adaptation: 0.7106009070294784\n",
      "WKappa-minexpcost before calib: 0.8061224181414832\n",
      "WKappa-minexpcost before adaptation: 0.8013988333942288\n",
      "WKappa-minexpcost after adaptation: 0.8148099838011358\n",
      "on fold 47\n",
      "Accuracy before calib: 0.6811347517730496\n",
      "Accuracy before adaptation: 0.6890780141843972\n",
      "Accuracy after adaptation: 0.7060992907801419\n",
      "WKappa-minexpcost before calib: 0.7571079736117102\n",
      "WKappa-minexpcost before adaptation: 0.7416326543458676\n",
      "WKappa-minexpcost after adaptation: 0.7874293918541267\n",
      "on fold 48\n",
      "Accuracy before calib: 0.6839275608375778\n",
      "Accuracy before adaptation: 0.6873231465761177\n",
      "Accuracy after adaptation: 0.6963780418788907\n",
      "WKappa-minexpcost before calib: 0.7846385040707203\n",
      "WKappa-minexpcost before adaptation: 0.7716349046294492\n",
      "WKappa-minexpcost after adaptation: 0.8109359510120306\n",
      "on fold 49\n",
      "Accuracy before calib: 0.6777023203169213\n",
      "Accuracy before adaptation: 0.6836445953593662\n",
      "Accuracy after adaptation: 0.6861912846632711\n",
      "WKappa-minexpcost before calib: 0.7883269655614097\n",
      "WKappa-minexpcost before adaptation: 0.7778225533420893\n",
      "WKappa-minexpcost after adaptation: 0.8136404816461165\n",
      "abstainer settings tempscalebiascor-calib_mcdrpreds_imbalanced_unadapted\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n"
     ]
    }
   ],
   "source": [
    "import abstention\n",
    "reload(abstention.abstention)\n",
    "reload(abstention.calibration)\n",
    "from abstention.calibration import (compute_ece, compute_ece_with_bins,\n",
    "                                    TempScaling,\n",
    "                                    BiasCorrectionWrapper,\n",
    "                                    EMImbalanceAdapter,\n",
    "                                    EMBiasCorrectorFactory)\n",
    "from abstention.abstention import (weighted_kappa_metric,\n",
    "                                   WeightedKappa, DistMaxClassProbFromOne,\n",
    "                                   Entropy, Uncertainty)\n",
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "def inverse_softmax(preds):\n",
    "    return np.log(preds) - np.mean(np.log(preds),axis=1)[:,None]\n",
    "\n",
    "quadratic_weights = np.array([[(i-j)**2 for i in range(5)]\n",
    "                             for j in range(5)])\n",
    "\n",
    "AbstainerSettings = namedtuple(\"AbstainerSettings\",\n",
    "                               [\"name\",\n",
    "                                \"abstainer_factories\",\n",
    "                                \"preds_lookup\",\n",
    "                                \"predsamples_lookup\",\n",
    "                                \"calibrator\",\n",
    "                                \"imbalance_subsampling\",\n",
    "                                \"imbalance_adapter\"])\n",
    "\n",
    "abstainer_factories = [\n",
    "        (\"expected_delta_argmaxWeightedKappa\", WeightedKappa(\n",
    "            weights=quadratic_weights, verbose=False, mode='argmax')),\n",
    "        (\"expected_delta_optimWeightedKappa\", WeightedKappa(\n",
    "            weights=quadratic_weights, verbose=False, mode='optim')),\n",
    "        (\"dist_maxclass_prob_from_one\", DistMaxClassProbFromOne()),\n",
    "        (\"entropy\", Entropy()),\n",
    "        (\"variance\", Uncertainty())]\n",
    "abstention_fractions = [0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "abstainer_settings_list = [\n",
    "    AbstainerSettings(\n",
    "        name=\"uncalib_weightrescalepreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=None,\n",
    "        imbalance_subsampling=None, imbalance_adapter=None),\n",
    "    AbstainerSettings(\n",
    "        name=\"tempscale-calib_weightrescalepreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=TempScaling(ece_bins=15, verbose=False),\n",
    "        imbalance_subsampling=None, imbalance_adapter=None),\n",
    "    AbstainerSettings(\n",
    "        name=\"tempscalebiascor-calib_weightrescalepreds_imbalanced_adapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=TempScaling(ece_bins=15, verbose=False,\n",
    "                               bias_positions=[0,1,2,3,4]),\n",
    "        imbalance_subsampling=[1, 2, 5, 8, 8], #these are upsample factors\n",
    "        imbalance_adapter=EMImbalanceAdapter(verbose=False)),\n",
    "    AbstainerSettings(\n",
    "        name=\"tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=TempScaling(ece_bins=15, verbose=False,\n",
    "                               bias_positions=[0,1,2,3,4]),\n",
    "        imbalance_subsampling=[1, 2, 5, 8, 8], #these are upsample factors\n",
    "        imbalance_adapter=None),\n",
    "    AbstainerSettings(\n",
    "        name=\"uncalib_mcdrpreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=None,\n",
    "        imbalance_subsampling=None, imbalance_adapter=None),\n",
    "    AbstainerSettings(\n",
    "        name=\"tempscale-calib_mcdrpreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=TempScaling(ece_bins=15, verbose=False),\n",
    "        imbalance_subsampling=None, imbalance_adapter=None),\n",
    "    AbstainerSettings(\n",
    "        name=\"tempscalebiascor-calib_mcdrpreds_imbalanced_adapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=TempScaling(ece_bins=15, verbose=False,\n",
    "                               bias_positions=[0,1,2,3,4]),\n",
    "        imbalance_subsampling=[1, 2, 5, 8, 8], #these are upsample factors\n",
    "        imbalance_adapter=EMImbalanceAdapter(verbose=False)),\n",
    "    AbstainerSettings(\n",
    "        name=\"tempscalebiascor-calib_mcdrpreds_imbalanced_unadapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        calibrator=TempScaling(ece_bins=15, verbose=False,\n",
    "                               bias_positions=[0,1,2,3,4]),\n",
    "        imbalance_subsampling=[1, 2, 5, 8, 8], #these are upsample factors\n",
    "        imbalance_adapter=None),\n",
    "]\n",
    "\n",
    "num_folds = 50\n",
    "\n",
    "settingsname_to_metric_to_fraction_to_method_to_perfs = {}\n",
    "settingsname_to_metric_to_baselineperfs = {}\n",
    "\n",
    "for abstainer_settings in abstainer_settings_list:\n",
    "    \n",
    "    settings_name = abstainer_settings.name\n",
    "    print(\"abstainer settings\", settings_name)\n",
    "    abstainer_factories = abstainer_settings.abstainer_factories\n",
    "    preds_lookup = abstainer_settings.preds_lookup\n",
    "    predsamples_lookup = abstainer_settings.predsamples_lookup\n",
    "    calibrator = abstainer_settings.calibrator\n",
    "    imbalance_subsampling = abstainer_settings.imbalance_subsampling\n",
    "    imbalance_adapter = abstainer_settings.imbalance_adapter\n",
    "    \n",
    "    metric_to_fraction_to_method_to_perfs =\\\n",
    "        defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    settingsname_to_metric_to_fraction_to_method_to_perfs[settings_name] =\\\n",
    "        metric_to_fraction_to_method_to_perfs\n",
    "    metric_to_baselineperfs = defaultdict(list)   \n",
    "    settingsname_to_metric_to_baselineperfs[settings_name] =\\\n",
    "        metric_to_baselineperfs\n",
    "    \n",
    "    for fold_number in range(num_folds):\n",
    "        print(\"on fold\",fold_number)\n",
    "\n",
    "        np.random.seed(fold_number*1000)\n",
    "        random.seed(fold_number*1000)\n",
    "        #the data is in pairs of (left eye, right eye) per patient (entry for\n",
    "        # the right eye comes after the entry for the left eye); hence, the number of\n",
    "        # unique patients is 0.5*len(valid_labels)\n",
    "        patient_id_ordering = list(range(int(0.5*len(valid_labels))))\n",
    "        np.random.shuffle(patient_id_ordering)\n",
    "\n",
    "        pseudovalid_uncalib_preds = []\n",
    "        pseudotest_uncalib_preds = []\n",
    "        pseudovalid_uncalib_predsamples = []\n",
    "        pseudotest_uncalib_predsamples = []\n",
    "        pseudovalid_labels = []\n",
    "        pseudotest_labels = []\n",
    "        pseudovalid_label_counts = np.zeros(5)\n",
    "        pseudotest_label_counts = np.zeros(5)\n",
    "        for i in patient_id_ordering:\n",
    "            left_eye_label = valid_labels[2*i]\n",
    "            right_eye_label = valid_labels[(2*i)+1]\n",
    "            most_diseased_label = max(np.argmax(left_eye_label),\n",
    "                                      np.argmax(right_eye_label))\n",
    "            if (pseudovalid_label_counts[most_diseased_label] <\n",
    "                pseudotest_label_counts[most_diseased_label]):\n",
    "                in_test = False\n",
    "                append_to_uncalib_preds = pseudovalid_uncalib_preds\n",
    "                append_to_uncalib_predsamples = pseudovalid_uncalib_predsamples\n",
    "                append_to_labels = pseudovalid_labels\n",
    "                append_to_label_counts = pseudovalid_label_counts\n",
    "            else:\n",
    "                in_test = True\n",
    "                append_to_uncalib_preds = pseudotest_uncalib_preds\n",
    "                append_to_uncalib_predsamples = pseudotest_uncalib_predsamples\n",
    "                append_to_labels = pseudotest_labels\n",
    "                append_to_label_counts = pseudotest_label_counts\n",
    "            \n",
    "            append_to_label_counts += valid_labels[2*i]\n",
    "            append_to_label_counts += valid_labels[(2*i)+1]\n",
    "            for parent_folder_idx,parent_folder in enumerate(parent_folders):\n",
    "                if ((not in_test) or (imbalance_subsampling is None) or\n",
    "                    imbalance_subsampling[np.argmax(valid_labels[2*i])] > parent_folder_idx):\n",
    "                    append_to_labels.append(valid_labels[2*i])\n",
    "                    append_to_uncalib_preds.append(\n",
    "                            preds_lookup[parent_folder][2*i])\n",
    "                    append_to_uncalib_predsamples.append(\n",
    "                        predsamples_lookup[parent_folder][:,(2*i)])                    \n",
    "                if ((not in_test) or (imbalance_subsampling is None) or\n",
    "                    imbalance_subsampling[np.argmax(valid_labels[(2*i) + 1])] > parent_folder_idx): \n",
    "                    append_to_labels.append(valid_labels[(2*i)+1])\n",
    "                    append_to_uncalib_preds.append(\n",
    "                        preds_lookup[parent_folder][(2*i)+1])\n",
    "                    append_to_uncalib_predsamples.append(\n",
    "                        predsamples_lookup[parent_folder][:,(2*i)+1])\n",
    "                \n",
    "        pseudovalid_uncalib_preds = np.array(pseudovalid_uncalib_preds)\n",
    "        pseudotest_uncalib_preds = np.array(pseudotest_uncalib_preds)\n",
    "        pseudovalid_uncalib_pred_logits = inverse_softmax(pseudovalid_uncalib_preds)\n",
    "        pseudotest_uncalib_pred_logits = inverse_softmax(pseudotest_uncalib_preds)\n",
    "        pseudovalid_uncalib_predsamples = np.array(pseudovalid_uncalib_predsamples).transpose((1,0,2))\n",
    "        pseudotest_uncalib_predsamples = np.array(pseudotest_uncalib_predsamples).transpose((1,0,2))\n",
    "        pseudovalid_uncalib_predsamples_logits = np.array([\n",
    "                inverse_softmax(x) for x in pseudovalid_uncalib_predsamples])        \n",
    "        pseudotest_uncalib_predsamples_logits = np.array([\n",
    "                inverse_softmax(x) for x in pseudotest_uncalib_predsamples])\n",
    "        pseudovalid_labels = np.array(pseudovalid_labels) \n",
    "        pseudotest_labels = np.array(pseudotest_labels)\n",
    "        \n",
    "        \n",
    "        if (calibrator is not None):\n",
    "            the_calibrator = calibrator(\n",
    "                                valid_preacts=pseudovalid_uncalib_pred_logits,\n",
    "                                valid_labels=pseudovalid_labels)\n",
    "            pseudovalid_calib_preds = the_calibrator(pseudovalid_uncalib_pred_logits)\n",
    "            pseudotest_calib_preds = the_calibrator(pseudotest_uncalib_pred_logits)\n",
    "            \n",
    "            \n",
    "            \"\"\"print(\"(Valid) Accuracy before calib:\",\n",
    "                  np.mean(np.argmax(pseudovalid_uncalib_preds,axis=-1)\n",
    "                          ==np.argmax(pseudovalid_labels,axis=-1)))\n",
    "            print(\"(Valid) Accuracy after calib:\",\n",
    "                  np.mean(np.argmax(pseudovalid_calib_preds,axis=-1)\n",
    "                          ==np.argmax(pseudovalid_labels,axis=-1)))\n",
    "            print(\"(Valid) WKappa-minexpcost before calib:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=pseudovalid_uncalib_preds,\n",
    "                        true_labels=pseudovalid_labels,\n",
    "                        weights=quadratic_weights,\n",
    "                        mode='optim'))\n",
    "            print(\"(Valid) WKappa-minexpcost after calib:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=pseudovalid_calib_preds,\n",
    "                        true_labels=pseudovalid_labels,\n",
    "                        weights=quadratic_weights,\n",
    "                        mode='optim'))\n",
    "            print(\"(Test) Accuracy before calib:\",\n",
    "                  np.mean(np.argmax(pseudotest_uncalib_preds,axis=-1)\n",
    "                          ==np.argmax(pseudotest_labels,axis=-1)))\n",
    "            print(\"(Test) Accuracy after calib:\",\n",
    "                  np.mean(np.argmax(pseudotest_calib_preds,axis=-1)\n",
    "                          ==np.argmax(pseudotest_labels,axis=-1)))\n",
    "            print(\"(Test) WKappa-minexpcost before calib:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=pseudotest_uncalib_preds,\n",
    "                        true_labels=pseudotest_labels,\n",
    "                        weights=quadratic_weights,\n",
    "                        mode='optim'))\n",
    "            print(\"(Test) WKappa-minexpcost after calib:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=pseudotest_calib_preds,\n",
    "                        true_labels=pseudotest_labels,\n",
    "                        weights=quadratic_weights,\n",
    "                        mode='optim'))\"\"\"\n",
    "            \n",
    "            \n",
    "            \"\"\"print(\"Distribution shift from true labels after calibration:\",\n",
    "                  \"\\nTrue:\\n\",np.mean(pseudovalid_labels,axis=0),\n",
    "                  \"\\nEstimated (valid):\\n\",np.mean(pseudovalid_calib_preds,axis=0),\n",
    "                  \"\\nDifference (valid):\\n\", np.mean(pseudovalid_labels-pseudovalid_calib_preds,\n",
    "                                         axis=0),\n",
    "                  \"\\nEstimated (test):\\n\",np.mean(pseudotest_calib_preds,axis=0),\n",
    "                  \"\\nDifference (test):\\n\", np.mean(pseudovalid_labels,axis=0)\n",
    "                                            -np.mean(pseudotest_calib_preds,axis=0))\"\"\"\n",
    "            pseudovalid_calib_predsamples = np.array(\n",
    "                [the_calibrator(x) for x in pseudovalid_uncalib_predsamples_logits])\n",
    "            pseudotest_calib_predsamples = np.array(\n",
    "                [the_calibrator(x) for x in pseudotest_uncalib_predsamples_logits])\n",
    "            \n",
    "            \n",
    "            \"\"\"print(\"ece before calibration - valid\",\n",
    "              compute_ece(softmax_out=pseudovalid_uncalib_preds,\n",
    "                          labels=pseudovalid_labels,\n",
    "                          bins=15))\n",
    "            print(\"ece after calibration - valid\",\n",
    "                  compute_ece(softmax_out=pseudovalid_calib_preds,\n",
    "                        labels=pseudovalid_labels,\n",
    "                        bins=15))\n",
    "            print(\"ece before calibration - test\",\n",
    "                  compute_ece(softmax_out=pseudotest_uncalib_preds,\n",
    "                              labels=pseudotest_labels,\n",
    "                              bins=15))            \n",
    "            print(\"ece after calibration - test\",\n",
    "                  compute_ece(softmax_out=pseudotest_calib_preds,\n",
    "                        labels=pseudotest_labels,\n",
    "                        bins=15))\"\"\"\n",
    "            \n",
    "            \n",
    "            \"\"\"#plot the calibration curves for each class\n",
    "            from sklearn.calibration import calibration_curve\n",
    "            %matplotlib inline\n",
    "            from matplotlib import pyplot as plt\n",
    "            for class_idx in range(valid_labels.shape[1]+1):\n",
    "                f, axarr = plt.subplots(1,4,figsize=(10,3))\n",
    "                for axidx,(the_preds,the_labels,the_name) in enumerate([\n",
    "                    (pseudovalid_uncalib_preds, pseudovalid_labels, \"valid_uncalib\"),\n",
    "                    (pseudovalid_calib_preds, pseudovalid_labels, \"valid_calib\"),\n",
    "                    (pseudotest_uncalib_preds, pseudotest_labels, \"test_uncalib\"),\n",
    "                    (pseudotest_calib_preds, pseudotest_labels, \"test_calib\")]):                    \n",
    "                    if (class_idx == the_preds.shape[1]):\n",
    "                        avg_confidence_bins, accuracy_bins, prop_in_bins, ece =\\\n",
    "                            compute_ece_with_bins(softmax_out=the_preds,\n",
    "                                                  labels=the_labels, bins=10)\n",
    "                        print(the_name, ece)\n",
    "                        axarr[axidx].plot(accuracy_bins, avg_confidence_bins)\n",
    "                        axarr[axidx].plot([0,1],[0,1])\n",
    "                        axarr[axidx].set_title(the_name)\n",
    "                    else:\n",
    "                        class_preds = the_preds[:,class_idx]\n",
    "                        class_labels = the_labels[:,class_idx]\n",
    "                        prob_true, prob_pred = calibration_curve(y_true=class_labels,\n",
    "                                                                 y_prob=class_preds,\n",
    "                                                                 n_bins=10)\n",
    "                        axarr[axidx].plot(prob_true, prob_pred)\n",
    "                        axarr[axidx].plot([0,1],[0,1])\n",
    "                        axarr[axidx].set_title(the_name)\n",
    "                plt.show()\"\"\"\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        if (calibrator is not None):\n",
    "            pseudotest_preds_to_use=pseudotest_calib_preds\n",
    "            pseudovalid_preds_to_use=pseudovalid_calib_preds\n",
    "            pseudotest_predsamples_to_use=pseudotest_calib_predsamples\n",
    "            pseudovalid_predsamples_to_use=pseudovalid_calib_predsamples\n",
    "        else:\n",
    "            pseudotest_preds_to_use=pseudotest_uncalib_preds\n",
    "            pseudovalid_preds_to_use=pseudovalid_uncalib_preds\n",
    "            pseudotest_predsamples_to_use=pseudotest_uncalib_predsamples\n",
    "            pseudovalid_predsamples_to_use=pseudovalid_uncalib_predsamples\n",
    "        \n",
    "        if (imbalance_adapter is not None):\n",
    "            imbalance_adaptation_func = imbalance_adapter(\n",
    "                #set the validation labels to be pseudovalid_preds_to_use\n",
    "                # (rather than pseudovalid_labels) for consistency;\n",
    "                # we want no adjustment to happen in the\n",
    "                # case where tofit_initial_posterior_probs=pseudovalid_preds_to_use\n",
    "                valid_labels=pseudovalid_preds_to_use,\n",
    "                tofit_initial_posterior_probs=pseudotest_preds_to_use)\n",
    "            preds_before_adaptation = pseudotest_preds_to_use\n",
    "            pseudotest_preds_to_use = imbalance_adaptation_func(pseudotest_preds_to_use)\n",
    "            print(\"Accuracy before calib:\",\n",
    "                  np.mean(np.argmax(pseudotest_uncalib_preds,axis=-1)\n",
    "                          ==np.argmax(pseudotest_labels,axis=-1)))\n",
    "            print(\"Accuracy before adaptation:\",\n",
    "                  np.mean(np.argmax(preds_before_adaptation,axis=-1)\n",
    "                          ==np.argmax(pseudotest_labels,axis=-1)))\n",
    "            print(\"Accuracy after adaptation:\",\n",
    "                  np.mean(np.argmax(pseudotest_preds_to_use,axis=-1)\n",
    "                          ==np.argmax(pseudotest_labels,axis=-1)))\n",
    "            print(\"WKappa-minexpcost before calib:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=pseudotest_uncalib_preds,\n",
    "                        true_labels=pseudotest_labels,\n",
    "                        weights=quadratic_weights,\n",
    "                        mode='optim'))\n",
    "            print(\"WKappa-minexpcost before adaptation:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=preds_before_adaptation,\n",
    "                        true_labels=pseudotest_labels,\n",
    "                        weights=quadratic_weights,\n",
    "                        mode='optim'))\n",
    "            print(\"WKappa-minexpcost after adaptation:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=pseudotest_preds_to_use,\n",
    "                        true_labels=pseudotest_labels,\n",
    "                        weights=quadratic_weights,\n",
    "                        mode='optim'))\n",
    "            sys.stdout.flush()\n",
    "            #print((zip(preds_before_adaptation,pseudotest_preds_to_use))[:20])\n",
    "            \n",
    "            #print(\"Difference from true imbalance\",\n",
    "            #      np.mean(pseudotest_preds_to_use,axis=0)-\n",
    "            #      np.mean(pseudotest_labels,axis=0))\n",
    "            pseudotest_predsamples_to_use = np.array([\n",
    "                    imbalance_adaptation_func(x) for\n",
    "                    x in pseudotest_predsamples_to_use])\n",
    "\n",
    "        pseudovalid_variance_to_use = np.sum(np.var(pseudovalid_predsamples_to_use, axis=0),\n",
    "                                             axis=-1)\n",
    "        pseudotest_variance_to_use = np.sum(np.var(pseudotest_predsamples_to_use, axis=0),\n",
    "                                            axis=-1)\n",
    "            \n",
    "        original_argmaxWeightedKappa_perf = weighted_kappa_metric(\n",
    "            predprobs=pseudotest_preds_to_use,\n",
    "            true_labels=pseudotest_labels,\n",
    "            weights=quadratic_weights,\n",
    "            mode='argmax')\n",
    "        original_optimWeightedKappa_perf = weighted_kappa_metric(\n",
    "            predprobs=pseudotest_preds_to_use,\n",
    "            true_labels=pseudotest_labels,\n",
    "            weights=quadratic_weights,\n",
    "            mode='optim')\n",
    "        \n",
    "        #print(\"\\nPseudotest set weighted kappa\",\n",
    "        #      original_weighted_kappa_perf)\n",
    "        metric_to_baselineperfs[\"argmaxWeightedKappa\"].append(\n",
    "            original_argmaxWeightedKappa_perf)\n",
    "        metric_to_baselineperfs[\"optimWeightedKappa\"].append(\n",
    "            original_optimWeightedKappa_perf)\n",
    "        original_accuracy_perf = np.mean(\n",
    "            np.argmax(pseudotest_preds_to_use,axis=-1)\n",
    "            ==np.argmax(pseudotest_labels,axis=-1))\n",
    "        #print(\"Pseudotest set accuracy\",original_accuracy_perf)\n",
    "        metric_to_baselineperfs[\"accuracy\"].append(original_accuracy_perf)\n",
    "        \n",
    "        for abstention_fraction in abstention_fractions:\n",
    "            #print(\"\\nabstention fraction:\",abstention_fraction)\n",
    "            for abstainer_name, abstainer_factory in abstainer_factories:\n",
    "                abstainer = abstainer_factory(\n",
    "                    valid_labels=pseudovalid_labels,\n",
    "                    valid_posterior=pseudovalid_preds_to_use)\n",
    "                abstainer_priorities = abstainer(\n",
    "                    posterior_probs=pseudotest_preds_to_use,\n",
    "                    uncertainties=pseudotest_variance_to_use)\n",
    "                indices_to_retain = (\n",
    "                    [y[0] for y in sorted(enumerate(abstainer_priorities),\n",
    "                        key=lambda x: x[1])][:int(len(abstainer_priorities)*\n",
    "                                                     (1-abstention_fraction))])\n",
    "                retained_pseudotest_preds = np.array(\n",
    "                    [pseudotest_preds_to_use[i] for i in indices_to_retain])\n",
    "                retained_pseudotest_labels = np.array(\n",
    "                    [pseudotest_labels[i] for i in indices_to_retain])\n",
    "                \n",
    "                argmaxWeightedKappa_perf = weighted_kappa_metric(\n",
    "                    predprobs=retained_pseudotest_preds,\n",
    "                    true_labels=retained_pseudotest_labels,\n",
    "                    weights=quadratic_weights,\n",
    "                    mode='argmax')\n",
    "                optimWeightedKappa_perf = weighted_kappa_metric(\n",
    "                    predprobs=retained_pseudotest_preds,\n",
    "                    true_labels=retained_pseudotest_labels,\n",
    "                    weights=quadratic_weights,\n",
    "                    mode='optim')\n",
    "                accuracy_perf = (np.mean(np.argmax(\n",
    "                    retained_pseudotest_preds,axis=-1)\n",
    "                    ==np.argmax(retained_pseudotest_labels,axis=-1)))\n",
    "                #print(\"\\nAbstention criterion:\",abstainer_name,optimWeightedKappa_perf)\n",
    "\n",
    "                metric_to_fraction_to_method_to_perfs[\"delta_argmaxWeightedKappa\"][\n",
    "                    abstention_fraction][abstainer_name].append(\n",
    "                        argmaxWeightedKappa_perf-original_argmaxWeightedKappa_perf)\n",
    "                metric_to_fraction_to_method_to_perfs[\"delta_optimWeightedKappa\"][\n",
    "                    abstention_fraction][abstainer_name].append(\n",
    "                        optimWeightedKappa_perf-original_optimWeightedKappa_perf)\n",
    "                metric_to_fraction_to_method_to_perfs[\"delta_accuracy\"][\n",
    "                    abstention_fraction][abstainer_name].append(\n",
    "                        accuracy_perf-original_accuracy_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fh = open(\"abstention_results.json\", 'w')\n",
    "fh.write(json.dumps({\n",
    "            \"settingsname_to_metric_to_fraction_to_method_to_perfs\":\n",
    "              settingsname_to_metric_to_fraction_to_method_to_perfs,\n",
    "            \"settingsname_to_metric_to_baselineperfs\":\n",
    "              settingsname_to_metric_to_baselineperfs},\n",
    "             sort_keys=True,\n",
    "             indent=4,\n",
    "             separators=(',', ': ')))\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "loaded_data = json.loads(open(\"abstention_results.json\").read())\n",
    "settingsname_to_metric_to_fraction_to_method_to_perfs =\\\n",
    "    loaded_data[\"settingsname_to_metric_to_fraction_to_method_to_perfs\"]\n",
    "settingsname_to_metric_to_baselineperfs =\\\n",
    "    loaded_data[\"settingsname_to_metric_to_baselineperfs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On comparison group Imbalanced, with weight rescaling\n",
      "On metric argmaxWeightedKappa\n",
      "\n",
      "Baseline argmaxWeightedKappa perfs:\n",
      "set([0.7279563070086574, 0.7795793098778482])\n",
      "set([0.003004075671940214, 0.0030207520124481343])\n",
      "\n",
      " Latex Table for metric argmaxWeightedKappa and group Imbalanced, with weight rescaling\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Adapted? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.7796 $\\pm$0.003 & \\textbf{0.8012 $\\pm$ 0.0028} & \\textbf{0.8164 $\\pm$ 0.0025} & \\textbf{0.8389 $\\pm$ 0.0022}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.728 $\\pm$0.003 & 0.7605 $\\pm$ 0.0028 & 0.778 $\\pm$ 0.0027 & 0.798 $\\pm$ 0.0026\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.7796 $\\pm$0.003 & 0.7918 $\\pm$ 0.0031 & 0.8018 $\\pm$ 0.0032 & 0.8177 $\\pm$ 0.0032\\\\ \\hline\n",
      "Max Class Prob. & N & 0.728 $\\pm$0.003 & 0.7401 $\\pm$ 0.0031 & 0.7497 $\\pm$ 0.0031 & 0.7648 $\\pm$ 0.0032\\\\ \\hline\n",
      "Entropy & Y & 0.7796 $\\pm$0.003 & 0.7968 $\\pm$ 0.003 & 0.8087 $\\pm$ 0.003 & 0.8278 $\\pm$ 0.0028\\\\ \\hline\n",
      "Entropy & N & 0.728 $\\pm$0.003 & 0.7464 $\\pm$ 0.003 & 0.7566 $\\pm$ 0.003 & 0.7725 $\\pm$ 0.0032\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.7796 $\\pm$0.003 & 0.769 $\\pm$ 0.0033 & 0.7661 $\\pm$ 0.0033 & 0.7716 $\\pm$ 0.0032\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.728 $\\pm$0.003 & 0.7189 $\\pm$ 0.0033 & 0.7131 $\\pm$ 0.0036 & 0.7324 $\\pm$ 0.0035\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.6935199576687541, 0.696593309583816])\n",
      "set([0.002951711786888147, 0.0012587543231834487])\n",
      "\n",
      " Latex Table for metric accuracy and group Imbalanced, with weight rescaling\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Adapted? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.6966 $\\pm$0.003 & 0.7099 $\\pm$ 0.0033 & 0.7223 $\\pm$ 0.0035 & 0.7437 $\\pm$ 0.0039\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.6935 $\\pm$0.0013 & \\textbf{0.7094 $\\pm$ 0.0013} & \\textbf{0.7237 $\\pm$ 0.0013} & \\textbf{0.7462 $\\pm$ 0.0016}\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.6966 $\\pm$0.003 & \\textbf{0.7139 $\\pm$ 0.003} & \\textbf{0.73 $\\pm$ 0.0031} & \\textbf{0.7592 $\\pm$ 0.0033}\\\\ \\hline\n",
      "Max Class Prob. & N & 0.6935 $\\pm$0.0013 & \\textbf{0.7101 $\\pm$ 0.0013} & \\textbf{0.7254 $\\pm$ 0.0013} & \\textbf{0.7532 $\\pm$ 0.0015}\\\\ \\hline\n",
      "Entropy & Y & 0.6966 $\\pm$0.003 & 0.7116 $\\pm$ 0.0031 & 0.7256 $\\pm$ 0.0033 & 0.7512 $\\pm$ 0.0035\\\\ \\hline\n",
      "Entropy & N & 0.6935 $\\pm$0.0013 & \\textbf{0.7078 $\\pm$ 0.0013} & \\textbf{0.7217 $\\pm$ 0.0014} & \\textbf{0.7477 $\\pm$ 0.0016}\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.6966 $\\pm$0.003 & \\textbf{0.7098 $\\pm$ 0.0033} & \\textbf{0.7239 $\\pm$ 0.0035} & 0.7505 $\\pm$ 0.0037\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.6935 $\\pm$0.0013 & \\textbf{0.708 $\\pm$ 0.0013} & \\textbf{0.7204 $\\pm$ 0.0014} & \\textbf{0.7478 $\\pm$ 0.0015}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n",
      "On comparison group Balanced, with weight rescaling\n",
      "On metric argmaxWeightedKappa\n",
      "\n",
      "Baseline argmaxWeightedKappa perfs:\n",
      "set([0.8093716251851193])\n",
      "set([0.0014118049916097442])\n",
      "\n",
      " Latex Table for metric argmaxWeightedKappa and group Balanced, with weight rescaling\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Calibrated? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.8094 $\\pm$0.0014 & \\textbf{0.8411 $\\pm$ 0.0013} & \\textbf{0.8578 $\\pm$ 0.0012} & \\textbf{0.8812 $\\pm$ 0.0012}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.8094 $\\pm$0.0014 & 0.8401 $\\pm$ 0.0013 & \\textbf{0.857 $\\pm$ 0.0013} & 0.8785 $\\pm$ 0.0012\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.8094 $\\pm$0.0014 & 0.8256 $\\pm$ 0.0015 & 0.8266 $\\pm$ 0.0017 & 0.8086 $\\pm$ 0.0024\\\\ \\hline\n",
      "Max Class Prob. & N & 0.8094 $\\pm$0.0014 & 0.8213 $\\pm$ 0.0015 & 0.823 $\\pm$ 0.0017 & 0.803 $\\pm$ 0.0024\\\\ \\hline\n",
      "Entropy & Y & 0.8094 $\\pm$0.0014 & 0.8285 $\\pm$ 0.0014 & 0.8402 $\\pm$ 0.0015 & 0.8298 $\\pm$ 0.0022\\\\ \\hline\n",
      "Entropy & N & 0.8094 $\\pm$0.0014 & 0.8291 $\\pm$ 0.0014 & 0.8364 $\\pm$ 0.0016 & 0.8119 $\\pm$ 0.0024\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.8094 $\\pm$0.0014 & 0.7811 $\\pm$ 0.0019 & 0.7634 $\\pm$ 0.0023 & 0.7277 $\\pm$ 0.0035\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.8094 $\\pm$0.0014 & 0.7895 $\\pm$ 0.0018 & 0.7773 $\\pm$ 0.0022 & 0.7586 $\\pm$ 0.0032\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.8275581829859641])\n",
      "set([0.0006387076805881076])\n",
      "\n",
      " Latex Table for metric accuracy and group Balanced, with weight rescaling\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Calibrated? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.8276 $\\pm$0.0006 & 0.8422 $\\pm$ 0.0007 & 0.8598 $\\pm$ 0.0006 & 0.8713 $\\pm$ 0.0007\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.8276 $\\pm$0.0006 & 0.846 $\\pm$ 0.0006 & 0.8632 $\\pm$ 0.0006 & 0.8758 $\\pm$ 0.0007\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.8276 $\\pm$0.0006 & \\textbf{0.8502 $\\pm$ 0.0006} & \\textbf{0.8695 $\\pm$ 0.0006} & 0.9019 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & N & 0.8276 $\\pm$0.0006 & \\textbf{0.8499 $\\pm$ 0.0006} & \\textbf{0.87 $\\pm$ 0.0006} & 0.9018 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & Y & 0.8276 $\\pm$0.0006 & 0.8475 $\\pm$ 0.0006 & 0.8674 $\\pm$ 0.0005 & 0.8985 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & N & 0.8276 $\\pm$0.0006 & \\textbf{0.8501 $\\pm$ 0.0006} & \\textbf{0.869 $\\pm$ 0.0006} & 0.9022 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.8276 $\\pm$0.0006 & 0.8462 $\\pm$ 0.0006 & 0.864 $\\pm$ 0.0005 & 0.9034 $\\pm$ 0.0004\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.8276 $\\pm$0.0006 & 0.8468 $\\pm$ 0.0006 & 0.8652 $\\pm$ 0.0005 & \\textbf{0.905 $\\pm$ 0.0005}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n",
      "On comparison group Imalanced, with MC dropout\n",
      "On metric argmaxWeightedKappa\n",
      "\n",
      "Baseline argmaxWeightedKappa perfs:\n",
      "set([0.7817471109654133, 0.7265153997862245])\n",
      "set([0.0029982221954689783, 0.0031657955218403182])\n",
      "\n",
      " Latex Table for metric argmaxWeightedKappa and group Imalanced, with MC dropout\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Adapted? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.7817 $\\pm$0.003 & \\textbf{0.8044 $\\pm$ 0.0027} & \\textbf{0.8204 $\\pm$ 0.0024} & \\textbf{0.8428 $\\pm$ 0.0022}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.7265 $\\pm$0.0032 & 0.7633 $\\pm$ 0.0028 & 0.7802 $\\pm$ 0.0027 & 0.7983 $\\pm$ 0.0026\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.7817 $\\pm$0.003 & 0.7932 $\\pm$ 0.0031 & 0.8035 $\\pm$ 0.0031 & 0.8198 $\\pm$ 0.0032\\\\ \\hline\n",
      "Max Class Prob. & N & 0.7265 $\\pm$0.0032 & 0.7387 $\\pm$ 0.0032 & 0.7495 $\\pm$ 0.0031 & 0.7643 $\\pm$ 0.0034\\\\ \\hline\n",
      "Entropy & Y & 0.7817 $\\pm$0.003 & 0.7988 $\\pm$ 0.003 & 0.8118 $\\pm$ 0.0029 & 0.8311 $\\pm$ 0.0029\\\\ \\hline\n",
      "Entropy & N & 0.7265 $\\pm$0.0032 & 0.7456 $\\pm$ 0.0032 & 0.7552 $\\pm$ 0.0032 & 0.7708 $\\pm$ 0.0033\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.7817 $\\pm$0.003 & 0.7708 $\\pm$ 0.0032 & 0.7677 $\\pm$ 0.0033 & 0.7745 $\\pm$ 0.0032\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.7265 $\\pm$0.0032 & 0.7224 $\\pm$ 0.0032 & 0.7184 $\\pm$ 0.0035 & 0.7382 $\\pm$ 0.0034\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.6955651093377198, 0.6927378421751803])\n",
      "set([0.0013169770617030793, 0.0032522001194893956])\n",
      "\n",
      " Latex Table for metric accuracy and group Imalanced, with MC dropout\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Adapted? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.6956 $\\pm$0.0033 & 0.7095 $\\pm$ 0.0035 & 0.7223 $\\pm$ 0.0038 & 0.7442 $\\pm$ 0.0042\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.6927 $\\pm$0.0013 & \\textbf{0.709 $\\pm$ 0.0013} & \\textbf{0.7234 $\\pm$ 0.0013} & \\textbf{0.746 $\\pm$ 0.0015}\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.6956 $\\pm$0.0033 & \\textbf{0.7125 $\\pm$ 0.0034} & \\textbf{0.729 $\\pm$ 0.0035} & \\textbf{0.7594 $\\pm$ 0.0036}\\\\ \\hline\n",
      "Max Class Prob. & N & 0.6927 $\\pm$0.0013 & \\textbf{0.7099 $\\pm$ 0.0013} & \\textbf{0.7256 $\\pm$ 0.0014} & \\textbf{0.7557 $\\pm$ 0.0015}\\\\ \\hline\n",
      "Entropy & Y & 0.6956 $\\pm$0.0033 & 0.7103 $\\pm$ 0.0034 & 0.725 $\\pm$ 0.0036 & 0.7524 $\\pm$ 0.0039\\\\ \\hline\n",
      "Entropy & N & 0.6927 $\\pm$0.0013 & \\textbf{0.7074 $\\pm$ 0.0014} & \\textbf{0.721 $\\pm$ 0.0014} & \\textbf{0.7477 $\\pm$ 0.0016}\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.6956 $\\pm$0.0033 & \\textbf{0.7099 $\\pm$ 0.0035} & \\textbf{0.7238 $\\pm$ 0.0037} & 0.7504 $\\pm$ 0.0041\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.6927 $\\pm$0.0013 & \\textbf{0.7095 $\\pm$ 0.0013} & \\textbf{0.722 $\\pm$ 0.0014} & \\textbf{0.7507 $\\pm$ 0.0015}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n",
      "On comparison group Balanced, with MC dropout\n",
      "On metric argmaxWeightedKappa\n",
      "\n",
      "Baseline argmaxWeightedKappa perfs:\n",
      "set([0.8095010350728309])\n",
      "set([0.0013891216281147626])\n",
      "\n",
      " Latex Table for metric argmaxWeightedKappa and group Balanced, with MC dropout\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Calibrated? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.8095 $\\pm$0.0014 & \\textbf{0.8414 $\\pm$ 0.0013} & \\textbf{0.8601 $\\pm$ 0.0012} & \\textbf{0.8847 $\\pm$ 0.0011}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.8095 $\\pm$0.0014 & 0.8402 $\\pm$ 0.0013 & 0.8582 $\\pm$ 0.0012 & 0.8826 $\\pm$ 0.0011\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.8095 $\\pm$0.0014 & 0.8259 $\\pm$ 0.0015 & 0.8246 $\\pm$ 0.0017 & 0.8036 $\\pm$ 0.0025\\\\ \\hline\n",
      "Max Class Prob. & N & 0.8095 $\\pm$0.0014 & 0.8227 $\\pm$ 0.0015 & 0.8225 $\\pm$ 0.0017 & 0.8007 $\\pm$ 0.0025\\\\ \\hline\n",
      "Entropy & Y & 0.8095 $\\pm$0.0014 & 0.829 $\\pm$ 0.0014 & 0.8384 $\\pm$ 0.0015 & 0.8265 $\\pm$ 0.0023\\\\ \\hline\n",
      "Entropy & N & 0.8095 $\\pm$0.0014 & 0.8286 $\\pm$ 0.0014 & 0.8364 $\\pm$ 0.0016 & 0.8099 $\\pm$ 0.0025\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.8095 $\\pm$0.0014 & 0.7826 $\\pm$ 0.0018 & 0.766 $\\pm$ 0.0023 & 0.7367 $\\pm$ 0.0033\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.8095 $\\pm$0.0014 & 0.7889 $\\pm$ 0.0018 & 0.7766 $\\pm$ 0.0022 & 0.7586 $\\pm$ 0.0032\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.8294379572476536])\n",
      "set([0.0006407416004073898])\n",
      "\n",
      " Latex Table for metric accuracy and group Balanced, with MC dropout\n",
      "\n",
      "\\begin{table*}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method& Calibrated? & Base. & @5\\% Abs. & @10\\% Abs. & @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.8294 $\\pm$0.0006 & 0.8432 $\\pm$ 0.0007 & 0.8613 $\\pm$ 0.0006 & 0.8743 $\\pm$ 0.0007\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.8294 $\\pm$0.0006 & 0.8453 $\\pm$ 0.0007 & 0.8637 $\\pm$ 0.0006 & 0.8773 $\\pm$ 0.0007\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.8294 $\\pm$0.0006 & \\textbf{0.8519 $\\pm$ 0.0006} & 0.871 $\\pm$ 0.0006 & 0.9033 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & N & 0.8294 $\\pm$0.0006 & \\textbf{0.8518 $\\pm$ 0.0006} & \\textbf{0.8715 $\\pm$ 0.0006} & 0.9033 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & Y & 0.8294 $\\pm$0.0006 & 0.8483 $\\pm$ 0.0006 & 0.868 $\\pm$ 0.0005 & 0.9005 $\\pm$ 0.0004\\\\ \\hline\n",
      "Entropy & N & 0.8294 $\\pm$0.0006 & 0.8508 $\\pm$ 0.0006 & 0.8696 $\\pm$ 0.0006 & 0.9034 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.8294 $\\pm$0.0006 & 0.8476 $\\pm$ 0.0006 & 0.8648 $\\pm$ 0.0005 & 0.9043 $\\pm$ 0.0004\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.8294 $\\pm$0.0006 & 0.8479 $\\pm$ 0.0006 & 0.8654 $\\pm$ 0.0005 & \\textbf{0.905 $\\pm$ 0.0005}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from abstention.figure_making_utils import (\n",
    "    wilcox_srs, get_ustats_mat,\n",
    "    get_tied_top_and_worst_methods)\n",
    "from collections import OrderedDict\n",
    "\n",
    "comparison_groups = OrderedDict([\n",
    "        ('Imbalanced, with weight rescaling', ([\n",
    "          #('tempscalebiascor-calib_weightrescalepreds_imbalanced_adapted', 'expected_delta_optimWeightedKappa'),\n",
    "          #('tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted', 'expected_delta_optimWeightedKappa'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_adapted', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_adapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_adapted', 'entropy'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted', 'entropy'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_adapted', 'variance'),\n",
    "          ('tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted', 'variance')],\n",
    "         ['adapted'])),\n",
    "        ('Balanced, with weight rescaling', ([\n",
    "          ('tempscale-calib_weightrescalepreds', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('uncalib_weightrescalepreds', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('tempscale-calib_weightrescalepreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('uncalib_weightrescalepreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('tempscale-calib_weightrescalepreds', 'entropy'),\n",
    "          ('uncalib_weightrescalepreds', 'entropy'),\n",
    "          ('tempscale-calib_weightrescalepreds', 'variance'),\n",
    "          ('uncalib_weightrescalepreds', 'variance')],\n",
    "         ['calib'])),\n",
    "       ('Imalanced, with MC dropout', ([\n",
    "          #('tempscalebiascor-calib_mcdrpreds_imbalanced_adapted', 'expected_delta_optimWeightedKappa'),\n",
    "          #('tempscalebiascor-calib_mcdrpreds_imbalanced_unadapted', 'expected_delta_optimWeightedKappa'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_adapted', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_unadapted', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_adapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_unadapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_adapted', 'entropy'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_unadapted', 'entropy'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_adapted', 'variance'),\n",
    "          ('tempscalebiascor-calib_mcdrpreds_imbalanced_unadapted', 'variance')],\n",
    "         ['adapted'])),\n",
    "       ('Balanced, with MC dropout', ([\n",
    "          #('tempscale-calib_mcdrpreds', 'expected_delta_optimWeightedKappa'),\n",
    "          #('uncalib_mcdrpreds', 'expected_delta_optimWeightedKappa'),\n",
    "          ('tempscale-calib_mcdrpreds', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('uncalib_mcdrpreds', 'expected_delta_argmaxWeightedKappa'),\n",
    "          ('tempscale-calib_mcdrpreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('uncalib_mcdrpreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('tempscale-calib_mcdrpreds', 'entropy'),\n",
    "          ('uncalib_mcdrpreds', 'entropy'),\n",
    "          ('tempscale-calib_mcdrpreds', 'variance'),\n",
    "          ('uncalib_mcdrpreds', 'variance')],\n",
    "         ['calib']))\n",
    "    ])\n",
    "\n",
    "friendly_method_names = {\n",
    "    'expected_delta_argmaxWeightedKappa': 'E[$\\Delta$Kappa]',\n",
    "    #'expected_delta_optimWeightedKappa': 'E[$\\Delta$Kappa]',\n",
    "    'dist_maxclass_prob_from_one': 'Max Class Prob.',\n",
    "    'entropy': 'Entropy',\n",
    "    'variance': 'MC Dropout Var.'\n",
    "}\n",
    "abstention_fractions = ['0.05', '0.1', #'0.15',\n",
    "                        '0.2']\n",
    "\n",
    "for comparison_group_name in comparison_groups:\n",
    "    \n",
    "    print(\"On comparison group\", comparison_group_name)\n",
    "    columnstowrite = comparison_groups[comparison_group_name][1]\n",
    "    for metric in [\"argmaxWeightedKappa\",\n",
    "                   #\"optimWeightedKappa\",\n",
    "                   \"accuracy\"]:\n",
    "        print(\"On metric\", metric)\n",
    "        \n",
    "        #gather all the necessary data\n",
    "        settingnmethod_to_baselineperfs = OrderedDict()\n",
    "        settingnmethod_to_abstentionfraction_to_perfs = OrderedDict()\n",
    "        \n",
    "        for (settingsname, methodname) in comparison_groups[comparison_group_name][0]:\n",
    "            settingnmethod = settingsname+\":\"+methodname\n",
    "            settingnmethod_to_baselineperfs[settingnmethod] =\\\n",
    "                settingsname_to_metric_to_baselineperfs[settingsname][metric]\n",
    "            \n",
    "            \n",
    "            abstentionfraction_to_perfs = OrderedDict()\n",
    "            settingnmethod_to_abstentionfraction_to_perfs[settingnmethod] =\\\n",
    "                abstentionfraction_to_perfs\n",
    "            for abstention_fraction in abstention_fractions:\n",
    "                abstentionfraction_to_perfs[abstention_fraction] = (\n",
    "                    settingsname_to_metric_to_fraction_to_method_to_perfs[\n",
    "                        settingsname][\"delta_\"+metric][abstention_fraction][\n",
    "                        methodname])\n",
    "        \n",
    "        #prepare the table contents\n",
    "        \n",
    "        settingnmethod_to_tablecontents = OrderedDict()\n",
    "        for settingnmethod in settingnmethod_to_baselineperfs:\n",
    "            tablerow = {}\n",
    "            settingnmethod_to_tablecontents[settingnmethod] = tablerow\n",
    "            tablerow['baseline'] = {\n",
    "                'mean': np.mean(settingnmethod_to_baselineperfs[settingnmethod]),\n",
    "                'stderr': np.std(settingnmethod_to_baselineperfs[settingnmethod],\n",
    "                                 ddof=1)/np.sqrt(num_folds)}\n",
    "            tablerow['method'] = settingnmethod.split(\":\")[1]\n",
    "            tablerow['mcdr'] = \"mcdr\" in settingnmethod.split(\":\")[0]\n",
    "            tablerow['calib'] = (\"uncalib\" in settingnmethod.split(\":\")[0])==False\n",
    "            #if neither 'balanced' nor 'imbalanced' is in the name, it means balanced\n",
    "            tablerow['imbalanced'] = ((\"imbalanced\" in settingnmethod.split(\":\")[0])\n",
    "                                      and (\"balanced\" in settingnmethod.split(\":\")[0]))\n",
    "            #if neither adapted nor unadapted is in the name, it means no adaptation\n",
    "            tablerow['adapted'] = ((\"unadapted\" not in settingnmethod.split(\":\")[0])\n",
    "                                     and (\"adapted\" in settingnmethod.split(\":\")[0]))\n",
    "        \n",
    "        for abstention_fraction in abstention_fractions:\n",
    "            method_to_perfs = OrderedDict()\n",
    "            for settingnmethod in settingnmethod_to_tablecontents:\n",
    "                tablerow = settingnmethod_to_tablecontents[settingnmethod]\n",
    "                perfsdelta = settingnmethod_to_abstentionfraction_to_perfs[\n",
    "                    settingnmethod][abstention_fraction]\n",
    "                \n",
    "                perfs = np.array(perfsdelta)+np.array(settingnmethod_to_baselineperfs[settingnmethod])\n",
    "                method_to_perfs[settingnmethod] = perfs\n",
    "                mean_perfs = np.mean(perfs)\n",
    "                stderr_perfs = (np.std(perfs,ddof=1)/np.sqrt(num_folds))\n",
    "                \n",
    "                tablerow[abstention_fraction] = {\n",
    "                    'mean': mean_perfs,\n",
    "                    'stderr': stderr_perfs}\n",
    "            methods_to_consider = list(method_to_perfs.keys())\n",
    "            ustats_mat = get_ustats_mat(\n",
    "                method_to_perfs,\n",
    "                methods_to_consider,\n",
    "                max_ustat=1275)\n",
    "            tied_top_methods, tied_worst_methods =(\n",
    "                get_tied_top_and_worst_methods(\n",
    "                    ustats_mat,\n",
    "                    methods_to_consider,\n",
    "                    #0.05 threshold for one-sided test when N=119 is 50\n",
    "                    #http://www.real-statistics.com/statistics-tables/wilcoxon-signed-ranks-table/\n",
    "                    threshold=120\n",
    "                ))\n",
    "            tied_top_methods = [methods_to_consider[x]\n",
    "                                for x in tied_top_methods]\n",
    "            #print(abstention_fraction)\n",
    "            #print(tied_top_methods)\n",
    "            for settingnmethod in settingnmethod_to_tablecontents:\n",
    "                settingnmethod_to_tablecontents[\n",
    "                    settingnmethod][abstention_fraction][\n",
    "                    'istop'] = (settingnmethod in tied_top_methods)\n",
    "            #print(settingnmethod_to_tablecontents)\n",
    "            \n",
    "        thestr = \"\\\\begin{table*}\\n\\\\begin{center}\\n\\\\begin{tabular}{ | c | c | c | c | c | c | c | }\\n\"\n",
    "        thestr += (\"\\\\hline Method\"\n",
    "                  +('& Calibrated? ' if 'calib' in columnstowrite else '')\n",
    "                  +('& Adapted? ' if 'adapted' in columnstowrite else '')\n",
    "                  +\"& Base. \"\n",
    "                  +\"& @\")\n",
    "        thestr += \" & @\".join([str(int(100*float(x)))+\"\\\\% Abs.\"\n",
    "                              for x in abstention_fractions])\n",
    "        thestr += \"\\\\\\\\ \\\\hline\\n\"\n",
    "        for settingnmethod in settingnmethod_to_tablecontents:\n",
    "            tablerow = settingnmethod_to_tablecontents[settingnmethod]\n",
    "            thestr += friendly_method_names[tablerow['method']]\n",
    "            if ('calib' in columnstowrite):\n",
    "                thestr += \" & \"+(\"Y\" if tablerow['calib'] else \"N\")\n",
    "            if ('adapted' in columnstowrite):\n",
    "                thestr += \" & \"+(\"Y\" if tablerow['adapted'] else \"N\")\n",
    "            thestr += (\" & \"+str(np.round(tablerow['baseline']['mean'],4))\n",
    "                        +\" $\\\\pm$\"\n",
    "                        +str(np.round(tablerow['baseline']['stderr'],4)))\n",
    "            \n",
    "            #thestr += \" & \"+(str(np.round(tablerow['baseline']['mean'],4))\n",
    "            #                 +\" $\\\\pm$ \"\n",
    "            #                 +str(np.round(tablerow['baseline']['stderr'],4)))\n",
    "            for abstention_fraction in abstention_fractions:\n",
    "                thestr += (\n",
    "                    \" & \"+\n",
    "                    (\"\\\\textbf{\" if tablerow[abstention_fraction]['istop'] else \"\")\n",
    "                    +str(np.round(tablerow[abstention_fraction]['mean'],4))\n",
    "                    +\" $\\\\pm$ \"\n",
    "                    +str(np.round(tablerow[abstention_fraction]['stderr'],4))\n",
    "                    +(\"}\" if tablerow[abstention_fraction]['istop'] else \"\"))\n",
    "            thestr += \"\\\\\\\\ \\hline\\n\"\n",
    "        thestr += \"\\\\end{tabular}\\n\\\\end{center}\\n\\\\end{table*}\\n\"\n",
    "        \n",
    "        print(\"\\nBaseline \"+metric+\" perfs:\")\n",
    "        baseline_mean = set(\n",
    "                x['baseline']['mean'] for x in\n",
    "                settingnmethod_to_tablecontents.values())\n",
    "        print(baseline_mean)\n",
    "        #assert that all the methods have the same baseline\n",
    "        #assert len(baseline_mean)==1\n",
    "        #baseline_mean = list(baseline_mean)[0]\n",
    "        baseline_stderr = set(\n",
    "                x['baseline']['stderr'] for x in\n",
    "                settingnmethod_to_tablecontents.values())\n",
    "        print(baseline_stderr)\n",
    "        #assert len(baseline_stderr)==1\n",
    "        #baseline_stderr = list(baseline_stderr)[0]\n",
    "        #print(np.round(baseline_mean,4),\"$\\\\pm$\",np.round(baseline_stderr,4))\n",
    "        \n",
    "        print(\"\\n Latex Table for metric \"\n",
    "              +metric+\" and group \"+comparison_group_name\n",
    "              +\"\\n\\n\"+thestr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.685374149659864,\n",
       " 0.6839716312056737,\n",
       " 0.703483432455395,\n",
       " 0.7083333333333334,\n",
       " 0.6933106575963719,\n",
       " 0.683404255319149,\n",
       " 0.6918489065606361,\n",
       " 0.689900426742532,\n",
       " 0.6831205673758866,\n",
       " 0.688737973967176,\n",
       " 0.6810979060554613,\n",
       " 0.7007403189066059,\n",
       " 0.6969869243888573,\n",
       " 0.6879613855763771,\n",
       " 0.7009637188208617,\n",
       " 0.7018439716312057,\n",
       " 0.7140022675736961,\n",
       " 0.70261066969353,\n",
       " 0.6961341671404206,\n",
       " 0.7018192154633315,\n",
       " 0.6967120181405896,\n",
       " 0.7080727686185333,\n",
       " 0.702357284862255,\n",
       " 0.6983588002263724,\n",
       " 0.713593616414933,\n",
       " 0.6947608200455581,\n",
       " 0.6864527122976427,\n",
       " 0.6933106575963719,\n",
       " 0.7036089798238135,\n",
       " 0.7026643990929705,\n",
       " 0.6978783592644979,\n",
       " 0.6942078364565588,\n",
       " 0.6882453151618398,\n",
       " 0.6959114139693356,\n",
       " 0.6790684464640727,\n",
       " 0.6941209883555808,\n",
       " 0.6893121091529278,\n",
       " 0.6859083191850595,\n",
       " 0.6861853325753269,\n",
       " 0.681378132118451,\n",
       " 0.6793524566884408,\n",
       " 0.6881750994883457,\n",
       " 0.6816638370118846,\n",
       " 0.6831065759637188,\n",
       " 0.6862411347517731,\n",
       " 0.7010514350667804,\n",
       " 0.7009637188208617,\n",
       " 0.6904964539007092,\n",
       " 0.6893039049235993,\n",
       " 0.687889077532541]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settingsname_to_metric_to_baselineperfs['tempscalebiascor-calib_weightrescalepreds_imbalanced_unadapted']['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
