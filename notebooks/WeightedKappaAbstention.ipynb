{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "valid_labels = LabelBinarizer().fit_transform(\n",
    "    np.array([float(x.decode(\"utf-8\").split(\"\\t\")[1])\n",
    "              for x in gzip.open(\"valid_labels.txt.gz\",'rb')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#augmenting the dataset with flips and rotaitons, for more robustness\n",
    "parent_folders = [\"flip-False_rotamt-0\",\n",
    "                  \"flip-True_rotamt-0\",\n",
    "                  \"flip-False_rotamt-90\",\n",
    "                  \"flip-True_rotamt-90\",\n",
    "                  \"flip-False_rotamt-180\",\n",
    "                  \"flip-True_rotamt-180\",]\n",
    "\n",
    "parent_folder_to_det_pred = {}\n",
    "for parent_folder in parent_folders:\n",
    "    det_preds = np.array([\n",
    "            [float(y) for y in x.decode(\"utf-8\").split(\"\\t\")[1:]]\n",
    "             for x in gzip.open(parent_folder+\"/deterministic_preds.txt.gz\", 'rb')])\n",
    "    parent_folder_to_det_pred[parent_folder] = det_preds\n",
    "    \n",
    "parent_folder_to_nondet_pred = {}\n",
    "parent_folder_to_mean_nondet_pred = {}\n",
    "for parent_folder in parent_folders:\n",
    "    nondet_preds = []\n",
    "    for i in range(100):\n",
    "        single_nondet_pred = np.array([\n",
    "            [float(y) for y in x.decode(\"utf-8\").split(\"\\t\")[1:]]\n",
    "             for x in gzip.open(\n",
    "              parent_folder+\"/nondeterministic_preds_\"+str(i)+\".txt.gz\", 'rb')])\n",
    "        nondet_preds.append(single_nondet_pred)\n",
    "    nondet_preds = np.array(nondet_preds)\n",
    "    parent_folder_to_nondet_pred[parent_folder] = nondet_preds\n",
    "    parent_folder_to_mean_nondet_pred[parent_folder] = np.mean(nondet_preds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flip-False_rotamt-0\n",
      "deterministic pred auROC 0.9118638796723656\n",
      "nondeterministic pred auROC 0.9129881925522253\n",
      "flip-True_rotamt-0\n",
      "deterministic pred auROC 0.9136845292158645\n",
      "nondeterministic pred auROC 0.9141930341618936\n",
      "flip-False_rotamt-90\n",
      "deterministic pred auROC 0.9077797755493358\n",
      "nondeterministic pred auROC 0.9079080860318696\n",
      "flip-True_rotamt-90\n",
      "deterministic pred auROC 0.9072243126739039\n",
      "nondeterministic pred auROC 0.9084814670645733\n",
      "flip-False_rotamt-180\n",
      "deterministic pred auROC 0.916166708887612\n",
      "nondeterministic pred auROC 0.9166587373671843\n",
      "flip-True_rotamt-180\n",
      "deterministic pred auROC 0.9131712872857287\n",
      "nondeterministic pred auROC 0.9138490879246036\n"
     ]
    }
   ],
   "source": [
    "#Compute the auROC/auPRC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for parent_folder in parent_folders:\n",
    "    print(parent_folder)\n",
    "    det_preds = parent_folder_to_det_pred[parent_folder]\n",
    "    mean_nondet_preds = parent_folder_to_mean_nondet_pred[parent_folder]\n",
    "    print(\"deterministic pred auROC\",\n",
    "          roc_auc_score(y_true=1-valid_labels[:,0],\n",
    "                              y_score=1-det_preds[:,0]))\n",
    "    print(\"nondeterministic pred auROC\",\n",
    "          roc_auc_score(y_true=1-valid_labels[:,0],\n",
    "                              y_score=1-mean_nondet_preds[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstainer settings calib_weightrescalepreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings uncalib_weightrescalepreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings calib_mcdrpreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings uncalib_mcdrpreds\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n"
     ]
    }
   ],
   "source": [
    "import abstention\n",
    "from abstention.calibration import compute_ece, TempScaling\n",
    "reload(abstention.abstention)\n",
    "from abstention.abstention import (weighted_kappa_metric,\n",
    "                                   WeightedKappa, DistMaxClassProbFromOne,\n",
    "                                   Entropy, Uncertainty)\n",
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def inverse_softmax(preds):\n",
    "    return np.log(preds) - np.mean(np.log(preds),axis=1)[:,None]\n",
    "\n",
    "quadratic_weights = np.array([[(i-j)**2 for i in range(5)]\n",
    "                             for j in range(5)])\n",
    "\n",
    "AbstainerSettings = namedtuple(\"AbstainerSettings\",\n",
    "                               [\"name\",\n",
    "                                \"abstainer_factories\",\n",
    "                                \"preds_lookup\",\n",
    "                                \"predsamples_lookup\",\n",
    "                                \"use_calib\"])\n",
    "\n",
    "abstainer_factories = [\n",
    "        (\"expected_delta_weighted_kappa\", WeightedKappa(\n",
    "            weights=quadratic_weights, verbose=False)),\n",
    "        (\"expected_delta_weighted_kappa_imbalance_from_valid\", WeightedKappa(\n",
    "                weights=quadratic_weights,\n",
    "                estimate_class_imbalance_from_valid=True,\n",
    "                verbose=False)),\n",
    "        (\"dist_maxclass_prob_from_one\", DistMaxClassProbFromOne()),\n",
    "        (\"entropy\", Entropy()),\n",
    "        (\"variance\", Uncertainty())]\n",
    "abstention_fractions = [0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "abstainer_settings_list = [\n",
    "    AbstainerSettings(\n",
    "        name=\"calib_weightrescalepreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=True),\n",
    "    AbstainerSettings(\n",
    "        name=\"uncalib_weightrescalepreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=False),\n",
    "    AbstainerSettings(\n",
    "        name=\"calib_mcdrpreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=True),\n",
    "    AbstainerSettings(\n",
    "        name=\"uncalib_mcdrpreds\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=False)\n",
    "]\n",
    "\n",
    "num_folds = 50\n",
    "\n",
    "settingsname_to_metric_to_fraction_to_method_to_perfs = {}\n",
    "settingsname_to_metric_to_baselineperfs = {}\n",
    "\n",
    "for abstainer_settings in abstainer_settings_list:\n",
    "    \n",
    "    settings_name = abstainer_settings.name\n",
    "    print(\"abstainer settings\", settings_name)\n",
    "    abstainer_factories = abstainer_settings.abstainer_factories\n",
    "    preds_lookup = abstainer_settings.preds_lookup\n",
    "    predsamples_lookup = abstainer_settings.predsamples_lookup\n",
    "    use_calib = abstainer_settings.use_calib\n",
    "    \n",
    "    metric_to_fraction_to_method_to_perfs =\\\n",
    "        defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    settingsname_to_metric_to_fraction_to_method_to_perfs[settings_name] =\\\n",
    "        metric_to_fraction_to_method_to_perfs\n",
    "    metric_to_baselineperfs = defaultdict(list)   \n",
    "    settingsname_to_metric_to_baselineperfs[settings_name] =\\\n",
    "        metric_to_baselineperfs\n",
    "    \n",
    "    for fold_number in range(num_folds):\n",
    "        print(\"on fold\",fold_number)\n",
    "\n",
    "        np.random.seed(fold_number*1000)\n",
    "        random.seed(fold_number*1000)\n",
    "        #the data is in pairs of (left eye, right eye) per patient (entry for\n",
    "        # the right eye comes after the entry for the left eye); hence, the number of\n",
    "        # unique patients is 0.5*len(valid_labels)\n",
    "        patient_id_ordering = list(range(int(0.5*len(valid_labels))))\n",
    "        np.random.shuffle(patient_id_ordering)\n",
    "\n",
    "        pseudovalid_uncalib_preds = []\n",
    "        pseudotest_uncalib_preds = []\n",
    "        pseudovalid_uncalib_predsamples = []\n",
    "        pseudotest_uncalib_predsamples = []\n",
    "        pseudovalid_labels = []\n",
    "        pseudotest_labels = []\n",
    "        pseudovalid_label_counts = np.zeros(5)\n",
    "        pseudotest_label_counts = np.zeros(5)\n",
    "        for i in patient_id_ordering:\n",
    "            left_eye_label = valid_labels[2*i]\n",
    "            right_eye_label = valid_labels[(2*i)+1]\n",
    "            most_diseased_label = max(np.argmax(left_eye_label),\n",
    "                                      np.argmax(right_eye_label))\n",
    "            if (pseudovalid_label_counts[most_diseased_label] <\n",
    "                pseudotest_label_counts[most_diseased_label]):\n",
    "                append_to_uncalib_preds = pseudovalid_uncalib_preds\n",
    "                append_to_uncalib_predsamples = pseudovalid_uncalib_predsamples\n",
    "                append_to_labels = pseudovalid_labels\n",
    "                append_to_label_counts = pseudovalid_label_counts\n",
    "            else:\n",
    "                append_to_uncalib_preds = pseudotest_uncalib_preds\n",
    "                append_to_uncalib_predsamples = pseudotest_uncalib_predsamples\n",
    "                append_to_labels = pseudotest_labels\n",
    "                append_to_label_counts = pseudotest_label_counts\n",
    "\n",
    "            for parent_folder in parent_folders:        \n",
    "                append_to_labels.append(valid_labels[2*i])\n",
    "                append_to_labels.append(valid_labels[(2*i)+1])\n",
    "                append_to_label_counts += valid_labels[2*i]\n",
    "                append_to_label_counts += valid_labels[(2*i)+1]\n",
    "                append_to_uncalib_preds.append(\n",
    "                        preds_lookup[parent_folder][2*i])\n",
    "                append_to_uncalib_preds.append(\n",
    "                        preds_lookup[parent_folder][(2*i)+1])\n",
    "                append_to_uncalib_predsamples.append(\n",
    "                        predsamples_lookup[parent_folder][:,(2*i)])\n",
    "                append_to_uncalib_predsamples.append(\n",
    "                        predsamples_lookup[parent_folder][:,(2*i)+1])\n",
    "\n",
    "        pseudovalid_uncalib_preds = np.array(pseudovalid_uncalib_preds)\n",
    "        pseudotest_uncalib_preds = np.array(pseudotest_uncalib_preds)\n",
    "        pseudovalid_uncalib_pred_logits = inverse_softmax(pseudovalid_uncalib_preds)\n",
    "        pseudotest_uncalib_pred_logits = inverse_softmax(pseudotest_uncalib_preds)\n",
    "        pseudovalid_uncalib_predsamples = np.array(pseudovalid_uncalib_predsamples).transpose((1,0,2))\n",
    "        pseudotest_uncalib_predsamples = np.array(pseudotest_uncalib_predsamples).transpose((1,0,2))\n",
    "        pseudovalid_uncalib_predsamples_logits = np.array([\n",
    "                inverse_softmax(x) for x in pseudovalid_uncalib_predsamples])        \n",
    "        pseudotest_uncalib_predsamples_logits = np.array([\n",
    "                inverse_softmax(x) for x in pseudotest_uncalib_predsamples])\n",
    "        pseudovalid_labels = np.array(pseudovalid_labels) \n",
    "        pseudotest_labels = np.array(pseudotest_labels)\n",
    "        #print(\"valid vs. test distribution shift\",\n",
    "        #      np.abs(pseudovalid_label_counts-pseudotest_label_counts)/\n",
    "        #            (pseudovalid_label_counts+pseudotest_label_counts))\n",
    "        \n",
    "        pseudovalid_uncalib_variance = np.sum(np.var(pseudovalid_uncalib_predsamples,axis=0),axis=-1)\n",
    "        pseudotest_uncalib_variance = np.sum(np.var(pseudotest_uncalib_predsamples,axis=0),axis=-1)\n",
    "        \n",
    "        if (use_calib):\n",
    "            #print(\"ece before temp scale - valid\",\n",
    "            #  compute_ece(softmax_out=pseudovalid_uncalib_preds,\n",
    "            #              labels=pseudovalid_labels,\n",
    "            #              bins=15))\n",
    "            #print(\"ece before temp scale - test\",\n",
    "            #      compute_ece(softmax_out=pseudotest_uncalib_preds,\n",
    "            #                  labels=pseudotest_labels,\n",
    "            #                  bins=15))\n",
    "            temp_scaler = TempScaling(ece_bins=15, verbose=False)(\n",
    "                                valid_preacts=pseudovalid_uncalib_pred_logits,\n",
    "                                valid_labels=pseudovalid_labels)\n",
    "            pseudovalid_calib_preds = temp_scaler(pseudovalid_uncalib_pred_logits)\n",
    "            pseudotest_calib_preds = temp_scaler(pseudotest_uncalib_pred_logits)\n",
    "            pseudovalid_calib_predsamples = np.array(\n",
    "                [temp_scaler(x) for x in pseudovalid_uncalib_predsamples_logits])\n",
    "            pseudotest_calib_predsamples = np.array(\n",
    "                [temp_scaler(x) for x in pseudotest_uncalib_predsamples_logits])\n",
    "            \n",
    "            pseudovalid_calib_variance = np.sum(np.var(pseudovalid_calib_predsamples, axis=0),axis=-1)\n",
    "            pseudotest_calib_variance = np.sum(np.var(pseudotest_calib_predsamples, axis=0),axis=-1)\n",
    "\n",
    "            #print(\"ece after temp scale - valid\",\n",
    "            #      compute_ece(softmax_out=pseudovalid_calib_preds,\n",
    "            #            labels=pseudovalid_labels,\n",
    "            #            bins=15))\n",
    "            #print(\"ece after temp scale - test\",\n",
    "            #      compute_ece(softmax_out=pseudotest_calib_preds,\n",
    "            #            labels=pseudotest_labels,\n",
    "            #            bins=15))\n",
    "            \n",
    "        if (use_calib):\n",
    "            pseudotest_preds_to_use=pseudotest_calib_preds\n",
    "            pseudovalid_preds_to_use=pseudovalid_calib_preds\n",
    "            pseudotest_variance_to_use=pseudotest_calib_variance\n",
    "            pseudovalid_variance_to_use=pseudovalid_calib_variance\n",
    "        else:\n",
    "            pseudotest_preds_to_use=pseudotest_uncalib_preds\n",
    "            pseudovalid_preds_to_use=pseudovalid_uncalib_preds\n",
    "            pseudotest_variance_to_use=pseudotest_uncalib_variance\n",
    "            pseudovalid_variance_to_use=pseudovalid_uncalib_variance\n",
    "\n",
    "        original_weighted_kappa_perf = weighted_kappa_metric(\n",
    "            predprobs=pseudotest_preds_to_use,\n",
    "            true_labels=pseudotest_labels,\n",
    "            weights=quadratic_weights)\n",
    "        \n",
    "        #print(\"\\nPseudotest set weighted kappa\",\n",
    "        #      original_weighted_kappa_perf)\n",
    "        metric_to_baselineperfs[\"weighted_kappa\"].append(\n",
    "            original_weighted_kappa_perf)\n",
    "        original_accuracy_perf = np.mean(\n",
    "            np.argmax(pseudotest_preds_to_use,axis=-1)\n",
    "            ==np.argmax(pseudotest_labels,axis=-1))\n",
    "        #print(\"Pseudotest set accuracy\",original_accuracy_perf)\n",
    "        metric_to_baselineperfs[\"accuracy\"].append(original_accuracy_perf)\n",
    "\n",
    "        for abstention_fraction in abstention_fractions:\n",
    "            #print(\"\\nabstention fraction:\",abstention_fraction)\n",
    "            for abstainer_name, abstainer_factory in abstainer_factories:\n",
    "                abstainer = abstainer_factory(\n",
    "                    valid_labels=pseudovalid_labels,\n",
    "                    valid_posterior=pseudovalid_preds_to_use)\n",
    "                abstainer_priorities = abstainer(\n",
    "                    posterior_probs=pseudotest_preds_to_use,\n",
    "                    uncertainties=pseudotest_variance_to_use)\n",
    "                indices_to_retain = (\n",
    "                    [y[0] for y in sorted(enumerate(abstainer_priorities),\n",
    "                        key=lambda x: x[1])][:int(len(abstainer_priorities)*\n",
    "                                                     (1-abstention_fraction))])\n",
    "                retained_pseudotest_preds = np.array(\n",
    "                    [pseudotest_preds_to_use[i] for i in indices_to_retain])\n",
    "                retained_pseudotest_labels = np.array(\n",
    "                    [pseudotest_labels[i] for i in indices_to_retain])\n",
    "                #print(\"\\nAbstention criterion:\",abstainer_name)\n",
    "                weighted_kappa_perf = weighted_kappa_metric(\n",
    "                    predprobs=retained_pseudotest_preds,\n",
    "                    true_labels=retained_pseudotest_labels,\n",
    "                    weights=quadratic_weights)\n",
    "                #print(\"weighted kappa\", weighted_kappa_perf)\n",
    "                accuracy_perf = (np.mean(np.argmax(\n",
    "                    retained_pseudotest_preds,axis=-1)\n",
    "                    ==np.argmax(retained_pseudotest_labels,axis=-1)))\n",
    "                #print(\"accuracy\", accuracy_perf)\n",
    "\n",
    "                metric_to_fraction_to_method_to_perfs[\"delta_weighted_kappa\"][\n",
    "                    abstention_fraction][abstainer_name].append(\n",
    "                        weighted_kappa_perf-original_weighted_kappa_perf)\n",
    "                metric_to_fraction_to_method_to_perfs[\"delta_accuracy\"][\n",
    "                    abstention_fraction][abstainer_name].append(\n",
    "                        accuracy_perf-original_accuracy_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fh = open(\"balanced_abstention_results.json\", 'w')\n",
    "fh.write(json.dumps({\n",
    "            \"settingsname_to_metric_to_fraction_to_method_to_perfs\":\n",
    "              settingsname_to_metric_to_fraction_to_method_to_perfs,\n",
    "            \"settingsname_to_metric_to_baselineperfs\":\n",
    "              settingsname_to_metric_to_baselineperfs},\n",
    "             sort_keys=True,\n",
    "             indent=4,\n",
    "             separators=(',', ': ')))\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "loaded_data = json.loads(open(\"balanced_abstention_results.json\").read())\n",
    "settingsname_to_metric_to_fraction_to_method_to_perfs =\\\n",
    "    loaded_data[\"settingsname_to_metric_to_fraction_to_method_to_perfs\"]\n",
    "settingsname_to_metric_to_baselineperfs =\\\n",
    "    loaded_data[\"settingsname_to_metric_to_baselineperfs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On comparison group With weight rescaling\n",
      "On metric weighted_kappa\n",
      "\n",
      "Baseline weighted_kappa perfs:\n",
      "0.8147 $\\pm$ 0.0014\n",
      "\n",
      " Latex Table for metric weighted_kappa and group With weight rescaling\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated? & $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & \\textbf{0.0311 $\\pm$ 0.0004} & \\textbf{0.0478 $\\pm$ 0.0005} & \\textbf{0.0603 $\\pm$ 0.0007} & \\textbf{0.07 $\\pm$ 0.0008}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.0303 $\\pm$ 0.0004 & \\textbf{0.0467 $\\pm$ 0.0005} & \\textbf{0.0592 $\\pm$ 0.0007} & 0.0667 $\\pm$ 0.0008\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.0162 $\\pm$ 0.0003 & 0.017 $\\pm$ 0.0006 & 0.015 $\\pm$ 0.0009 & 0.0035 $\\pm$ 0.0015\\\\ \\hline\n",
      "Max Class Prob. & N & 0.0121 $\\pm$ 0.0003 & 0.0137 $\\pm$ 0.0005 & 0.011 $\\pm$ 0.0009 & -0.0013 $\\pm$ 0.0015\\\\ \\hline\n",
      "Entropy & Y & 0.0197 $\\pm$ 0.0004 & 0.0308 $\\pm$ 0.0006 & 0.0334 $\\pm$ 0.0009 & 0.0242 $\\pm$ 0.0013\\\\ \\hline\n",
      "Entropy & N & 0.0197 $\\pm$ 0.0004 & 0.0275 $\\pm$ 0.0006 & 0.0221 $\\pm$ 0.001 & 0.0068 $\\pm$ 0.0014\\\\ \\hline\n",
      "MC Dropout Var. & Y & -0.028 $\\pm$ 0.0008 & -0.0445 $\\pm$ 0.0012 & -0.0543 $\\pm$ 0.0017 & -0.0732 $\\pm$ 0.0026\\\\ \\hline\n",
      "MC Dropout Var. & N & -0.0194 $\\pm$ 0.0006 & -0.0323 $\\pm$ 0.001 & -0.0358 $\\pm$ 0.0015 & -0.0432 $\\pm$ 0.0022\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "0.8277 $\\pm$ 0.0007\n",
      "\n",
      " Latex Table for metric accuracy and group With weight rescaling\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated? & $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.0143 $\\pm$ 0.0002 & 0.0322 $\\pm$ 0.0003 & 0.0402 $\\pm$ 0.0004 & 0.0433 $\\pm$ 0.0005\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.0181 $\\pm$ 0.0001 & 0.0352 $\\pm$ 0.0003 & 0.0432 $\\pm$ 0.0004 & 0.0474 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & Y & \\textbf{0.0229 $\\pm$ 0.0001} & \\textbf{0.0416 $\\pm$ 0.0002} & \\textbf{0.0602 $\\pm$ 0.0003} & 0.0748 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & N & \\textbf{0.0227 $\\pm$ 0.0001} & \\textbf{0.042 $\\pm$ 0.0002} & \\textbf{0.0605 $\\pm$ 0.0003} & 0.0746 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & Y & 0.02 $\\pm$ 0.0002 & 0.04 $\\pm$ 0.0003 & 0.0558 $\\pm$ 0.0004 & 0.0717 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & N & \\textbf{0.0224 $\\pm$ 0.0002} & \\textbf{0.0415 $\\pm$ 0.0003} & 0.0591 $\\pm$ 0.0003 & 0.0751 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.0183 $\\pm$ 0.0002 & 0.0363 $\\pm$ 0.0003 & 0.0557 $\\pm$ 0.0004 & 0.0761 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.0189 $\\pm$ 0.0002 & 0.0376 $\\pm$ 0.0003 & 0.0585 $\\pm$ 0.0003 & \\textbf{0.0776 $\\pm$ 0.0005}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On comparison group With MC dropout\n",
      "On metric weighted_kappa\n",
      "\n",
      "Baseline weighted_kappa perfs:\n",
      "0.815 $\\pm$ 0.0013\n",
      "\n",
      " Latex Table for metric weighted_kappa and group With MC dropout\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated? & $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & \\textbf{0.0307 $\\pm$ 0.0004} & \\textbf{0.0496 $\\pm$ 0.0006} & \\textbf{0.062 $\\pm$ 0.0007} & \\textbf{0.0729 $\\pm$ 0.0009}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & \\textbf{0.03 $\\pm$ 0.0004} & 0.0478 $\\pm$ 0.0005 & \\textbf{0.0608 $\\pm$ 0.0007} & 0.0709 $\\pm$ 0.0009\\\\ \\hline\n",
      "Max Class Prob. & Y & 0.0163 $\\pm$ 0.0004 & 0.0149 $\\pm$ 0.0006 & 0.0127 $\\pm$ 0.001 & -0.0016 $\\pm$ 0.0016\\\\ \\hline\n",
      "Max Class Prob. & N & 0.0134 $\\pm$ 0.0003 & 0.0128 $\\pm$ 0.0006 & 0.0083 $\\pm$ 0.001 & -0.0042 $\\pm$ 0.0015\\\\ \\hline\n",
      "Entropy & Y & 0.0193 $\\pm$ 0.0004 & 0.0288 $\\pm$ 0.0007 & 0.0309 $\\pm$ 0.001 & 0.0215 $\\pm$ 0.0015\\\\ \\hline\n",
      "Entropy & N & 0.0186 $\\pm$ 0.0004 & 0.0272 $\\pm$ 0.0006 & 0.0256 $\\pm$ 0.0009 & 0.005 $\\pm$ 0.0015\\\\ \\hline\n",
      "MC Dropout Var. & Y & -0.0264 $\\pm$ 0.0008 & -0.0426 $\\pm$ 0.0012 & -0.0503 $\\pm$ 0.0016 & -0.0645 $\\pm$ 0.0024\\\\ \\hline\n",
      "MC Dropout Var. & N & -0.0199 $\\pm$ 0.0007 & -0.0329 $\\pm$ 0.001 & -0.0361 $\\pm$ 0.0015 & -0.0435 $\\pm$ 0.0022\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "0.8295 $\\pm$ 0.0007\n",
      "\n",
      " Latex Table for metric accuracy and group With MC dropout\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated? & $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & Y & 0.0134 $\\pm$ 0.0002 & 0.0319 $\\pm$ 0.0003 & 0.0402 $\\pm$ 0.0004 & 0.0445 $\\pm$ 0.0005\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & N & 0.0155 $\\pm$ 0.0002 & 0.0339 $\\pm$ 0.0003 & 0.0425 $\\pm$ 0.0004 & 0.047 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & Y & \\textbf{0.0229 $\\pm$ 0.0001} & \\textbf{0.0415 $\\pm$ 0.0002} & \\textbf{0.0599 $\\pm$ 0.0003} & 0.0744 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & N & \\textbf{0.0228 $\\pm$ 0.0001} & \\textbf{0.0418 $\\pm$ 0.0002} & \\textbf{0.0599 $\\pm$ 0.0003} & 0.0742 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & Y & 0.019 $\\pm$ 0.0002 & 0.0389 $\\pm$ 0.0003 & 0.0544 $\\pm$ 0.0004 & 0.0715 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & N & 0.0215 $\\pm$ 0.0002 & 0.0404 $\\pm$ 0.0002 & 0.0575 $\\pm$ 0.0003 & \\textbf{0.0746 $\\pm$ 0.0005}\\\\ \\hline\n",
      "MC Dropout Var. & Y & 0.0178 $\\pm$ 0.0002 & 0.0352 $\\pm$ 0.0003 & 0.0546 $\\pm$ 0.0004 & \\textbf{0.075 $\\pm$ 0.0005}\\\\ \\hline\n",
      "MC Dropout Var. & N & 0.0182 $\\pm$ 0.0002 & 0.0359 $\\pm$ 0.0003 & 0.0567 $\\pm$ 0.0003 & \\textbf{0.0757 $\\pm$ 0.0004}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(abstention.figure_making_utils)\n",
    "from abstention.figure_making_utils import (\n",
    "    wilcox_srs, get_ustats_mat,\n",
    "    get_tied_top_and_worst_methods)\n",
    "from collections import OrderedDict\n",
    "\n",
    "comparison_groups = OrderedDict([\n",
    "        ('With weight rescaling', [\n",
    "          ('calib_weightrescalepreds', 'expected_delta_weighted_kappa'),\n",
    "          ('uncalib_weightrescalepreds', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_weightrescalepreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('uncalib_weightrescalepreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_weightrescalepreds', 'entropy'),\n",
    "          ('uncalib_weightrescalepreds', 'entropy'),\n",
    "          ('calib_weightrescalepreds', 'variance'),\n",
    "          ('uncalib_weightrescalepreds', 'variance')]),\n",
    "       ('With MC dropout', [\n",
    "          ('calib_mcdrpreds', 'expected_delta_weighted_kappa'),\n",
    "          ('uncalib_mcdrpreds', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_mcdrpreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('uncalib_mcdrpreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_mcdrpreds', 'entropy'),\n",
    "          ('uncalib_mcdrpreds', 'entropy'),\n",
    "          ('calib_mcdrpreds', 'variance'),\n",
    "          ('uncalib_mcdrpreds', 'variance')])\n",
    "    ])\n",
    "\n",
    "friendly_method_names = {\n",
    "    'expected_delta_weighted_kappa': 'E[$\\Delta$Kappa]',\n",
    "    'dist_maxclass_prob_from_one': 'Max Class Prob.',\n",
    "    'entropy': 'Entropy',\n",
    "    'variance': 'MC Dropout Var.'\n",
    "}\n",
    "abstention_fractions = ['0.05', '0.1', '0.15', '0.2']\n",
    "\n",
    "for comparison_group_name in comparison_groups:\n",
    "    \n",
    "    print(\"On comparison group\", comparison_group_name)\n",
    "    \n",
    "    for metric in [\"weighted_kappa\",\n",
    "                   \"accuracy\"]:\n",
    "        print(\"On metric\", metric)\n",
    "        \n",
    "        #gather all the necessary data\n",
    "        settingnmethod_to_baselineperfs = OrderedDict()\n",
    "        settingnmethod_to_abstentionfraction_to_perfs = OrderedDict()\n",
    "        for (settingsname, methodname) in comparison_groups[comparison_group_name]:\n",
    "            settingnmethod = settingsname+\"-\"+methodname\n",
    "            settingnmethod_to_baselineperfs[settingnmethod] =\\\n",
    "                settingsname_to_metric_to_baselineperfs[settingsname][metric]\n",
    "            \n",
    "            \n",
    "            abstentionfraction_to_perfs = OrderedDict()\n",
    "            settingnmethod_to_abstentionfraction_to_perfs[settingnmethod] =\\\n",
    "                abstentionfraction_to_perfs\n",
    "            for abstention_fraction in abstention_fractions:\n",
    "                abstentionfraction_to_perfs[abstention_fraction] = (\n",
    "                    settingsname_to_metric_to_fraction_to_method_to_perfs[\n",
    "                        settingsname][\"delta_\"+metric][abstention_fraction][\n",
    "                        methodname])\n",
    "        \n",
    "        #prepare the table contents\n",
    "        \n",
    "        settingnmethod_to_tablecontents = OrderedDict()\n",
    "        for settingnmethod in settingnmethod_to_baselineperfs:\n",
    "            tablerow = {}\n",
    "            settingnmethod_to_tablecontents[settingnmethod] = tablerow\n",
    "            tablerow['baseline'] = {\n",
    "                'mean': np.mean(settingnmethod_to_baselineperfs[settingnmethod]),\n",
    "                'stderr': np.std(settingnmethod_to_baselineperfs[settingnmethod],\n",
    "                                 ddof=1)/np.sqrt(num_folds)}\n",
    "            tablerow['method'] = settingnmethod.split(\"-\")[1]\n",
    "            tablerow['mcdr'] = \"mcdr\" in settingnmethod.split(\"-\")[0]\n",
    "            tablerow['calib'] = (\"uncalib\" in settingnmethod.split(\"-\")[0])==False\n",
    "        \n",
    "        for abstention_fraction in abstention_fractions:\n",
    "            method_to_perfs = OrderedDict()\n",
    "            for settingnmethod in settingnmethod_to_tablecontents:\n",
    "                perfsdelta = settingnmethod_to_abstentionfraction_to_perfs[\n",
    "                    settingnmethod][abstention_fraction]\n",
    "                method_to_perfs[settingnmethod] = perfsdelta\n",
    "                mean_perfsdelta = np.mean(perfsdelta)\n",
    "                stderr_perfsdelta = (np.std(perfsdelta,ddof=1)/\n",
    "                                     np.sqrt(num_folds))\n",
    "                tablerow = settingnmethod_to_tablecontents[settingnmethod]\n",
    "                tablerow[abstention_fraction] = {\n",
    "                    'mean': mean_perfsdelta,\n",
    "                    'stderr': stderr_perfsdelta}\n",
    "            methods_to_consider = list(method_to_perfs.keys())\n",
    "            ustats_mat = get_ustats_mat(\n",
    "                method_to_perfs,\n",
    "                methods_to_consider,\n",
    "                max_ustat=1275)\n",
    "            tied_top_methods, tied_worst_methods =(\n",
    "                get_tied_top_and_worst_methods(\n",
    "                    ustats_mat,\n",
    "                    methods_to_consider,\n",
    "                    #0.05 threshold for one-sided test when N=119 is 50\n",
    "                    #http://www.real-statistics.com/statistics-tables/wilcoxon-signed-ranks-table/\n",
    "                    threshold=120\n",
    "                ))\n",
    "            tied_top_methods = [methods_to_consider[x]\n",
    "                                for x in tied_top_methods]\n",
    "            #print(abstention_fraction)\n",
    "            #print(tied_top_methods)\n",
    "            for settingnmethod in settingnmethod_to_tablecontents:\n",
    "                settingnmethod_to_tablecontents[\n",
    "                    settingnmethod][abstention_fraction][\n",
    "                    'istop'] = (settingnmethod in tied_top_methods)\n",
    "            #print(settingnmethod_to_tablecontents)\n",
    "            \n",
    "        thestr = \"\\\\begin{tabular}{ | c | c | c | c | c | c | c | }\\n\"\n",
    "        thestr += \"\\\\hline Method & Calibrated? & $\\\\Delta$ @\"\n",
    "        thestr += \" & $\\\\Delta$ @\".join([str(int(100*float(x)))+\"\\\\% Abs.\"\n",
    "                              for x in abstention_fractions])\n",
    "        thestr += \"\\\\\\\\ \\\\hline\\n\"\n",
    "        for settingnmethod in settingnmethod_to_tablecontents:\n",
    "            tablerow = settingnmethod_to_tablecontents[settingnmethod]\n",
    "            thestr += friendly_method_names[tablerow['method']]\n",
    "            thestr += \" & \"+(\"Y\" if tablerow['calib'] else \"N\")\n",
    "            #thestr += \" & \"+(str(np.round(tablerow['baseline']['mean'],4))\n",
    "            #                 +\" $\\\\pm$ \"\n",
    "            #                 +str(np.round(tablerow['baseline']['stderr'],4)))\n",
    "            for abstention_fraction in abstention_fractions:\n",
    "                thestr += (\n",
    "                    \" & \"+\n",
    "                    (\"\\\\textbf{\" if tablerow[abstention_fraction]['istop'] else \"\")\n",
    "                    +str(np.round(tablerow[abstention_fraction]['mean'],4))\n",
    "                    +\" $\\\\pm$ \"\n",
    "                    +str(np.round(tablerow[abstention_fraction]['stderr'],4))\n",
    "                    +(\"}\" if tablerow[abstention_fraction]['istop'] else \"\"))\n",
    "            thestr += \"\\\\\\\\ \\hline\\n\"\n",
    "        thestr += \"\\\\end{tabular}\\n\"\n",
    "        \n",
    "        print(\"\\nBaseline \"+metric+\" perfs:\")\n",
    "        baseline_mean = set(\n",
    "                x['baseline']['mean'] for x in\n",
    "                settingnmethod_to_tablecontents.values())\n",
    "        #assert that all the methods have the same baseline\n",
    "        assert len(baseline_mean)==1\n",
    "        baseline_mean = list(baseline_mean)[0]\n",
    "        baseline_stderr = set(\n",
    "                x['baseline']['stderr'] for x in\n",
    "                settingnmethod_to_tablecontents.values())\n",
    "        assert len(baseline_stderr)==1\n",
    "        baseline_stderr = list(baseline_stderr)[0]\n",
    "        print(np.round(baseline_mean,4),\"$\\\\pm$\",np.round(baseline_stderr,4))\n",
    "        \n",
    "        print(\"\\n Latex Table for metric \"\n",
    "              +metric+\" and group \"+comparison_group_name\n",
    "              +\"\\n\\n\"+thestr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.023462290436192412,\n",
       " 0.0214366488865273,\n",
       " 0.021476275917065357,\n",
       " 0.02205111960379591,\n",
       " 0.023896461911217304,\n",
       " 0.02309351717497876,\n",
       " 0.022030682661979717,\n",
       " 0.021571986856775638,\n",
       " 0.022255034867693535,\n",
       " 0.023564593301435455,\n",
       " 0.023848684210526327,\n",
       " 0.023895009298009917,\n",
       " 0.022561272303673885,\n",
       " 0.023088294400959164,\n",
       " 0.022722288676236047,\n",
       " 0.02257453239141738,\n",
       " 0.02234350079744818,\n",
       " 0.024645248751643534,\n",
       " 0.023342393710871767,\n",
       " 0.022606173047142963,\n",
       " 0.023083525781202097,\n",
       " 0.022604393034181935,\n",
       " 0.022345184488815284,\n",
       " 0.023021331738437012,\n",
       " 0.022396509807839582,\n",
       " 0.023595279055228402,\n",
       " 0.023372822046464514,\n",
       " 0.021531680665971775,\n",
       " 0.022360171579480448,\n",
       " 0.022368421052631593,\n",
       " 0.02195972886762365,\n",
       " 0.02309783164047341,\n",
       " 0.023507592323884663,\n",
       " 0.024001598623004328,\n",
       " 0.022489719082880888,\n",
       " 0.022229829306120386,\n",
       " 0.022500051092354534,\n",
       " 0.021511164274322203,\n",
       " 0.02097762734874986,\n",
       " 0.021981953144220356,\n",
       " 0.024266257019521897,\n",
       " 0.02314324706673121,\n",
       " 0.023031299840510444,\n",
       " 0.021946096430574547,\n",
       " 0.022938877648570943,\n",
       " 0.023373276200727044,\n",
       " 0.0213270841706803,\n",
       " 0.023497487391542338,\n",
       " 0.021132376395534225,\n",
       " 0.022522926634768736]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settingsname_to_metric_to_fraction_to_method_to_perfs[\n",
    "    'uncalib_weightrescalepreds']['delta_accuracy']['0.05'][\n",
    "    'dist_maxclass_prob_from_one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.023562090835394067,\n",
       " 0.021536449285728843,\n",
       " 0.021476275917065357,\n",
       " 0.02205111960379591,\n",
       " 0.0244952643064269,\n",
       " 0.023891920368591557,\n",
       " 0.02233008385958457,\n",
       " 0.022170789251985235,\n",
       " 0.02215523446849188,\n",
       " 0.022966507177033524,\n",
       " 0.02444677033492826,\n",
       " 0.02479420002635435,\n",
       " 0.021562171494402205,\n",
       " 0.023387695598563907,\n",
       " 0.022124202551834227,\n",
       " 0.022774133189820578,\n",
       " 0.0227422248803828,\n",
       " 0.024745049150845078,\n",
       " 0.02294319211406537,\n",
       " 0.023205633532705994,\n",
       " 0.022883924982798898,\n",
       " 0.023203195429391532,\n",
       " 0.022844186484823337,\n",
       " 0.023220693779904322,\n",
       " 0.022496419888766717,\n",
       " 0.02299581856966537,\n",
       " 0.023173221248061315,\n",
       " 0.022429884258786115,\n",
       " 0.02216057078107725,\n",
       " 0.022268740031897938,\n",
       " 0.022258771929824617,\n",
       " 0.023497033237279696,\n",
       " 0.023707193122287862,\n",
       " 0.02380199782460113,\n",
       " 0.02209051748607449,\n",
       " 0.021830627709313988,\n",
       " 0.022699651890757733,\n",
       " 0.021112440191387583,\n",
       " 0.021077537429676996,\n",
       " 0.02168222290143884,\n",
       " 0.024266257019521897,\n",
       " 0.022843845869126467,\n",
       " 0.02352970494417872,\n",
       " 0.0222454976281794,\n",
       " 0.02333807924537734,\n",
       " 0.023872278196734986,\n",
       " 0.022325088162696183,\n",
       " 0.02359728779074388,\n",
       " 0.02242822966507174,\n",
       " 0.02391846092503991]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settingsname_to_metric_to_fraction_to_method_to_perfs[\n",
    "    'calib_weightrescalepreds']['delta_accuracy']['0.05'][\n",
    "    'dist_maxclass_prob_from_one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
