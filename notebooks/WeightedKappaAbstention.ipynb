{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "valid_labels = LabelBinarizer().fit_transform(\n",
    "    np.array([float(x.decode(\"utf-8\").split(\"\\t\")[1])\n",
    "              for x in gzip.open(\"valid_labels.txt.gz\",'rb')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#augmenting the dataset with flips and rotaitons, for more robustness\n",
    "parent_folders = [\"flip-False_rotamt-0\",\n",
    "                  \"flip-True_rotamt-0\",\n",
    "                  \"flip-False_rotamt-90\",\n",
    "                  \"flip-True_rotamt-90\",\n",
    "                  \"flip-False_rotamt-180\",\n",
    "                  \"flip-True_rotamt-180\",\n",
    "                  \"flip-False_rotamt-270\",\n",
    "                  \"flip-True_rotamt-270\",]\n",
    "\n",
    "parent_folder_to_det_pred = {}\n",
    "for parent_folder in parent_folders:\n",
    "    det_preds = np.array([\n",
    "            [float(y) for y in x.decode(\"utf-8\").split(\"\\t\")[1:]]\n",
    "             for x in gzip.open(parent_folder+\"/deterministic_preds.txt.gz\", 'rb')])\n",
    "    parent_folder_to_det_pred[parent_folder] = det_preds\n",
    "    \n",
    "parent_folder_to_nondet_pred = {}\n",
    "parent_folder_to_mean_nondet_pred = {}\n",
    "for parent_folder in parent_folders:\n",
    "    nondet_preds = []\n",
    "    for i in range(100):\n",
    "        single_nondet_pred = np.array([\n",
    "            [float(y) for y in x.decode(\"utf-8\").split(\"\\t\")[1:]]\n",
    "             for x in gzip.open(\n",
    "              parent_folder+\"/nondeterministic_preds_\"+str(i)+\".txt.gz\", 'rb')])\n",
    "        nondet_preds.append(single_nondet_pred)\n",
    "    nondet_preds = np.array(nondet_preds)\n",
    "    parent_folder_to_nondet_pred[parent_folder] = nondet_preds\n",
    "    parent_folder_to_mean_nondet_pred[parent_folder] = np.mean(nondet_preds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flip-False_rotamt-0\n",
      "deterministic pred auROC 0.9118638796723656\n",
      "nondeterministic pred auROC 0.9129881925522253\n",
      "flip-True_rotamt-0\n",
      "deterministic pred auROC 0.9136845292158645\n",
      "nondeterministic pred auROC 0.9141930341618936\n",
      "flip-False_rotamt-90\n",
      "deterministic pred auROC 0.9077797755493358\n",
      "nondeterministic pred auROC 0.9079080860318696\n",
      "flip-True_rotamt-90\n",
      "deterministic pred auROC 0.9072243126739039\n",
      "nondeterministic pred auROC 0.9084814670645733\n",
      "flip-False_rotamt-180\n",
      "deterministic pred auROC 0.916166708887612\n",
      "nondeterministic pred auROC 0.9166587373671843\n",
      "flip-True_rotamt-180\n",
      "deterministic pred auROC 0.9131712872857287\n",
      "nondeterministic pred auROC 0.9138490879246036\n",
      "flip-False_rotamt-270\n",
      "deterministic pred auROC 0.9074450973244279\n",
      "nondeterministic pred auROC 0.9075221248051144\n",
      "flip-True_rotamt-270\n",
      "deterministic pred auROC 0.9051834963473733\n",
      "nondeterministic pred auROC 0.9063560028916199\n"
     ]
    }
   ],
   "source": [
    "#Compute the auROC/auPRC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for parent_folder in parent_folders:\n",
    "    print(parent_folder)\n",
    "    det_preds = parent_folder_to_det_pred[parent_folder]\n",
    "    mean_nondet_preds = parent_folder_to_mean_nondet_pred[parent_folder]\n",
    "    print(\"deterministic pred auROC\",\n",
    "          roc_auc_score(y_true=1-valid_labels[:,0],\n",
    "                              y_score=1-det_preds[:,0]))\n",
    "    print(\"nondeterministic pred auROC\",\n",
    "          roc_auc_score(y_true=1-valid_labels[:,0],\n",
    "                              y_score=1-mean_nondet_preds[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstainer settings calib_weightrescalepreds_imbalanced_adapted\n",
      "on fold 0\n",
      "Accuracy before adaptation: 0.9042874933285893\n",
      "WKappa before adaptation: 0.6467983358357531\n",
      "Accuracy after adaptation: 0.9162960327343889\n",
      "WKappa after adaptation: 0.6794150406334958\n",
      "on fold 1\n",
      "Accuracy before adaptation: 0.9134495641344956\n",
      "WKappa before adaptation: 0.6984021000037309\n",
      "Accuracy after adaptation: 0.9235901085216154\n",
      "WKappa after adaptation: 0.7348968956541244\n",
      "on fold 2\n",
      "Accuracy before adaptation: 0.9123944913371834\n",
      "WKappa before adaptation: 0.6891483873770146\n",
      "Accuracy after adaptation: 0.9240337627721013\n",
      "WKappa after adaptation: 0.7244807859497363\n",
      "on fold 3\n",
      "Accuracy before adaptation: 0.912026329834549\n",
      "WKappa before adaptation: 0.6874012421137264\n",
      "Accuracy after adaptation: 0.9211884006404555\n",
      "WKappa after adaptation: 0.7127548591326953\n",
      "on fold 4\n",
      "Accuracy before adaptation: 0.9049991104785625\n",
      "WKappa before adaptation: 0.6740599195369246\n",
      "Accuracy after adaptation: 0.9161181284468956\n",
      "WKappa after adaptation: 0.7162410423661276\n",
      "on fold 5\n",
      "Accuracy before adaptation: 0.9150507027219356\n",
      "WKappa before adaptation: 0.6878850899566745\n",
      "Accuracy after adaptation: 0.9273260985589753\n",
      "WKappa after adaptation: 0.7119385915031877\n",
      "on fold 6\n",
      "Accuracy before adaptation: 0.9136812161080985\n",
      "WKappa before adaptation: 0.6883051327305694\n",
      "Accuracy after adaptation: 0.9204373722108632\n",
      "WKappa after adaptation: 0.7216556525191726\n",
      "on fold 7\n",
      "Accuracy before adaptation: 0.9145116857726828\n",
      "WKappa before adaptation: 0.6981840201063105\n",
      "Accuracy after adaptation: 0.9210877099440149\n",
      "WKappa after adaptation: 0.7140445889808915\n",
      "on fold 8\n",
      "Accuracy before adaptation: 0.9102472869596158\n",
      "WKappa before adaptation: 0.6755989557872963\n",
      "Accuracy after adaptation: 0.9209215442092155\n",
      "WKappa after adaptation: 0.6743272737741584\n",
      "on fold 9\n",
      "Accuracy before adaptation: 0.9117411785619056\n",
      "WKappa before adaptation: 0.7106342641428319\n",
      "Accuracy after adaptation: 0.9186738956537197\n",
      "WKappa after adaptation: 0.7466326121518037\n",
      "on fold 10\n",
      "Accuracy before adaptation: 0.9079192960625723\n",
      "WKappa before adaptation: 0.6906193428473222\n",
      "Accuracy after adaptation: 0.9224068971646965\n",
      "WKappa after adaptation: 0.7175593957336197\n",
      "on fold 11\n",
      "Accuracy before adaptation: 0.9050711743772242\n",
      "WKappa before adaptation: 0.6733721913182518\n",
      "Accuracy after adaptation: 0.9160142348754449\n",
      "WKappa after adaptation: 0.7010747504273862\n",
      "on fold 12\n",
      "Accuracy before adaptation: 0.906372374510502\n",
      "WKappa before adaptation: 0.6476408225701804\n",
      "Accuracy after adaptation: 0.9141153435386259\n",
      "WKappa after adaptation: 0.6829448689747146\n",
      "on fold 13\n",
      "Accuracy before adaptation: 0.9077005157389294\n",
      "WKappa before adaptation: 0.662025436247109\n",
      "Accuracy after adaptation: 0.9202383069535836\n",
      "WKappa after adaptation: 0.6951444420806915\n",
      "on fold 14\n",
      "Accuracy before adaptation: 0.9129275877387828\n",
      "WKappa before adaptation: 0.7155644075770407\n",
      "Accuracy after adaptation: 0.9253665037760995\n",
      "WKappa after adaptation: 0.7292016375402892\n",
      "on fold 15\n",
      "Accuracy before adaptation: 0.9099804305283757\n",
      "WKappa before adaptation: 0.6880961646613233\n",
      "Accuracy after adaptation: 0.9218110656466821\n",
      "WKappa after adaptation: 0.7301435398551694\n",
      "on fold 16\n",
      "Accuracy before adaptation: 0.9068858285206575\n",
      "WKappa before adaptation: 0.6732568087402473\n",
      "Accuracy after adaptation: 0.9187027987561084\n",
      "WKappa after adaptation: 0.7001919752458381\n",
      "on fold 17\n",
      "Accuracy before adaptation: 0.9076033792796798\n",
      "WKappa before adaptation: 0.6680309902040302\n",
      "Accuracy after adaptation: 0.9180080035571365\n",
      "WKappa after adaptation: 0.7075439366572331\n",
      "on fold 18\n",
      "Accuracy before adaptation: 0.9058582985154235\n",
      "WKappa before adaptation: 0.6865246498200128\n",
      "Accuracy after adaptation: 0.9166148102053516\n",
      "WKappa after adaptation: 0.7258370213854135\n",
      "on fold 19\n",
      "Accuracy before adaptation: 0.9098433606265575\n",
      "WKappa before adaptation: 0.6732455677373249\n",
      "Accuracy after adaptation: 0.9196333214667142\n",
      "WKappa after adaptation: 0.694731467313564\n",
      "on fold 20\n",
      "Accuracy before adaptation: 0.9139832769969757\n",
      "WKappa before adaptation: 0.6943209737250083\n",
      "Accuracy after adaptation: 0.925813912115282\n",
      "WKappa after adaptation: 0.7252631149028255\n",
      "on fold 21\n",
      "Accuracy before adaptation: 0.9120810738732332\n",
      "WKappa before adaptation: 0.7154249948748783\n",
      "Accuracy after adaptation: 0.9160814294603965\n",
      "WKappa after adaptation: 0.745993404038063\n",
      "on fold 22\n",
      "Accuracy before adaptation: 0.9104809316383679\n",
      "WKappa before adaptation: 0.6873825877815043\n",
      "Accuracy after adaptation: 0.9234598630989421\n",
      "WKappa after adaptation: 0.7245018587442301\n",
      "on fold 23\n",
      "Accuracy before adaptation: 0.9110301306550529\n",
      "WKappa before adaptation: 0.6827877592019445\n",
      "Accuracy after adaptation: 0.9218736112345569\n",
      "WKappa after adaptation: 0.7082452901964493\n",
      "on fold 24\n",
      "Accuracy before adaptation: 0.9060832443970117\n",
      "WKappa before adaptation: 0.6847947876793052\n",
      "Accuracy after adaptation: 0.9131981501245109\n",
      "WKappa after adaptation: 0.72673651510946\n",
      "on fold 25\n",
      "Accuracy before adaptation: 0.9070284697508897\n",
      "WKappa before adaptation: 0.6790905923272372\n",
      "Accuracy after adaptation: 0.9186832740213523\n",
      "WKappa after adaptation: 0.7089087260275272\n",
      "on fold 26\n",
      "Accuracy before adaptation: 0.9079029247044181\n",
      "WKappa before adaptation: 0.6576342636261342\n",
      "Accuracy after adaptation: 0.918748333185172\n",
      "WKappa after adaptation: 0.6762838283514427\n",
      "on fold 27\n",
      "Accuracy before adaptation: 0.913093755559509\n",
      "WKappa before adaptation: 0.6827919188405582\n",
      "Accuracy after adaptation: 0.9215442092154421\n",
      "WKappa after adaptation: 0.718611478613852\n",
      "on fold 28\n",
      "Accuracy before adaptation: 0.9134803485683799\n",
      "WKappa before adaptation: 0.6931559506885498\n",
      "Accuracy after adaptation: 0.9226391605904322\n",
      "WKappa after adaptation: 0.7319064566721041\n",
      "on fold 29\n",
      "Accuracy before adaptation: 0.916215015548645\n",
      "WKappa before adaptation: 0.7168608269053096\n",
      "Accuracy after adaptation: 0.9266992447800977\n",
      "WKappa after adaptation: 0.7290528175296299\n",
      "on fold 30\n",
      "Accuracy before adaptation: 0.9095839260312945\n",
      "WKappa before adaptation: 0.6724169203269936\n",
      "Accuracy after adaptation: 0.9174964438122333\n",
      "WKappa after adaptation: 0.705607479495423\n",
      "on fold 31\n",
      "Accuracy before adaptation: 0.900409034323315\n",
      "WKappa before adaptation: 0.6398794620736613\n",
      "Accuracy after adaptation: 0.908767561799751\n",
      "WKappa after adaptation: 0.6905562522699782\n",
      "on fold 32\n",
      "Accuracy before adaptation: 0.9109905744264627\n",
      "WKappa before adaptation: 0.6709801430626874\n",
      "Accuracy after adaptation: 0.9182820558420772\n",
      "WKappa after adaptation: 0.7111995447609508\n",
      "on fold 33\n",
      "Accuracy before adaptation: 0.9113462564467366\n",
      "WKappa before adaptation: 0.7102700127633557\n",
      "Accuracy after adaptation: 0.9221945580650898\n",
      "WKappa after adaptation: 0.7260952905803975\n",
      "on fold 34\n",
      "Accuracy before adaptation: 0.9123477642457107\n",
      "WKappa before adaptation: 0.6998243594294576\n",
      "Accuracy after adaptation: 0.9234598630989421\n",
      "WKappa after adaptation: 0.7245950221142081\n",
      "on fold 35\n",
      "Accuracy before adaptation: 0.9088807894035026\n",
      "WKappa before adaptation: 0.6700967605309024\n",
      "Accuracy after adaptation: 0.9204373722108632\n",
      "WKappa after adaptation: 0.7068888746977193\n",
      "on fold 36\n",
      "Accuracy before adaptation: 0.9170592941594808\n",
      "WKappa before adaptation: 0.6955788904090692\n",
      "Accuracy after adaptation: 0.927104631522802\n",
      "WKappa after adaptation: 0.720026966599642\n",
      "on fold 37\n",
      "Accuracy before adaptation: 0.9072082481557195\n",
      "WKappa before adaptation: 0.6844556152274233\n",
      "Accuracy after adaptation: 0.9177850857701537\n",
      "WKappa after adaptation: 0.6953419471867306\n",
      "on fold 38\n",
      "Accuracy before adaptation: 0.9102883588465646\n",
      "WKappa before adaptation: 0.6846130747288297\n",
      "Accuracy after adaptation: 0.9214133143467426\n",
      "WKappa after adaptation: 0.691813868465896\n",
      "on fold 39\n",
      "Accuracy before adaptation: 0.9162811387900356\n",
      "WKappa before adaptation: 0.7057583882150054\n",
      "Accuracy after adaptation: 0.9255338078291815\n",
      "WKappa after adaptation: 0.7244926374663434\n",
      "on fold 40\n",
      "Accuracy before adaptation: 0.9142145968530536\n",
      "WKappa before adaptation: 0.7086609056573008\n",
      "Accuracy after adaptation: 0.9255933860787625\n",
      "WKappa after adaptation: 0.7551883641784761\n",
      "on fold 41\n",
      "Accuracy before adaptation: 0.9095919637301093\n",
      "WKappa before adaptation: 0.6591909047982774\n",
      "Accuracy after adaptation: 0.9193706107209529\n",
      "WKappa after adaptation: 0.6867057325203889\n",
      "on fold 42\n",
      "Accuracy before adaptation: 0.9132521553639676\n",
      "WKappa before adaptation: 0.692207372479989\n",
      "Accuracy after adaptation: 0.9221402541996268\n",
      "WKappa after adaptation: 0.7209723036036748\n",
      "on fold 43\n",
      "Accuracy before adaptation: 0.912026329834549\n",
      "WKappa before adaptation: 0.7068481702158903\n",
      "Accuracy after adaptation: 0.9211884006404555\n",
      "WKappa after adaptation: 0.7218039193985168\n",
      "on fold 44\n",
      "Accuracy before adaptation: 0.9092688133784024\n",
      "WKappa before adaptation: 0.6691922362975247\n",
      "Accuracy after adaptation: 0.9140722291407223\n",
      "WKappa after adaptation: 0.6913874940006463\n",
      "on fold 45\n",
      "Accuracy before adaptation: 0.910279210385915\n",
      "WKappa before adaptation: 0.679071380171712\n",
      "Accuracy after adaptation: 0.9192601813978304\n",
      "WKappa after adaptation: 0.7129655583358872\n",
      "on fold 46\n",
      "Accuracy before adaptation: 0.910158334815869\n",
      "WKappa before adaptation: 0.687891255558019\n",
      "Accuracy after adaptation: 0.923945917096602\n",
      "WKappa after adaptation: 0.7300163213299844\n",
      "on fold 47\n",
      "Accuracy before adaptation: 0.9078455790784558\n",
      "WKappa before adaptation: 0.6625764351859995\n",
      "Accuracy after adaptation: 0.9170076498843622\n",
      "WKappa after adaptation: 0.6902963359707903\n",
      "on fold 48\n",
      "Accuracy before adaptation: 0.9071193671673629\n",
      "WKappa before adaptation: 0.6630346129334452\n",
      "Accuracy after adaptation: 0.9150297751310995\n",
      "WKappa after adaptation: 0.6898967015194903\n",
      "on fold 49\n",
      "Accuracy before adaptation: 0.9108523686783397\n",
      "WKappa before adaptation: 0.7061279775446505\n",
      "Accuracy after adaptation: 0.9182294907119367\n",
      "WKappa after adaptation: 0.7285291577006731\n",
      "abstainer settings calib_weightrescalepreds_imbalanced_unadapted\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n",
      "abstainer settings calib_mcdrpreds_imbalanced_adapted\n",
      "on fold 0\n",
      "Accuracy before adaptation: 0.9042874933285893\n",
      "WKappa before adaptation: 0.6443819757354835\n",
      "Accuracy after adaptation: 0.9173634584593489\n",
      "WKappa after adaptation: 0.6823476541248198\n",
      "on fold 1\n",
      "Accuracy before adaptation: 0.9139832769969757\n",
      "WKappa before adaptation: 0.6974936532120499\n",
      "Accuracy after adaptation: 0.9243906778153353\n",
      "WKappa after adaptation: 0.7312998906733943\n",
      "on fold 2\n",
      "Accuracy before adaptation: 0.9127498889382497\n",
      "WKappa before adaptation: 0.6882248351358285\n",
      "Accuracy after adaptation: 0.925277654375833\n",
      "WKappa after adaptation: 0.7273699283055299\n",
      "on fold 3\n",
      "Accuracy before adaptation: 0.9124710905532823\n",
      "WKappa before adaptation: 0.6891213777013139\n",
      "Accuracy after adaptation: 0.9210994484967088\n",
      "WKappa after adaptation: 0.7127479180535843\n",
      "on fold 4\n",
      "Accuracy before adaptation: 0.9058886319160292\n",
      "WKappa before adaptation: 0.6740233688364277\n",
      "Accuracy after adaptation: 0.9159402241594022\n",
      "WKappa after adaptation: 0.7168387336742821\n",
      "on fold 5\n",
      "Accuracy before adaptation: 0.916029176303149\n",
      "WKappa before adaptation: 0.6889345336794312\n",
      "Accuracy after adaptation: 0.927948763565202\n",
      "WKappa after adaptation: 0.7136783030586713\n",
      "on fold 6\n",
      "Accuracy before adaptation: 0.9141257000622277\n",
      "WKappa before adaptation: 0.6881407548363907\n",
      "Accuracy after adaptation: 0.9214152369099475\n",
      "WKappa after adaptation: 0.7233506922092999\n",
      "on fold 7\n",
      "Accuracy before adaptation: 0.9148671465387008\n",
      "WKappa before adaptation: 0.6972768120621027\n",
      "Accuracy after adaptation: 0.9216209010930418\n",
      "WKappa after adaptation: 0.7110940909019141\n",
      "on fold 8\n",
      "Accuracy before adaptation: 0.9107809998220957\n",
      "WKappa before adaptation: 0.6726861850637809\n",
      "Accuracy after adaptation: 0.9218110656466821\n",
      "WKappa after adaptation: 0.6754033745402426\n",
      "on fold 9\n",
      "Accuracy before adaptation: 0.9123633454804018\n",
      "WKappa before adaptation: 0.708984770441724\n",
      "Accuracy after adaptation: 0.9192960625722159\n",
      "WKappa after adaptation: 0.7475247788492114\n",
      "on fold 10\n",
      "Accuracy before adaptation: 0.9084525819927117\n",
      "WKappa before adaptation: 0.6890089621684236\n",
      "Accuracy after adaptation: 0.9236512310016888\n",
      "WKappa after adaptation: 0.7228692405542141\n",
      "on fold 11\n",
      "Accuracy before adaptation: 0.9059608540925267\n",
      "WKappa before adaptation: 0.673357877083359\n",
      "Accuracy after adaptation: 0.9169928825622776\n",
      "WKappa after adaptation: 0.6984753413535839\n",
      "on fold 12\n",
      "Accuracy before adaptation: 0.9067283730865077\n",
      "WKappa before adaptation: 0.6470844861830247\n",
      "Accuracy after adaptation: 0.9142043431826272\n",
      "WKappa after adaptation: 0.6885677733025466\n",
      "on fold 13\n",
      "Accuracy before adaptation: 0.9085897207896141\n",
      "WKappa before adaptation: 0.6633432185869894\n",
      "Accuracy after adaptation: 0.9206829094789258\n",
      "WKappa after adaptation: 0.6941409427033292\n",
      "on fold 14\n",
      "Accuracy before adaptation: 0.9133718347401155\n",
      "WKappa before adaptation: 0.7143802241381303\n",
      "Accuracy after adaptation: 0.9258107507774322\n",
      "WKappa after adaptation: 0.7242564463056429\n",
      "on fold 15\n",
      "Accuracy before adaptation: 0.910158334815869\n",
      "WKappa before adaptation: 0.6843123131657574\n",
      "Accuracy after adaptation: 0.9218110656466821\n",
      "WKappa after adaptation: 0.7307856668972568\n",
      "on fold 16\n",
      "Accuracy before adaptation: 0.9074189249222567\n",
      "WKappa before adaptation: 0.6712355240565138\n",
      "Accuracy after adaptation: 0.9192358951577076\n",
      "WKappa after adaptation: 0.7001148636478354\n",
      "on fold 17\n",
      "Accuracy before adaptation: 0.9086705202312139\n",
      "WKappa before adaptation: 0.6662643038690664\n",
      "Accuracy after adaptation: 0.9179190751445087\n",
      "WKappa after adaptation: 0.7074803480789333\n",
      "on fold 18\n",
      "Accuracy before adaptation: 0.9061249888879012\n",
      "WKappa before adaptation: 0.6823179367324584\n",
      "Accuracy after adaptation: 0.9165259134145257\n",
      "WKappa after adaptation: 0.7204911821221973\n",
      "on fold 19\n",
      "Accuracy before adaptation: 0.9096653613385547\n",
      "WKappa before adaptation: 0.6716874359586608\n",
      "Accuracy after adaptation: 0.9211463154147383\n",
      "WKappa after adaptation: 0.7012850543052689\n",
      "on fold 20\n",
      "Accuracy before adaptation: 0.9138053727094823\n",
      "WKappa before adaptation: 0.694144475876651\n",
      "Accuracy after adaptation: 0.9267923856964952\n",
      "WKappa after adaptation: 0.726757603610836\n",
      "on fold 21\n",
      "Accuracy before adaptation: 0.9128811449906659\n",
      "WKappa before adaptation: 0.7164295698551644\n",
      "Accuracy after adaptation: 0.9157258422970931\n",
      "WKappa after adaptation: 0.7469827740397987\n",
      "on fold 22\n",
      "Accuracy before adaptation: 0.9111921059649747\n",
      "WKappa before adaptation: 0.6849068192539114\n",
      "Accuracy after adaptation: 0.9236376566805938\n",
      "WKappa after adaptation: 0.7224544089368117\n",
      "on fold 23\n",
      "Accuracy before adaptation: 0.9110301306550529\n",
      "WKappa before adaptation: 0.6816749668899715\n",
      "Accuracy after adaptation: 0.9221402541996268\n",
      "WKappa after adaptation: 0.7071188228937579\n",
      "on fold 24\n",
      "Accuracy before adaptation: 0.906350053361793\n",
      "WKappa before adaptation: 0.6834901651297878\n",
      "Accuracy after adaptation: 0.9136428317324795\n",
      "WKappa after adaptation: 0.7244218437606671\n",
      "on fold 25\n",
      "Accuracy before adaptation: 0.9074733096085409\n",
      "WKappa before adaptation: 0.6766911194124661\n",
      "Accuracy after adaptation: 0.9205516014234876\n",
      "WKappa after adaptation: 0.7100000107727393\n",
      "on fold 26\n",
      "Accuracy before adaptation: 0.9082585118677216\n",
      "WKappa before adaptation: 0.6561594193664924\n",
      "Accuracy after adaptation: 0.919992888256734\n",
      "WKappa after adaptation: 0.6785257142553714\n",
      "on fold 27\n",
      "Accuracy before adaptation: 0.9134495641344956\n",
      "WKappa before adaptation: 0.679812930200857\n",
      "Accuracy after adaptation: 0.9218110656466821\n",
      "WKappa after adaptation: 0.7231004838638055\n",
      "on fold 28\n",
      "Accuracy before adaptation: 0.9143695536190646\n",
      "WKappa before adaptation: 0.6906123496410057\n",
      "Accuracy after adaptation: 0.9233505246309799\n",
      "WKappa after adaptation: 0.7303615288956505\n",
      "on fold 29\n",
      "Accuracy before adaptation: 0.9163038649489116\n",
      "WKappa before adaptation: 0.71314860491037\n",
      "Accuracy after adaptation: 0.9268769435806309\n",
      "WKappa after adaptation: 0.7240570353531716\n",
      "on fold 30\n",
      "Accuracy before adaptation: 0.9094061166429588\n",
      "WKappa before adaptation: 0.6700400741026867\n",
      "Accuracy after adaptation: 0.9184743954480796\n",
      "WKappa after adaptation: 0.7088942299111184\n",
      "on fold 31\n",
      "Accuracy before adaptation: 0.9011203983638627\n",
      "WKappa before adaptation: 0.6395303153297458\n",
      "Accuracy after adaptation: 0.908945402809888\n",
      "WKappa after adaptation: 0.6909321199638288\n",
      "on fold 32\n",
      "Accuracy before adaptation: 0.9113462564467366\n",
      "WKappa before adaptation: 0.6683601862488784\n",
      "Accuracy after adaptation: 0.918993419882625\n",
      "WKappa after adaptation: 0.7179887034681234\n",
      "on fold 33\n",
      "Accuracy before adaptation: 0.9119686999822159\n",
      "WKappa before adaptation: 0.7056453863092486\n",
      "Accuracy after adaptation: 0.9231726836208429\n",
      "WKappa after adaptation: 0.7260409726611529\n",
      "on fold 34\n",
      "Accuracy before adaptation: 0.9131478353631434\n",
      "WKappa before adaptation: 0.6992409592660411\n",
      "Accuracy after adaptation: 0.924171037425549\n",
      "WKappa after adaptation: 0.7236164292463002\n",
      "on fold 35\n",
      "Accuracy before adaptation: 0.9094141701484576\n",
      "WKappa before adaptation: 0.6685316207806532\n",
      "Accuracy after adaptation: 0.920526269001689\n",
      "WKappa after adaptation: 0.7041800052401888\n",
      "on fold 36\n",
      "Accuracy before adaptation: 0.9176815716952618\n",
      "WKappa before adaptation: 0.6946280549436035\n",
      "Accuracy after adaptation: 0.9279935994310605\n",
      "WKappa after adaptation: 0.7182271035059675\n",
      "on fold 37\n",
      "Accuracy before adaptation: 0.9073860101324327\n",
      "WKappa before adaptation: 0.6820814765088824\n",
      "Accuracy after adaptation: 0.9184072526886499\n",
      "WKappa after adaptation: 0.6934733287810995\n",
      "on fold 38\n",
      "Accuracy before adaptation: 0.9101993592025632\n",
      "WKappa before adaptation: 0.6816998370743053\n",
      "Accuracy after adaptation: 0.9223033107867569\n",
      "WKappa after adaptation: 0.6870008682528237\n",
      "on fold 39\n",
      "Accuracy before adaptation: 0.9166370106761565\n",
      "WKappa before adaptation: 0.7023804559843292\n",
      "Accuracy after adaptation: 0.9252669039145908\n",
      "WKappa after adaptation: 0.719217046085996\n",
      "on fold 40\n",
      "Accuracy before adaptation: 0.9140368032714019\n",
      "WKappa before adaptation: 0.7036425846555293\n",
      "Accuracy after adaptation: 0.9269268379411503\n",
      "WKappa after adaptation: 0.7554441534607911\n",
      "on fold 41\n",
      "Accuracy before adaptation: 0.909769757311761\n",
      "WKappa before adaptation: 0.6569977087763902\n",
      "Accuracy after adaptation: 0.9207929593741666\n",
      "WKappa after adaptation: 0.6923146760679604\n",
      "on fold 42\n",
      "Accuracy before adaptation: 0.9143187272242468\n",
      "WKappa before adaptation: 0.6925868089456584\n",
      "Accuracy after adaptation: 0.9241845169318282\n",
      "WKappa after adaptation: 0.7219416089431716\n",
      "on fold 43\n",
      "Accuracy before adaptation: 0.9126489948407757\n",
      "WKappa before adaptation: 0.7049276294548794\n",
      "Accuracy after adaptation: 0.9218110656466821\n",
      "WKappa after adaptation: 0.7170919467854996\n",
      "on fold 44\n",
      "Accuracy before adaptation: 0.9093577655221491\n",
      "WKappa before adaptation: 0.668777774528165\n",
      "Accuracy after adaptation: 0.9144280377157089\n",
      "WKappa after adaptation: 0.695378520727012\n",
      "on fold 45\n",
      "Accuracy before adaptation: 0.9104570513960519\n",
      "WKappa before adaptation: 0.6768220349572667\n",
      "Accuracy after adaptation: 0.9197047839231727\n",
      "WKappa after adaptation: 0.7134338480764915\n",
      "on fold 46\n",
      "Accuracy before adaptation: 0.9110478562533357\n",
      "WKappa before adaptation: 0.6858163884800246\n",
      "Accuracy after adaptation: 0.925280199252802\n",
      "WKappa after adaptation: 0.7339269991631829\n",
      "on fold 47\n",
      "Accuracy before adaptation: 0.9081124355096958\n",
      "WKappa before adaptation: 0.6615787865373292\n",
      "Accuracy after adaptation: 0.9170966020281088\n",
      "WKappa after adaptation: 0.689494273474839\n",
      "on fold 48\n",
      "Accuracy before adaptation: 0.9073860101324327\n",
      "WKappa before adaptation: 0.6638338789965448\n",
      "Accuracy after adaptation: 0.9150297751310995\n",
      "WKappa after adaptation: 0.6891781072883618\n",
      "on fold 49\n",
      "Accuracy before adaptation: 0.9104968447249133\n",
      "WKappa before adaptation: 0.7029106772755265\n",
      "Accuracy after adaptation: 0.9185850146653631\n",
      "WKappa after adaptation: 0.7281734829095405\n",
      "abstainer settings calib_mcdrpreds_imbalanced_unadapted\n",
      "on fold 0\n",
      "on fold 1\n",
      "on fold 2\n",
      "on fold 3\n",
      "on fold 4\n",
      "on fold 5\n",
      "on fold 6\n",
      "on fold 7\n",
      "on fold 8\n",
      "on fold 9\n",
      "on fold 10\n",
      "on fold 11\n",
      "on fold 12\n",
      "on fold 13\n",
      "on fold 14\n",
      "on fold 15\n",
      "on fold 16\n",
      "on fold 17\n",
      "on fold 18\n",
      "on fold 19\n",
      "on fold 20\n",
      "on fold 21\n",
      "on fold 22\n",
      "on fold 23\n",
      "on fold 24\n",
      "on fold 25\n",
      "on fold 26\n",
      "on fold 27\n",
      "on fold 28\n",
      "on fold 29\n",
      "on fold 30\n",
      "on fold 31\n",
      "on fold 32\n",
      "on fold 33\n",
      "on fold 34\n",
      "on fold 35\n",
      "on fold 36\n",
      "on fold 37\n",
      "on fold 38\n",
      "on fold 39\n",
      "on fold 40\n",
      "on fold 41\n",
      "on fold 42\n",
      "on fold 43\n",
      "on fold 44\n",
      "on fold 45\n",
      "on fold 46\n",
      "on fold 47\n",
      "on fold 48\n",
      "on fold 49\n"
     ]
    }
   ],
   "source": [
    "import abstention\n",
    "reload(abstention.abstention)\n",
    "reload(abstention.calibration)\n",
    "from abstention.calibration import (compute_ece, TempScaling,\n",
    "                                    EMImbalanceAdapter)\n",
    "from abstention.abstention import (weighted_kappa_metric,\n",
    "                                   WeightedKappa, DistMaxClassProbFromOne,\n",
    "                                   Entropy, Uncertainty)\n",
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def inverse_softmax(preds):\n",
    "    return np.log(preds) - np.mean(np.log(preds),axis=1)[:,None]\n",
    "\n",
    "quadratic_weights = np.array([[(i-j)**2 for i in range(5)]\n",
    "                             for j in range(5)])\n",
    "\n",
    "AbstainerSettings = namedtuple(\"AbstainerSettings\",\n",
    "                               [\"name\",\n",
    "                                \"abstainer_factories\",\n",
    "                                \"preds_lookup\",\n",
    "                                \"predsamples_lookup\",\n",
    "                                \"use_calib\",\n",
    "                                \"imbalance_subsampling\",\n",
    "                                \"imbalance_adapter\"])\n",
    "\n",
    "abstainer_factories = [\n",
    "        (\"expected_delta_weighted_kappa\", WeightedKappa(\n",
    "            weights=quadratic_weights, verbose=False)),\n",
    "        (\"expected_delta_weighted_kappa_imbalance_from_valid\", WeightedKappa(\n",
    "                weights=quadratic_weights,\n",
    "                estimate_class_imbalance_from_valid=True,\n",
    "                verbose=False)),\n",
    "        (\"dist_maxclass_prob_from_one\", DistMaxClassProbFromOne()),\n",
    "        (\"entropy\", Entropy()),\n",
    "        (\"variance\", Uncertainty())]\n",
    "abstention_fractions = [0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "abstainer_settings_list = [\n",
    "    AbstainerSettings(\n",
    "        name=\"calib_weightrescalepreds_imbalanced_adapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=True,\n",
    "        imbalance_subsampling=[8, 5, 1, 1, 1], #these are upsample factors\n",
    "        imbalance_adapter=EMImbalanceAdapter(verbose=False)),\n",
    "    AbstainerSettings(\n",
    "        name=\"calib_weightrescalepreds_imbalanced_unadapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_det_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=True,\n",
    "        imbalance_subsampling=[8, 5, 1, 1, 1],\n",
    "        imbalance_adapter=None),\n",
    "    AbstainerSettings(\n",
    "        name=\"calib_mcdrpreds_imbalanced_adapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=True,\n",
    "        imbalance_subsampling=[8, 5, 1, 1, 1],\n",
    "        imbalance_adapter=EMImbalanceAdapter(verbose=False)),\n",
    "    AbstainerSettings(\n",
    "        name=\"calib_mcdrpreds_imbalanced_unadapted\",\n",
    "        abstainer_factories=abstainer_factories,\n",
    "        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "        use_calib=True,\n",
    "        imbalance_subsampling=[8, 5, 1, 1, 1],\n",
    "        imbalance_adapter=None),\n",
    "#    AbstainerSettings(\n",
    "#        name=\"calib_weightrescalepreds\",\n",
    "#        abstainer_factories=abstainer_factories,\n",
    "#        preds_lookup=parent_folder_to_det_pred,\n",
    "#        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "#        use_calib=True,\n",
    "#        imbalance_subsampling=None),  \n",
    "#    AbstainerSettings(\n",
    "#        name=\"uncalib_weightrescalepreds\",\n",
    "#        abstainer_factories=abstainer_factories,\n",
    "#        preds_lookup=parent_folder_to_det_pred,\n",
    "#        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "#        use_calib=False,\n",
    "#        imbalance_subsampling=None),\n",
    "#    AbstainerSettings(\n",
    "#        name=\"calib_mcdrpreds\",\n",
    "#        abstainer_factories=abstainer_factories,\n",
    "#        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "#        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "#        use_calib=True,\n",
    "#        imbalance_subsampling=None),\n",
    "#    AbstainerSettings(\n",
    "#        name=\"uncalib_mcdrpreds\",\n",
    "#        abstainer_factories=abstainer_factories,\n",
    "#        preds_lookup=parent_folder_to_mean_nondet_pred,\n",
    "#        predsamples_lookup=parent_folder_to_nondet_pred,\n",
    "#        use_calib=False,\n",
    "#        imbalance_subsampling=None)\n",
    "]\n",
    "\n",
    "num_folds = 50\n",
    "\n",
    "settingsname_to_metric_to_fraction_to_method_to_perfs = {}\n",
    "settingsname_to_metric_to_baselineperfs = {}\n",
    "\n",
    "for abstainer_settings in abstainer_settings_list:\n",
    "    \n",
    "    settings_name = abstainer_settings.name\n",
    "    print(\"abstainer settings\", settings_name)\n",
    "    abstainer_factories = abstainer_settings.abstainer_factories\n",
    "    preds_lookup = abstainer_settings.preds_lookup\n",
    "    predsamples_lookup = abstainer_settings.predsamples_lookup\n",
    "    use_calib = abstainer_settings.use_calib\n",
    "    imbalance_subsampling = abstainer_settings.imbalance_subsampling\n",
    "    imbalance_adapter = abstainer_settings.imbalance_adapter\n",
    "    \n",
    "    metric_to_fraction_to_method_to_perfs =\\\n",
    "        defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    settingsname_to_metric_to_fraction_to_method_to_perfs[settings_name] =\\\n",
    "        metric_to_fraction_to_method_to_perfs\n",
    "    metric_to_baselineperfs = defaultdict(list)   \n",
    "    settingsname_to_metric_to_baselineperfs[settings_name] =\\\n",
    "        metric_to_baselineperfs\n",
    "    \n",
    "    for fold_number in range(num_folds):\n",
    "        print(\"on fold\",fold_number)\n",
    "\n",
    "        np.random.seed(fold_number*1000)\n",
    "        random.seed(fold_number*1000)\n",
    "        #the data is in pairs of (left eye, right eye) per patient (entry for\n",
    "        # the right eye comes after the entry for the left eye); hence, the number of\n",
    "        # unique patients is 0.5*len(valid_labels)\n",
    "        patient_id_ordering = list(range(int(0.5*len(valid_labels))))\n",
    "        np.random.shuffle(patient_id_ordering)\n",
    "\n",
    "        pseudovalid_uncalib_preds = []\n",
    "        pseudotest_uncalib_preds = []\n",
    "        pseudovalid_uncalib_predsamples = []\n",
    "        pseudotest_uncalib_predsamples = []\n",
    "        pseudovalid_labels = []\n",
    "        pseudotest_labels = []\n",
    "        pseudovalid_label_counts = np.zeros(5)\n",
    "        pseudotest_label_counts = np.zeros(5)\n",
    "        for i in patient_id_ordering:\n",
    "            left_eye_label = valid_labels[2*i]\n",
    "            right_eye_label = valid_labels[(2*i)+1]\n",
    "            most_diseased_label = max(np.argmax(left_eye_label),\n",
    "                                      np.argmax(right_eye_label))\n",
    "            if (pseudovalid_label_counts[most_diseased_label] <\n",
    "                pseudotest_label_counts[most_diseased_label]):\n",
    "                in_test = False\n",
    "                append_to_uncalib_preds = pseudovalid_uncalib_preds\n",
    "                append_to_uncalib_predsamples = pseudovalid_uncalib_predsamples\n",
    "                append_to_labels = pseudovalid_labels\n",
    "                append_to_label_counts = pseudovalid_label_counts\n",
    "            else:\n",
    "                in_test = True\n",
    "                append_to_uncalib_preds = pseudotest_uncalib_preds\n",
    "                append_to_uncalib_predsamples = pseudotest_uncalib_predsamples\n",
    "                append_to_labels = pseudotest_labels\n",
    "                append_to_label_counts = pseudotest_label_counts\n",
    "            \n",
    "            append_to_label_counts += valid_labels[2*i]\n",
    "            append_to_label_counts += valid_labels[(2*i)+1]\n",
    "            for parent_folder_idx,parent_folder in enumerate(parent_folders):\n",
    "                if ((not in_test) or\n",
    "                    imbalance_subsampling[np.argmax(valid_labels[2*i])] > parent_folder_idx):\n",
    "                    append_to_labels.append(valid_labels[2*i])\n",
    "                    append_to_uncalib_preds.append(\n",
    "                            preds_lookup[parent_folder][2*i])\n",
    "                    append_to_uncalib_predsamples.append(\n",
    "                        predsamples_lookup[parent_folder][:,(2*i)])                    \n",
    "                if ((not in_test) or\n",
    "                    imbalance_subsampling[np.argmax(valid_labels[(2*i) + 1])] > parent_folder_idx): \n",
    "                    append_to_labels.append(valid_labels[(2*i)+1])\n",
    "                    append_to_uncalib_preds.append(\n",
    "                        preds_lookup[parent_folder][(2*i)+1])\n",
    "                    append_to_uncalib_predsamples.append(\n",
    "                        predsamples_lookup[parent_folder][:,(2*i)+1])\n",
    "                \n",
    "        pseudovalid_uncalib_preds = np.array(pseudovalid_uncalib_preds)\n",
    "        pseudotest_uncalib_preds = np.array(pseudotest_uncalib_preds)\n",
    "        pseudovalid_uncalib_pred_logits = inverse_softmax(pseudovalid_uncalib_preds)\n",
    "        pseudotest_uncalib_pred_logits = inverse_softmax(pseudotest_uncalib_preds)\n",
    "        pseudovalid_uncalib_predsamples = np.array(pseudovalid_uncalib_predsamples).transpose((1,0,2))\n",
    "        pseudotest_uncalib_predsamples = np.array(pseudotest_uncalib_predsamples).transpose((1,0,2))\n",
    "        pseudovalid_uncalib_predsamples_logits = np.array([\n",
    "                inverse_softmax(x) for x in pseudovalid_uncalib_predsamples])        \n",
    "        pseudotest_uncalib_predsamples_logits = np.array([\n",
    "                inverse_softmax(x) for x in pseudotest_uncalib_predsamples])\n",
    "        pseudovalid_labels = np.array(pseudovalid_labels) \n",
    "        pseudotest_labels = np.array(pseudotest_labels)\n",
    "        \n",
    "        \n",
    "        if (use_calib):\n",
    "            #print(\"ece before temp scale - valid\",\n",
    "            #  compute_ece(softmax_out=pseudovalid_uncalib_preds,\n",
    "            #              labels=pseudovalid_labels,\n",
    "            #              bins=15))\n",
    "            #print(\"ece before temp scale - test\",\n",
    "            #      compute_ece(softmax_out=pseudotest_uncalib_preds,\n",
    "            #                  labels=pseudotest_labels,\n",
    "            #                  bins=15))\n",
    "            temp_scaler = TempScaling(ece_bins=15, verbose=False)(\n",
    "                                valid_preacts=pseudovalid_uncalib_pred_logits,\n",
    "                                valid_labels=pseudovalid_labels)\n",
    "            pseudovalid_calib_preds = temp_scaler(pseudovalid_uncalib_pred_logits)\n",
    "            pseudotest_calib_preds = temp_scaler(pseudotest_uncalib_pred_logits)\n",
    "            #print(\"Distribution shift from true labels after calibration:\",\n",
    "            #      \"True:\",np.mean(pseudovalid_labels,axis=0),\n",
    "            #      \"Estimated:\",np.mean(pseudovalid_calib_preds,axis=0),\n",
    "            #      \"Difference:\", np.mean(pseudovalid_labels-pseudovalid_calib_preds,\n",
    "            #                             axis=0))\n",
    "            pseudovalid_calib_predsamples = np.array(\n",
    "                [temp_scaler(x) for x in pseudovalid_uncalib_predsamples_logits])\n",
    "            pseudotest_calib_predsamples = np.array(\n",
    "                [temp_scaler(x) for x in pseudotest_uncalib_predsamples_logits])       \n",
    "            #print(\"ece after temp scale - valid\",\n",
    "            #      compute_ece(softmax_out=pseudovalid_calib_preds,\n",
    "            #            labels=pseudovalid_labels,\n",
    "            #            bins=15))\n",
    "            #print(\"ece after temp scale - test\",\n",
    "            #      compute_ece(softmax_out=pseudotest_calib_preds,\n",
    "            #            labels=pseudotest_labels,\n",
    "            #            bins=15))\n",
    "            \n",
    "        if (use_calib):\n",
    "            pseudotest_preds_to_use=pseudotest_calib_preds\n",
    "            pseudovalid_preds_to_use=pseudovalid_calib_preds\n",
    "            pseudotest_predsamples_to_use=pseudotest_calib_predsamples\n",
    "            pseudovalid_predsamples_to_use=pseudovalid_calib_predsamples\n",
    "        else:\n",
    "            pseudotest_preds_to_use=pseudotest_uncalib_preds\n",
    "            pseudovalid_preds_to_use=pseudovalid_uncalib_preds\n",
    "            pseudotest_predsamples_to_use=pseudotest_uncalib_predsamples\n",
    "            pseudovalid_predsamples_to_use=pseudovalid_uncalib_predsamples\n",
    "        \n",
    "        if (imbalance_adapter is not None):\n",
    "            imbalance_adaptation_func = imbalance_adapter(\n",
    "                #set the validation labels to be pseudovalid_preds_to_use\n",
    "                # (rather than pseudovalid_labels) for consistency;\n",
    "                # we want no adjustment to happen in the\n",
    "                # case where tofit_initial_posterior_probs=pseudovalid_preds_to_use\n",
    "                valid_labels=pseudovalid_preds_to_use,\n",
    "                tofit_initial_posterior_probs=pseudotest_preds_to_use)\n",
    "            preds_before_adaptation = pseudotest_preds_to_use\n",
    "            pseudotest_preds_to_use = imbalance_adaptation_func(pseudotest_preds_to_use)\n",
    "            print(\"Accuracy before adaptation:\",\n",
    "                  np.mean(np.argmax(preds_before_adaptation,axis=-1)\n",
    "                          ==np.argmax(pseudotest_labels,axis=-1)))\n",
    "            print(\"WKappa before adaptation:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=preds_before_adaptation,\n",
    "                        true_labels=pseudotest_labels,\n",
    "                        weights=quadratic_weights))\n",
    "            print(\"Accuracy after adaptation:\",\n",
    "                  np.mean(np.argmax(pseudotest_preds_to_use,axis=-1)\n",
    "                          ==np.argmax(pseudotest_labels,axis=-1)))\n",
    "            print(\"WKappa after adaptation:\",\n",
    "                  weighted_kappa_metric(\n",
    "                        predprobs=pseudotest_preds_to_use,\n",
    "                        true_labels=pseudotest_labels,\n",
    "                        weights=quadratic_weights))\n",
    "            #print((zip(preds_before_adaptation,pseudotest_preds_to_use))[:20])\n",
    "            \n",
    "            #print(\"Difference from true imbalance\",\n",
    "            #      np.mean(pseudotest_preds_to_use,axis=0)-\n",
    "            #      np.mean(pseudotest_labels,axis=0))\n",
    "            pseudotest_predsamples_to_use = np.array([\n",
    "                    imbalance_adaptation_func(x) for\n",
    "                    x in pseudotest_predsamples_to_use])\n",
    "\n",
    "        pseudovalid_variance_to_use = np.sum(np.var(pseudovalid_predsamples_to_use, axis=0),\n",
    "                                             axis=-1)\n",
    "        pseudotest_variance_to_use = np.sum(np.var(pseudotest_predsamples_to_use, axis=0),\n",
    "                                            axis=-1)\n",
    "            \n",
    "        original_weighted_kappa_perf = weighted_kappa_metric(\n",
    "            predprobs=pseudotest_preds_to_use,\n",
    "            true_labels=pseudotest_labels,\n",
    "            weights=quadratic_weights)\n",
    "        \n",
    "        #print(\"\\nPseudotest set weighted kappa\",\n",
    "        #      original_weighted_kappa_perf)\n",
    "        metric_to_baselineperfs[\"weighted_kappa\"].append(\n",
    "            original_weighted_kappa_perf)\n",
    "        original_accuracy_perf = np.mean(\n",
    "            np.argmax(pseudotest_preds_to_use,axis=-1)\n",
    "            ==np.argmax(pseudotest_labels,axis=-1))\n",
    "        #print(\"Pseudotest set accuracy\",original_accuracy_perf)\n",
    "        metric_to_baselineperfs[\"accuracy\"].append(original_accuracy_perf)\n",
    "        \n",
    "        for abstention_fraction in abstention_fractions:\n",
    "            #print(\"\\nabstention fraction:\",abstention_fraction)\n",
    "            for abstainer_name, abstainer_factory in abstainer_factories:\n",
    "                abstainer = abstainer_factory(\n",
    "                    valid_labels=pseudovalid_labels,\n",
    "                    valid_posterior=pseudovalid_preds_to_use)\n",
    "                abstainer_priorities = abstainer(\n",
    "                    posterior_probs=pseudotest_preds_to_use,\n",
    "                    uncertainties=pseudotest_variance_to_use)\n",
    "                indices_to_retain = (\n",
    "                    [y[0] for y in sorted(enumerate(abstainer_priorities),\n",
    "                        key=lambda x: x[1])][:int(len(abstainer_priorities)*\n",
    "                                                     (1-abstention_fraction))])\n",
    "                retained_pseudotest_preds = np.array(\n",
    "                    [pseudotest_preds_to_use[i] for i in indices_to_retain])\n",
    "                retained_pseudotest_labels = np.array(\n",
    "                    [pseudotest_labels[i] for i in indices_to_retain])\n",
    "                #print(\"\\nAbstention criterion:\",abstainer_name)\n",
    "                weighted_kappa_perf = weighted_kappa_metric(\n",
    "                    predprobs=retained_pseudotest_preds,\n",
    "                    true_labels=retained_pseudotest_labels,\n",
    "                    weights=quadratic_weights)\n",
    "                #print(\"weighted kappa\", weighted_kappa_perf)\n",
    "                accuracy_perf = (np.mean(np.argmax(\n",
    "                    retained_pseudotest_preds,axis=-1)\n",
    "                    ==np.argmax(retained_pseudotest_labels,axis=-1)))\n",
    "                #print(\"accuracy\", accuracy_perf)\n",
    "\n",
    "                metric_to_fraction_to_method_to_perfs[\"delta_weighted_kappa\"][\n",
    "                    abstention_fraction][abstainer_name].append(\n",
    "                        weighted_kappa_perf-original_weighted_kappa_perf)\n",
    "                metric_to_fraction_to_method_to_perfs[\"delta_accuracy\"][\n",
    "                    abstention_fraction][abstainer_name].append(\n",
    "                        accuracy_perf-original_accuracy_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fh = open(\"imbalanced_abstention_results.json\", 'w')\n",
    "fh.write(json.dumps({\n",
    "            \"settingsname_to_metric_to_fraction_to_method_to_perfs\":\n",
    "              settingsname_to_metric_to_fraction_to_method_to_perfs,\n",
    "            \"settingsname_to_metric_to_baselineperfs\":\n",
    "              settingsname_to_metric_to_baselineperfs},\n",
    "             sort_keys=True,\n",
    "             indent=4,\n",
    "             separators=(',', ': ')))\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "loaded_data = json.loads(open(\"balanced_abstention_results.json\").read())\n",
    "settingsname_to_metric_to_fraction_to_method_to_perfs.update(\n",
    "    loaded_data[\"settingsname_to_metric_to_fraction_to_method_to_perfs\"])\n",
    "settingsname_to_metric_to_baselineperfs.update(\n",
    "    loaded_data[\"settingsname_to_metric_to_baselineperfs\"])\n",
    "\n",
    "import json\n",
    "fh = open(\"abstention_results.json\", 'w')\n",
    "fh.write(json.dumps({\n",
    "            \"settingsname_to_metric_to_fraction_to_method_to_perfs\":\n",
    "              settingsname_to_metric_to_fraction_to_method_to_perfs,\n",
    "            \"settingsname_to_metric_to_baselineperfs\":\n",
    "              settingsname_to_metric_to_baselineperfs},\n",
    "             sort_keys=True,\n",
    "             indent=4,\n",
    "             separators=(',', ': ')))\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "loaded_data = json.loads(open(\"abstention_results.json\").read())\n",
    "settingsname_to_metric_to_fraction_to_method_to_perfs =\\\n",
    "    loaded_data[\"settingsname_to_metric_to_fraction_to_method_to_perfs\"]\n",
    "settingsname_to_metric_to_baselineperfs =\\\n",
    "    loaded_data[\"settingsname_to_metric_to_baselineperfs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On comparison group Imbalanced, with weight rescaling\n",
      "On metric weighted_kappa\n",
      "\n",
      "Baseline weighted_kappa perfs:\n",
      "set([0.683954267251486, 0.7128128728052141])\n",
      "set([0.0026072289917694, 0.002648536560705184])\n",
      "\n",
      " Latex Table for metric weighted_kappa and group Imbalanced, with weight rescaling\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Adapted?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.7128128728052141 & Y & 0.0419 $\\pm$ 0.0009 & \\textbf{0.0654 $\\pm$ 0.0009} & \\textbf{0.0824 $\\pm$ 0.0011} & \\textbf{0.0946 $\\pm$ 0.0013}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.683954267251486 & N & 0.0203 $\\pm$ 0.0004 & 0.0373 $\\pm$ 0.0006 & 0.0512 $\\pm$ 0.0006 & 0.0598 $\\pm$ 0.0007\\\\ \\hline\n",
      "Max Class Prob. & 0.7128128728052141 & Y & -0.0118 $\\pm$ 0.0018 & -0.0649 $\\pm$ 0.0082 & -0.1362 $\\pm$ 0.0109 & -0.1364 $\\pm$ 0.0108\\\\ \\hline\n",
      "Max Class Prob. & 0.683954267251486 & N & 0.0387 $\\pm$ 0.0011 & 0.0226 $\\pm$ 0.0025 & -0.016 $\\pm$ 0.0047 & -0.0482 $\\pm$ 0.0061\\\\ \\hline\n",
      "Entropy & 0.7128128728052141 & Y & -0.0193 $\\pm$ 0.0037 & -0.0993 $\\pm$ 0.0091 & -0.1619 $\\pm$ 0.0106 & -0.157 $\\pm$ 0.0102\\\\ \\hline\n",
      "Entropy & 0.683954267251486 & N & \\textbf{0.0538 $\\pm$ 0.001} & \\textbf{0.0707 $\\pm$ 0.0022} & 0.0112 $\\pm$ 0.0041 & -0.0323 $\\pm$ 0.0062\\\\ \\hline\n",
      "MC Dropout Var. & 0.7128128728052141 & Y & -0.1073 $\\pm$ 0.0057 & -0.2176 $\\pm$ 0.0123 & -0.2866 $\\pm$ 0.013 & -0.2872 $\\pm$ 0.0145\\\\ \\hline\n",
      "MC Dropout Var. & 0.683954267251486 & N & -0.0804 $\\pm$ 0.0027 & -0.1011 $\\pm$ 0.0049 & -0.1844 $\\pm$ 0.0087 & -0.2053 $\\pm$ 0.0105\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.9201927623351813, 0.910258691618873])\n",
      "set([0.0004877984835269973, 0.0005552603542073933])\n",
      "\n",
      " Latex Table for metric accuracy and group Imbalanced, with weight rescaling\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Adapted?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.9201927623351813 & Y & 0.0113 $\\pm$ 0.0003 & 0.0215 $\\pm$ 0.0004 & 0.0262 $\\pm$ 0.0005 & 0.029 $\\pm$ 0.0005\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.910258691618873 & N & 0.0041 $\\pm$ 0.0001 & 0.0096 $\\pm$ 0.0003 & 0.0148 $\\pm$ 0.0003 & 0.0169 $\\pm$ 0.0003\\\\ \\hline\n",
      "Max Class Prob. & 0.9201927623351813 & Y & 0.0246 $\\pm$ 0.0002 & 0.0408 $\\pm$ 0.0004 & 0.0465 $\\pm$ 0.0006 & 0.0498 $\\pm$ 0.0006\\\\ \\hline\n",
      "Max Class Prob. & 0.910258691618873 & N & \\textbf{0.0256 $\\pm$ 0.0001} & 0.0435 $\\pm$ 0.0003 & 0.0534 $\\pm$ 0.0005 & 0.0588 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & 0.9201927623351813 & Y & \\textbf{0.026 $\\pm$ 0.0003} & 0.0411 $\\pm$ 0.0005 & 0.0466 $\\pm$ 0.0006 & 0.0497 $\\pm$ 0.0006\\\\ \\hline\n",
      "Entropy & 0.910258691618873 & N & \\textbf{0.0257 $\\pm$ 0.0002} & 0.0431 $\\pm$ 0.0003 & 0.0533 $\\pm$ 0.0004 & 0.0584 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & 0.9201927623351813 & Y & 0.0241 $\\pm$ 0.0002 & 0.0408 $\\pm$ 0.0004 & 0.0468 $\\pm$ 0.0006 & 0.0498 $\\pm$ 0.0006\\\\ \\hline\n",
      "MC Dropout Var. & 0.910258691618873 & N & 0.0222 $\\pm$ 0.0002 & \\textbf{0.0448 $\\pm$ 0.0003} & \\textbf{0.0554 $\\pm$ 0.0005} & \\textbf{0.0594 $\\pm$ 0.0006}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On comparison group Balanced, with weight rescaling\n",
      "On metric weighted_kappa\n",
      "\n",
      "Baseline weighted_kappa perfs:\n",
      "set([0.8147220386187509])\n",
      "set([0.001372817930539155])\n",
      "\n",
      " Latex Table for metric weighted_kappa and group Balanced, with weight rescaling\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8147220386187509 & Y & \\textbf{0.0311 $\\pm$ 0.0004} & \\textbf{0.0478 $\\pm$ 0.0005} & \\textbf{0.0603 $\\pm$ 0.0007} & \\textbf{0.07 $\\pm$ 0.0008}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8147220386187509 & N & 0.0303 $\\pm$ 0.0004 & \\textbf{0.0467 $\\pm$ 0.0005} & \\textbf{0.0592 $\\pm$ 0.0007} & 0.0667 $\\pm$ 0.0008\\\\ \\hline\n",
      "Max Class Prob. & 0.8147220386187509 & Y & 0.0162 $\\pm$ 0.0003 & 0.017 $\\pm$ 0.0006 & 0.015 $\\pm$ 0.0009 & 0.0035 $\\pm$ 0.0015\\\\ \\hline\n",
      "Max Class Prob. & 0.8147220386187509 & N & 0.0121 $\\pm$ 0.0003 & 0.0137 $\\pm$ 0.0005 & 0.011 $\\pm$ 0.0009 & -0.0013 $\\pm$ 0.0015\\\\ \\hline\n",
      "Entropy & 0.8147220386187509 & Y & 0.0197 $\\pm$ 0.0004 & 0.0308 $\\pm$ 0.0006 & 0.0334 $\\pm$ 0.0009 & 0.0242 $\\pm$ 0.0013\\\\ \\hline\n",
      "Entropy & 0.8147220386187509 & N & 0.0197 $\\pm$ 0.0004 & 0.0275 $\\pm$ 0.0006 & 0.0221 $\\pm$ 0.001 & 0.0068 $\\pm$ 0.0014\\\\ \\hline\n",
      "MC Dropout Var. & 0.8147220386187509 & Y & -0.028 $\\pm$ 0.0008 & -0.0445 $\\pm$ 0.0012 & -0.0543 $\\pm$ 0.0017 & -0.0732 $\\pm$ 0.0026\\\\ \\hline\n",
      "MC Dropout Var. & 0.8147220386187509 & N & -0.0194 $\\pm$ 0.0006 & -0.0323 $\\pm$ 0.001 & -0.0358 $\\pm$ 0.0015 & -0.0432 $\\pm$ 0.0022\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.8276819264396005])\n",
      "set([0.000663805459842235])\n",
      "\n",
      " Latex Table for metric accuracy and group Balanced, with weight rescaling\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8276819264396005 & Y & 0.0143 $\\pm$ 0.0002 & 0.0322 $\\pm$ 0.0003 & 0.0402 $\\pm$ 0.0004 & 0.0433 $\\pm$ 0.0005\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8276819264396005 & N & 0.0181 $\\pm$ 0.0001 & 0.0352 $\\pm$ 0.0003 & 0.0432 $\\pm$ 0.0004 & 0.0474 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & 0.8276819264396005 & Y & \\textbf{0.0229 $\\pm$ 0.0001} & \\textbf{0.0416 $\\pm$ 0.0002} & \\textbf{0.0602 $\\pm$ 0.0003} & 0.0748 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & 0.8276819264396005 & N & \\textbf{0.0227 $\\pm$ 0.0001} & \\textbf{0.042 $\\pm$ 0.0002} & \\textbf{0.0605 $\\pm$ 0.0003} & 0.0746 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & 0.8276819264396005 & Y & 0.02 $\\pm$ 0.0002 & 0.04 $\\pm$ 0.0003 & 0.0558 $\\pm$ 0.0004 & 0.0717 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & 0.8276819264396005 & N & \\textbf{0.0224 $\\pm$ 0.0002} & \\textbf{0.0415 $\\pm$ 0.0003} & 0.0591 $\\pm$ 0.0003 & 0.0751 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & 0.8276819264396005 & Y & 0.0183 $\\pm$ 0.0002 & 0.0363 $\\pm$ 0.0003 & 0.0557 $\\pm$ 0.0004 & 0.0761 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & 0.8276819264396005 & N & 0.0189 $\\pm$ 0.0002 & 0.0376 $\\pm$ 0.0003 & 0.0585 $\\pm$ 0.0003 & \\textbf{0.0776 $\\pm$ 0.0005}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On comparison group Imalanced, with MC dropout\n",
      "On metric weighted_kappa\n",
      "\n",
      "Baseline weighted_kappa perfs:\n",
      "set([0.6823872721667855, 0.7129970174811562])\n",
      "set([0.002564719778120457, 0.0025607313495203364])\n",
      "\n",
      " Latex Table for metric weighted_kappa and group Imalanced, with MC dropout\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Adapted?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.7129970174811562 & Y & 0.0426 $\\pm$ 0.001 & \\textbf{0.0673 $\\pm$ 0.0011} & \\textbf{0.0854 $\\pm$ 0.0012} & \\textbf{0.0974 $\\pm$ 0.0013}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.6823872721667855 & N & 0.0222 $\\pm$ 0.0005 & 0.0383 $\\pm$ 0.0006 & 0.0547 $\\pm$ 0.0007 & 0.0646 $\\pm$ 0.0007\\\\ \\hline\n",
      "Max Class Prob. & 0.7129970174811562 & Y & -0.0113 $\\pm$ 0.0022 & -0.0882 $\\pm$ 0.0103 & -0.1747 $\\pm$ 0.0125 & -0.1852 $\\pm$ 0.0125\\\\ \\hline\n",
      "Max Class Prob. & 0.6823872721667855 & N & 0.0327 $\\pm$ 0.0011 & 0.0204 $\\pm$ 0.0029 & -0.0336 $\\pm$ 0.0046 & -0.0539 $\\pm$ 0.0068\\\\ \\hline\n",
      "Entropy & 0.7129970174811562 & Y & -0.0304 $\\pm$ 0.0044 & -0.1363 $\\pm$ 0.0117 & -0.2174 $\\pm$ 0.012 & -0.2105 $\\pm$ 0.0126\\\\ \\hline\n",
      "Entropy & 0.6823872721667855 & N & \\textbf{0.0557 $\\pm$ 0.0011} & \\textbf{0.071 $\\pm$ 0.0025} & -0.0014 $\\pm$ 0.0053 & -0.0631 $\\pm$ 0.007\\\\ \\hline\n",
      "MC Dropout Var. & 0.7129970174811562 & Y & -0.0966 $\\pm$ 0.0049 & -0.1975 $\\pm$ 0.0119 & -0.273 $\\pm$ 0.0123 & -0.2714 $\\pm$ 0.0146\\\\ \\hline\n",
      "MC Dropout Var. & 0.6823872721667855 & N & -0.0689 $\\pm$ 0.0025 & -0.0879 $\\pm$ 0.0047 & -0.1631 $\\pm$ 0.0091 & -0.1999 $\\pm$ 0.0104\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.9106748306307645, 0.9208151987327889])\n",
      "set([0.0005795692387600026, 0.0004882681438665038])\n",
      "\n",
      " Latex Table for metric accuracy and group Imalanced, with MC dropout\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Adapted?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.9208151987327889 & Y & 0.0106 $\\pm$ 0.0003 & 0.0211 $\\pm$ 0.0004 & 0.0261 $\\pm$ 0.0005 & 0.0291 $\\pm$ 0.0006\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.9106748306307645 & N & 0.0041 $\\pm$ 0.0001 & 0.0091 $\\pm$ 0.0003 & 0.0155 $\\pm$ 0.0003 & 0.0182 $\\pm$ 0.0003\\\\ \\hline\n",
      "Max Class Prob. & 0.9208151987327889 & Y & 0.0245 $\\pm$ 0.0002 & 0.0401 $\\pm$ 0.0004 & 0.0459 $\\pm$ 0.0006 & 0.0494 $\\pm$ 0.0007\\\\ \\hline\n",
      "Max Class Prob. & 0.9106748306307645 & N & \\textbf{0.025 $\\pm$ 0.0001} & 0.0427 $\\pm$ 0.0003 & 0.053 $\\pm$ 0.0005 & \\textbf{0.0589 $\\pm$ 0.0005}\\\\ \\hline\n",
      "Entropy & 0.9208151987327889 & Y & \\textbf{0.0255 $\\pm$ 0.0003} & 0.0405 $\\pm$ 0.0005 & 0.0459 $\\pm$ 0.0006 & 0.0493 $\\pm$ 0.0007\\\\ \\hline\n",
      "Entropy & 0.9106748306307645 & N & \\textbf{0.0249 $\\pm$ 0.0002} & 0.0425 $\\pm$ 0.0003 & 0.0534 $\\pm$ 0.0005 & 0.0584 $\\pm$ 0.0005\\\\ \\hline\n",
      "MC Dropout Var. & 0.9208151987327889 & Y & 0.0234 $\\pm$ 0.0002 & 0.0399 $\\pm$ 0.0004 & 0.0461 $\\pm$ 0.0006 & 0.0493 $\\pm$ 0.0007\\\\ \\hline\n",
      "MC Dropout Var. & 0.9106748306307645 & N & 0.022 $\\pm$ 0.0002 & \\textbf{0.0443 $\\pm$ 0.0003} & \\textbf{0.0549 $\\pm$ 0.0005} & \\textbf{0.059 $\\pm$ 0.0006}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On comparison group Balanced, with MC dropout\n",
      "On metric weighted_kappa\n",
      "\n",
      "Baseline weighted_kappa perfs:\n",
      "set([0.8150171373964402])\n",
      "set([0.0013478126288873832])\n",
      "\n",
      " Latex Table for metric weighted_kappa and group Balanced, with MC dropout\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8150171373964402 & Y & \\textbf{0.0307 $\\pm$ 0.0004} & \\textbf{0.0496 $\\pm$ 0.0006} & \\textbf{0.062 $\\pm$ 0.0007} & \\textbf{0.0729 $\\pm$ 0.0009}\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8150171373964402 & N & \\textbf{0.03 $\\pm$ 0.0004} & 0.0478 $\\pm$ 0.0005 & \\textbf{0.0608 $\\pm$ 0.0007} & 0.0709 $\\pm$ 0.0009\\\\ \\hline\n",
      "Max Class Prob. & 0.8150171373964402 & Y & 0.0163 $\\pm$ 0.0004 & 0.0149 $\\pm$ 0.0006 & 0.0127 $\\pm$ 0.001 & -0.0016 $\\pm$ 0.0016\\\\ \\hline\n",
      "Max Class Prob. & 0.8150171373964402 & N & 0.0134 $\\pm$ 0.0003 & 0.0128 $\\pm$ 0.0006 & 0.0083 $\\pm$ 0.001 & -0.0042 $\\pm$ 0.0015\\\\ \\hline\n",
      "Entropy & 0.8150171373964402 & Y & 0.0193 $\\pm$ 0.0004 & 0.0288 $\\pm$ 0.0007 & 0.0309 $\\pm$ 0.001 & 0.0215 $\\pm$ 0.0015\\\\ \\hline\n",
      "Entropy & 0.8150171373964402 & N & 0.0186 $\\pm$ 0.0004 & 0.0272 $\\pm$ 0.0006 & 0.0256 $\\pm$ 0.0009 & 0.005 $\\pm$ 0.0015\\\\ \\hline\n",
      "MC Dropout Var. & 0.8150171373964402 & Y & -0.0264 $\\pm$ 0.0008 & -0.0426 $\\pm$ 0.0012 & -0.0503 $\\pm$ 0.0016 & -0.0645 $\\pm$ 0.0024\\\\ \\hline\n",
      "MC Dropout Var. & 0.8150171373964402 & N & -0.0199 $\\pm$ 0.0007 & -0.0329 $\\pm$ 0.001 & -0.0361 $\\pm$ 0.0015 & -0.0435 $\\pm$ 0.0022\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n",
      "On metric accuracy\n",
      "\n",
      "Baseline accuracy perfs:\n",
      "set([0.8295190115821545])\n",
      "set([0.0006567570345799697])\n",
      "\n",
      " Latex Table for metric accuracy and group Balanced, with MC dropout\n",
      "\n",
      "\\begin{tabular}{ | c | c | c | c | c | c | c | }\n",
      "\\hline Method & Calibrated?& $\\Delta$ @5\\% Abs. & $\\Delta$ @10\\% Abs. & $\\Delta$ @15\\% Abs. & $\\Delta$ @20\\% Abs.\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8295190115821545 & Y & 0.0134 $\\pm$ 0.0002 & 0.0319 $\\pm$ 0.0003 & 0.0402 $\\pm$ 0.0004 & 0.0445 $\\pm$ 0.0005\\\\ \\hline\n",
      "E[$\\Delta$Kappa] & 0.8295190115821545 & N & 0.0155 $\\pm$ 0.0002 & 0.0339 $\\pm$ 0.0003 & 0.0425 $\\pm$ 0.0004 & 0.047 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & 0.8295190115821545 & Y & \\textbf{0.0229 $\\pm$ 0.0001} & \\textbf{0.0415 $\\pm$ 0.0002} & \\textbf{0.0599 $\\pm$ 0.0003} & 0.0744 $\\pm$ 0.0005\\\\ \\hline\n",
      "Max Class Prob. & 0.8295190115821545 & N & \\textbf{0.0228 $\\pm$ 0.0001} & \\textbf{0.0418 $\\pm$ 0.0002} & \\textbf{0.0599 $\\pm$ 0.0003} & 0.0742 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & 0.8295190115821545 & Y & 0.019 $\\pm$ 0.0002 & 0.0389 $\\pm$ 0.0003 & 0.0544 $\\pm$ 0.0004 & 0.0715 $\\pm$ 0.0005\\\\ \\hline\n",
      "Entropy & 0.8295190115821545 & N & 0.0215 $\\pm$ 0.0002 & 0.0404 $\\pm$ 0.0002 & 0.0575 $\\pm$ 0.0003 & \\textbf{0.0746 $\\pm$ 0.0005}\\\\ \\hline\n",
      "MC Dropout Var. & 0.8295190115821545 & Y & 0.0178 $\\pm$ 0.0002 & 0.0352 $\\pm$ 0.0003 & 0.0546 $\\pm$ 0.0004 & \\textbf{0.075 $\\pm$ 0.0005}\\\\ \\hline\n",
      "MC Dropout Var. & 0.8295190115821545 & N & 0.0182 $\\pm$ 0.0002 & 0.0359 $\\pm$ 0.0003 & 0.0567 $\\pm$ 0.0003 & \\textbf{0.0757 $\\pm$ 0.0004}\\\\ \\hline\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from abstention.figure_making_utils import (\n",
    "    wilcox_srs, get_ustats_mat,\n",
    "    get_tied_top_and_worst_methods)\n",
    "from collections import OrderedDict\n",
    "\n",
    "comparison_groups = OrderedDict([\n",
    "        ('Imbalanced, with weight rescaling', ([\n",
    "          ('calib_weightrescalepreds_imbalanced_adapted', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_weightrescalepreds_imbalanced_unadapted', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_weightrescalepreds_imbalanced_adapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_weightrescalepreds_imbalanced_unadapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_weightrescalepreds_imbalanced_adapted', 'entropy'),\n",
    "          ('calib_weightrescalepreds_imbalanced_unadapted', 'entropy'),\n",
    "          ('calib_weightrescalepreds_imbalanced_adapted', 'variance'),\n",
    "          ('calib_weightrescalepreds_imbalanced_unadapted', 'variance')],\n",
    "         ['adapted'])),\n",
    "        ('Balanced, with weight rescaling', ([\n",
    "          ('calib_weightrescalepreds', 'expected_delta_weighted_kappa'),\n",
    "          ('uncalib_weightrescalepreds', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_weightrescalepreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('uncalib_weightrescalepreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_weightrescalepreds', 'entropy'),\n",
    "          ('uncalib_weightrescalepreds', 'entropy'),\n",
    "          ('calib_weightrescalepreds', 'variance'),\n",
    "          ('uncalib_weightrescalepreds', 'variance')],\n",
    "         ['calib'])),\n",
    "       ('Imalanced, with MC dropout', ([\n",
    "          ('calib_mcdrpreds_imbalanced_adapted', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_mcdrpreds_imbalanced_unadapted', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_mcdrpreds_imbalanced_adapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_mcdrpreds_imbalanced_unadapted', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_mcdrpreds_imbalanced_adapted', 'entropy'),\n",
    "          ('calib_mcdrpreds_imbalanced_unadapted', 'entropy'),\n",
    "          ('calib_mcdrpreds_imbalanced_adapted', 'variance'),\n",
    "          ('calib_mcdrpreds_imbalanced_unadapted', 'variance')],\n",
    "         ['adapted'])),\n",
    "       ('Balanced, with MC dropout', ([\n",
    "          ('calib_mcdrpreds', 'expected_delta_weighted_kappa'),\n",
    "          ('uncalib_mcdrpreds', 'expected_delta_weighted_kappa'),\n",
    "          ('calib_mcdrpreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('uncalib_mcdrpreds', 'dist_maxclass_prob_from_one'),\n",
    "          ('calib_mcdrpreds', 'entropy'),\n",
    "          ('uncalib_mcdrpreds', 'entropy'),\n",
    "          ('calib_mcdrpreds', 'variance'),\n",
    "          ('uncalib_mcdrpreds', 'variance')],\n",
    "         ['calib']))\n",
    "    ])\n",
    "\n",
    "friendly_method_names = {\n",
    "    'expected_delta_weighted_kappa': 'E[$\\Delta$Kappa]',\n",
    "    'dist_maxclass_prob_from_one': 'Max Class Prob.',\n",
    "    'entropy': 'Entropy',\n",
    "    'variance': 'MC Dropout Var.'\n",
    "}\n",
    "abstention_fractions = ['0.05', '0.1', '0.15', '0.2']\n",
    "\n",
    "for comparison_group_name in comparison_groups:\n",
    "    \n",
    "    print(\"On comparison group\", comparison_group_name)\n",
    "    columnstowrite = comparison_groups[comparison_group_name][1]\n",
    "    for metric in [\"weighted_kappa\",\n",
    "                   \"accuracy\"]:\n",
    "        print(\"On metric\", metric)\n",
    "        \n",
    "        #gather all the necessary data\n",
    "        settingnmethod_to_baselineperfs = OrderedDict()\n",
    "        settingnmethod_to_abstentionfraction_to_perfs = OrderedDict()\n",
    "        \n",
    "        for (settingsname, methodname) in comparison_groups[comparison_group_name][0]:\n",
    "            settingnmethod = settingsname+\"-\"+methodname\n",
    "            settingnmethod_to_baselineperfs[settingnmethod] =\\\n",
    "                settingsname_to_metric_to_baselineperfs[settingsname][metric]\n",
    "            \n",
    "            \n",
    "            abstentionfraction_to_perfs = OrderedDict()\n",
    "            settingnmethod_to_abstentionfraction_to_perfs[settingnmethod] =\\\n",
    "                abstentionfraction_to_perfs\n",
    "            for abstention_fraction in abstention_fractions:\n",
    "                abstentionfraction_to_perfs[abstention_fraction] = (\n",
    "                    settingsname_to_metric_to_fraction_to_method_to_perfs[\n",
    "                        settingsname][\"delta_\"+metric][abstention_fraction][\n",
    "                        methodname])\n",
    "        \n",
    "        #prepare the table contents\n",
    "        \n",
    "        settingnmethod_to_tablecontents = OrderedDict()\n",
    "        for settingnmethod in settingnmethod_to_baselineperfs:\n",
    "            tablerow = {}\n",
    "            settingnmethod_to_tablecontents[settingnmethod] = tablerow\n",
    "            tablerow['baseline'] = {\n",
    "                'mean': np.mean(settingnmethod_to_baselineperfs[settingnmethod]),\n",
    "                'stderr': np.std(settingnmethod_to_baselineperfs[settingnmethod],\n",
    "                                 ddof=1)/np.sqrt(num_folds)}\n",
    "            tablerow['method'] = settingnmethod.split(\"-\")[1]\n",
    "            tablerow['mcdr'] = \"mcdr\" in settingnmethod.split(\"-\")[0]\n",
    "            tablerow['calib'] = (\"uncalib\" in settingnmethod.split(\"-\")[0])==False\n",
    "            #if neither 'balanced' nor 'imbalanced' is in the name, it means balanced\n",
    "            tablerow['imbalanced'] = ((\"imbalanced\" in settingnmethod.split(\"-\")[0])\n",
    "                                      and (\"balanced\" in settingnmethod.split(\"-\")[0]))\n",
    "            #if neither adapted nor unadapted is in the name, it means no adaptation\n",
    "            tablerow['adapted'] = ((\"unadapted\" not in settingnmethod.split(\"-\")[0])\n",
    "                                     and (\"adapted\" in settingnmethod.split(\"-\")[0]))\n",
    "        \n",
    "        for abstention_fraction in abstention_fractions:\n",
    "            method_to_perfs = OrderedDict()\n",
    "            for settingnmethod in settingnmethod_to_tablecontents:\n",
    "                perfsdelta = settingnmethod_to_abstentionfraction_to_perfs[\n",
    "                    settingnmethod][abstention_fraction]\n",
    "                method_to_perfs[settingnmethod] = perfsdelta\n",
    "                mean_perfsdelta = np.mean(perfsdelta)\n",
    "                stderr_perfsdelta = (np.std(perfsdelta,ddof=1)/\n",
    "                                     np.sqrt(num_folds))\n",
    "                tablerow = settingnmethod_to_tablecontents[settingnmethod]\n",
    "                tablerow[abstention_fraction] = {\n",
    "                    'mean': mean_perfsdelta,\n",
    "                    'stderr': stderr_perfsdelta}\n",
    "            methods_to_consider = list(method_to_perfs.keys())\n",
    "            ustats_mat = get_ustats_mat(\n",
    "                method_to_perfs,\n",
    "                methods_to_consider,\n",
    "                max_ustat=1275)\n",
    "            tied_top_methods, tied_worst_methods =(\n",
    "                get_tied_top_and_worst_methods(\n",
    "                    ustats_mat,\n",
    "                    methods_to_consider,\n",
    "                    #0.05 threshold for one-sided test when N=119 is 50\n",
    "                    #http://www.real-statistics.com/statistics-tables/wilcoxon-signed-ranks-table/\n",
    "                    threshold=120\n",
    "                ))\n",
    "            tied_top_methods = [methods_to_consider[x]\n",
    "                                for x in tied_top_methods]\n",
    "            #print(abstention_fraction)\n",
    "            #print(tied_top_methods)\n",
    "            for settingnmethod in settingnmethod_to_tablecontents:\n",
    "                settingnmethod_to_tablecontents[\n",
    "                    settingnmethod][abstention_fraction][\n",
    "                    'istop'] = (settingnmethod in tied_top_methods)\n",
    "            #print(settingnmethod_to_tablecontents)\n",
    "            \n",
    "        thestr = \"\\\\begin{tabular}{ | c | c | c | c | c | c | c | }\\n\"\n",
    "        thestr += (\"\\\\hline Method & \"\n",
    "                  +('Calibrated?' if 'calib' in columnstowrite else '')\n",
    "                  +('Adapted?' if 'adapted' in columnstowrite else '')\n",
    "                  +\"& $\\\\Delta$ @\")\n",
    "        thestr += \" & $\\\\Delta$ @\".join([str(int(100*float(x)))+\"\\\\% Abs.\"\n",
    "                              for x in abstention_fractions])\n",
    "        thestr += \"\\\\\\\\ \\\\hline\\n\"\n",
    "        for settingnmethod in settingnmethod_to_tablecontents:\n",
    "            tablerow = settingnmethod_to_tablecontents[settingnmethod]\n",
    "            thestr += friendly_method_names[tablerow['method']]\n",
    "            thestr += \" & \"+str(tablerow['baseline']['mean'])\n",
    "            if ('calib' in columnstowrite):\n",
    "                thestr += \" & \"+(\"Y\" if tablerow['calib'] else \"N\")\n",
    "            if ('adapted' in columnstowrite):\n",
    "                thestr += \" & \"+(\"Y\" if tablerow['adapted'] else \"N\")\n",
    "            #thestr += \" & \"+(str(np.round(tablerow['baseline']['mean'],4))\n",
    "            #                 +\" $\\\\pm$ \"\n",
    "            #                 +str(np.round(tablerow['baseline']['stderr'],4)))\n",
    "            for abstention_fraction in abstention_fractions:\n",
    "                thestr += (\n",
    "                    \" & \"+\n",
    "                    (\"\\\\textbf{\" if tablerow[abstention_fraction]['istop'] else \"\")\n",
    "                    +str(np.round(tablerow[abstention_fraction]['mean'],4))\n",
    "                    +\" $\\\\pm$ \"\n",
    "                    +str(np.round(tablerow[abstention_fraction]['stderr'],4))\n",
    "                    +(\"}\" if tablerow[abstention_fraction]['istop'] else \"\"))\n",
    "            thestr += \"\\\\\\\\ \\hline\\n\"\n",
    "        thestr += \"\\\\end{tabular}\\n\"\n",
    "        \n",
    "        print(\"\\nBaseline \"+metric+\" perfs:\")\n",
    "        baseline_mean = set(\n",
    "                x['baseline']['mean'] for x in\n",
    "                settingnmethod_to_tablecontents.values())\n",
    "        print(baseline_mean)\n",
    "        #assert that all the methods have the same baseline\n",
    "        #assert len(baseline_mean)==1\n",
    "        #baseline_mean = list(baseline_mean)[0]\n",
    "        baseline_stderr = set(\n",
    "                x['baseline']['stderr'] for x in\n",
    "                settingnmethod_to_tablecontents.values())\n",
    "        print(baseline_stderr)\n",
    "        #assert len(baseline_stderr)==1\n",
    "        #baseline_stderr = list(baseline_stderr)[0]\n",
    "        #print(np.round(baseline_mean,4),\"$\\\\pm$\",np.round(baseline_stderr,4))\n",
    "        \n",
    "        print(\"\\n Latex Table for metric \"\n",
    "              +metric+\" and group \"+comparison_group_name\n",
    "              +\"\\n\\n\"+thestr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
